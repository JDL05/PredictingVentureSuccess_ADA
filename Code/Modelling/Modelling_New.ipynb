{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7917d461468ce96a",
   "metadata": {},
   "source": [
    "# Prediction\n",
    "Explanation:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d89661898e930c7",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "id": "4bfa78658af2b407",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T19:36:26.470849Z",
     "start_time": "2025-01-15T19:36:26.463421Z"
    }
   },
   "source": [
    "from importnb import Notebook\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, roc_auc_score, roc_curve, precision_recall_curve, make_scorer\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, LeakyReLU\n",
    "from tensorflow.keras.initializers import HeNormal\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "import shap\n",
    "from sklearn.calibration import calibration_curve\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ],
   "outputs": [],
   "execution_count": 86
  },
  {
   "cell_type": "markdown",
   "id": "c66d297d4dfe8590",
   "metadata": {},
   "source": [
    "## Dataframe import from 'FeatureEngineering'"
   ]
  },
  {
   "cell_type": "code",
   "id": "26daa4a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T19:36:26.481243Z",
     "start_time": "2025-01-15T19:36:26.479146Z"
    }
   },
   "source": [
    "pd.set_option('display.max_columns', None)"
   ],
   "outputs": [],
   "execution_count": 87
  },
  {
   "cell_type": "code",
   "id": "dcbd2b89652d5181",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T19:36:26.530510Z",
     "start_time": "2025-01-15T19:36:26.529164Z"
    }
   },
   "source": [
    "#with Notebook():\n",
    "#    from ModelPreparation import data\n"
   ],
   "outputs": [],
   "execution_count": 88
  },
  {
   "cell_type": "markdown",
   "id": "64fa4fc5",
   "metadata": {},
   "source": [
    "## Manual Upload"
   ]
  },
  {
   "cell_type": "code",
   "id": "7e2a957c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T19:36:26.583544Z",
     "start_time": "2025-01-15T19:36:26.568870Z"
    }
   },
   "source": [
    "data = pd.read_csv('data.csv')"
   ],
   "outputs": [],
   "execution_count": 89
  },
  {
   "cell_type": "code",
   "id": "870ec4eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T19:36:26.637460Z",
     "start_time": "2025-01-15T19:36:26.623043Z"
    }
   },
   "source": [
    "data"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "        ID  Number of Founders  Number of Employees  Number of Funding Rounds  \\\n",
       "0        1              1.0000              30.5000                         2   \n",
       "1        2              2.0000              30.5000                         2   \n",
       "2        3              2.0000              30.5000                         1   \n",
       "3        4              3.0000               5.5000                         1   \n",
       "4        5              4.0000              30.5000                         1   \n",
       "...    ...                 ...                  ...                       ...   \n",
       "1505  1506              4.0000               5.5000                         2   \n",
       "1506  1507              3.0000              30.5000                         3   \n",
       "1507  1508              4.0000              30.5000                         2   \n",
       "1508  1509              2.0000               5.5000                         2   \n",
       "1509  1510              1.0000              30.5000                         1   \n",
       "\n",
       "      Last Funding Amount  Total Funding Amount  Number of Lead Investors  \\\n",
       "0            7000000.0000          7000000.0000                    3.0000   \n",
       "1            3000000.0000          3000000.0000                    1.0000   \n",
       "2                  0.0000                0.0000                    1.0000   \n",
       "3            1000000.0000          1000000.0000                    2.0000   \n",
       "4                  0.0000                0.0000                    0.0000   \n",
       "...                   ...                   ...                       ...   \n",
       "1505           36000.0000           161000.0000                    2.0000   \n",
       "1506               0.0000            60000.0000                    0.0000   \n",
       "1507               0.0000                0.0000                    2.0000   \n",
       "1508               0.0000                0.0000                    1.0000   \n",
       "1509               0.0000                0.0000                    0.0000   \n",
       "\n",
       "      Number of Investors  Number of Acquisitions  X: Followers  X: Following  \\\n",
       "0                  4.0000                  0.0000        0.0000        0.0000   \n",
       "1                  2.0000                  0.0000        0.0000        0.0000   \n",
       "2                  1.0000                  0.0000        0.0000        0.0000   \n",
       "3                  2.0000                  0.0000      269.0000      346.0000   \n",
       "4                  0.0000                  0.0000        0.0000        0.0000   \n",
       "...                   ...                     ...           ...           ...   \n",
       "1505               2.0000                  0.0000      102.0000      438.0000   \n",
       "1506               2.0000                  0.0000        0.0000        0.0000   \n",
       "1507               4.0000                  0.0000      232.0000      482.0000   \n",
       "1508               2.0000                  0.0000       69.0000      298.0000   \n",
       "1509               1.0000                  0.0000        0.0000        0.0000   \n",
       "\n",
       "      X: Number of Tweets  X: Account Age Days  X: Tweet Activity  \\\n",
       "0                  0.0000               0.0000             0.0000   \n",
       "1                  0.0000               0.0000             0.0000   \n",
       "2                  0.0000               0.0000             0.0000   \n",
       "3                248.0000            3389.0000             0.0700   \n",
       "4                  0.0000               0.0000             0.0000   \n",
       "...                   ...                  ...                ...   \n",
       "1505             169.0000            2499.0000             0.0700   \n",
       "1506               0.0000               0.0000             0.0000   \n",
       "1507             786.0000            2467.0000             0.3200   \n",
       "1508              71.0000            3456.0000             0.0200   \n",
       "1509               0.0000               0.0000             0.0000   \n",
       "\n",
       "      X: Followers Max Growth  X: Followers Max Loss  X: Tweets Max Growth  \\\n",
       "0                      0.0000                 0.0000                0.0000   \n",
       "1                      0.0000                 0.0000                0.0000   \n",
       "2                      0.0000                 0.0000                0.0000   \n",
       "3                      2.0000                -3.0000                0.0000   \n",
       "4                      0.0000                 0.0000                0.0000   \n",
       "...                       ...                    ...                   ...   \n",
       "1505                   4.0000                -2.0000                0.0000   \n",
       "1506                   0.0000                 0.0000                0.0000   \n",
       "1507                  29.0000               -42.0000              104.0000   \n",
       "1508                   4.0000                -4.0000                0.0000   \n",
       "1509                   0.0000                 0.0000                0.0000   \n",
       "\n",
       "      X: Tweets Max Loss  Months until First Round  Grant Y/N  \\\n",
       "0                 0.0000                   40.0000          0   \n",
       "1                 0.0000                   15.0000          0   \n",
       "2                 0.0000                   29.0000          0   \n",
       "3                -2.0000                   -1.0000          0   \n",
       "4                 0.0000                   32.0000          0   \n",
       "...                  ...                       ...        ...   \n",
       "1505             -2.0000                    4.0000          1   \n",
       "1506              0.0000                   59.0000          0   \n",
       "1507             -8.0000                    4.0000          0   \n",
       "1508             -1.0000                   14.0000          0   \n",
       "1509              0.0000                    9.0000          0   \n",
       "\n",
       "      Made Acquisitions  Project Funding  Startup Funding  Growth Funding  \\\n",
       "0                     0                1                1               0   \n",
       "1                     0                1                1               0   \n",
       "2                     0                1                1               0   \n",
       "3                     0                0                0               0   \n",
       "4                     0                1                1               0   \n",
       "...                 ...              ...              ...             ...   \n",
       "1505                  0                1                0               0   \n",
       "1506                  0                1                1               0   \n",
       "1507                  0                1                1               0   \n",
       "1508                  0                1                1               0   \n",
       "1509                  0                1                1               0   \n",
       "\n",
       "      Expansion Funding  Exit Funding  Average Time To Next Round  \\\n",
       "0                     0             0                     31.5500   \n",
       "1                     0             0                     16.4333   \n",
       "2                     0             0                     29.4000   \n",
       "3                     0             0                     -1.0000   \n",
       "4                     0             0                     32.4333   \n",
       "...                 ...           ...                         ...   \n",
       "1505                  0             0                     11.6333   \n",
       "1506                  0             0                     34.3667   \n",
       "1507                  0             0                     21.6833   \n",
       "1508                  0             0                     20.3833   \n",
       "1509                  0             0                      8.3667   \n",
       "\n",
       "      Average Funding Size  Average Number of Investments by Investors  \\\n",
       "0             7000000.0000                                      0.0000   \n",
       "1             3000000.0000                                    321.0000   \n",
       "2                   0.0000                                     36.0000   \n",
       "3                   0.0000                                      0.0000   \n",
       "4                   0.0000                                      0.0000   \n",
       "...                    ...                                         ...   \n",
       "1505            80500.0000                                     28.0000   \n",
       "1506            30000.0000                                      4.0000   \n",
       "1507                0.0000                                     58.6667   \n",
       "1508                0.0000                                     53.0000   \n",
       "1509                0.0000                                      2.0000   \n",
       "\n",
       "      Average Number of Exits by Investors  \\\n",
       "0                                 182.0000   \n",
       "1                                  32.0000   \n",
       "2                                   7.0000   \n",
       "3                                   0.0000   \n",
       "4                                   0.0000   \n",
       "...                                    ...   \n",
       "1505                                2.0000   \n",
       "1506                                0.0000   \n",
       "1507                                6.0000   \n",
       "1508                                8.0000   \n",
       "1509                                0.0000   \n",
       "\n",
       "      Average Number of Lead Investments by Investors  \\\n",
       "0                                            220.0000   \n",
       "1                                             49.5000   \n",
       "2                                             19.0000   \n",
       "3                                              0.0000   \n",
       "4                                              0.0000   \n",
       "...                                               ...   \n",
       "1505                                           8.5000   \n",
       "1506                                           0.0000   \n",
       "1507                                          23.3333   \n",
       "1508                                          27.0000   \n",
       "1509                                           0.0000   \n",
       "\n",
       "      Average Number of Portfolio Organizations by Investors  \\\n",
       "0                                                     0.0000   \n",
       "1                                                   277.0000   \n",
       "2                                                    29.0000   \n",
       "3                                                     0.0000   \n",
       "4                                                     0.0000   \n",
       "...                                                      ...   \n",
       "1505                                                 27.0000   \n",
       "1506                                                  3.0000   \n",
       "1507                                                 41.0000   \n",
       "1508                                                 53.0000   \n",
       "1509                                                  2.0000   \n",
       "\n",
       "      Investor Country_AUS  Investor Country_GER  Investor Country_SWI  \\\n",
       "0                   0.0000                0.0000                0.0000   \n",
       "1                   0.0000                1.0000                0.0000   \n",
       "2                   0.0000                1.0000                0.0000   \n",
       "3                   0.0000                0.0000                0.0000   \n",
       "4                   0.0000                0.0000                0.0000   \n",
       "...                    ...                   ...                   ...   \n",
       "1505                0.0000                1.0000                0.0000   \n",
       "1506                0.0000                1.0000                0.0000   \n",
       "1507                0.0000                1.0000                0.0000   \n",
       "1508                0.0000                1.0000                0.0000   \n",
       "1509                0.0000                1.0000                0.0000   \n",
       "\n",
       "      Investor Country_UK  Investor Country_USA  Top Investor Participation  \\\n",
       "0                  0.0000                1.0000                           1   \n",
       "1                  0.0000                1.0000                           1   \n",
       "2                  0.0000                0.0000                           0   \n",
       "3                  0.0000                0.0000                           0   \n",
       "4                  0.0000                0.0000                           0   \n",
       "...                   ...                   ...                         ...   \n",
       "1505               0.0000                0.0000                           0   \n",
       "1506               0.0000                0.0000                           0   \n",
       "1507               0.0000                0.0000                           0   \n",
       "1508               0.0000                0.0000                           0   \n",
       "1509               0.0000                0.0000                           0   \n",
       "\n",
       "      Industry_administrative services  Industry_advertising  \\\n",
       "0                               0.0000                0.0000   \n",
       "1                               0.0000                0.0000   \n",
       "2                               0.0000                0.0000   \n",
       "3                               0.0000                0.0000   \n",
       "4                               0.0000                0.0000   \n",
       "...                                ...                   ...   \n",
       "1505                            1.0000                0.0000   \n",
       "1506                            0.0000                0.0000   \n",
       "1507                            0.0000                0.0000   \n",
       "1508                            0.0000                0.0000   \n",
       "1509                            0.0000                0.0000   \n",
       "\n",
       "      Industry_agriculture and farming  Industry_apps  \\\n",
       "0                               0.0000         0.0000   \n",
       "1                               0.0000         0.0000   \n",
       "2                               0.0000         0.0000   \n",
       "3                               0.0000         0.0000   \n",
       "4                               0.0000         0.0000   \n",
       "...                                ...            ...   \n",
       "1505                            0.0000         1.0000   \n",
       "1506                            0.0000         1.0000   \n",
       "1507                            0.0000         0.0000   \n",
       "1508                            1.0000         0.0000   \n",
       "1509                            0.0000         0.0000   \n",
       "\n",
       "      Industry_artificial intelligence (ai)  Industry_biotechnology  \\\n",
       "0                                    0.0000                  0.0000   \n",
       "1                                    1.0000                  0.0000   \n",
       "2                                    0.0000                  0.0000   \n",
       "3                                    0.0000                  0.0000   \n",
       "4                                    1.0000                  0.0000   \n",
       "...                                     ...                     ...   \n",
       "1505                                 0.0000                  0.0000   \n",
       "1506                                 1.0000                  0.0000   \n",
       "1507                                 1.0000                  0.0000   \n",
       "1508                                 0.0000                  0.0000   \n",
       "1509                                 0.0000                  0.0000   \n",
       "\n",
       "      Industry_blockchain and cryptocurrency  Industry_clothing and apparel  \\\n",
       "0                                     0.0000                         0.0000   \n",
       "1                                     0.0000                         0.0000   \n",
       "2                                     0.0000                         0.0000   \n",
       "3                                     0.0000                         0.0000   \n",
       "4                                     0.0000                         0.0000   \n",
       "...                                      ...                            ...   \n",
       "1505                                  0.0000                         0.0000   \n",
       "1506                                  0.0000                         0.0000   \n",
       "1507                                  0.0000                         0.0000   \n",
       "1508                                  0.0000                         0.0000   \n",
       "1509                                  0.0000                         0.0000   \n",
       "\n",
       "      Industry_commerce and shopping  Industry_community and lifestyle  \\\n",
       "0                             0.0000                            0.0000   \n",
       "1                             0.0000                            0.0000   \n",
       "2                             0.0000                            0.0000   \n",
       "3                             0.0000                            0.0000   \n",
       "4                             0.0000                            0.0000   \n",
       "...                              ...                               ...   \n",
       "1505                          0.0000                            0.0000   \n",
       "1506                          0.0000                            0.0000   \n",
       "1507                          0.0000                            0.0000   \n",
       "1508                          1.0000                            1.0000   \n",
       "1509                          0.0000                            0.0000   \n",
       "\n",
       "      Industry_consumer electronics  Industry_consumer goods  \\\n",
       "0                            0.0000                   0.0000   \n",
       "1                            0.0000                   0.0000   \n",
       "2                            0.0000                   0.0000   \n",
       "3                            0.0000                   0.0000   \n",
       "4                            0.0000                   0.0000   \n",
       "...                             ...                      ...   \n",
       "1505                         0.0000                   0.0000   \n",
       "1506                         0.0000                   0.0000   \n",
       "1507                         0.0000                   0.0000   \n",
       "1508                         0.0000                   1.0000   \n",
       "1509                         0.0000                   0.0000   \n",
       "\n",
       "      Industry_content and publishing  Industry_data and analytics  \\\n",
       "0                              0.0000                       0.0000   \n",
       "1                              0.0000                       1.0000   \n",
       "2                              0.0000                       0.0000   \n",
       "3                              0.0000                       0.0000   \n",
       "4                              0.0000                       1.0000   \n",
       "...                               ...                          ...   \n",
       "1505                           0.0000                       0.0000   \n",
       "1506                           0.0000                       1.0000   \n",
       "1507                           0.0000                       1.0000   \n",
       "1508                           0.0000                       0.0000   \n",
       "1509                           0.0000                       0.0000   \n",
       "\n",
       "      Industry_design  Industry_education  Industry_energy  Industry_events  \\\n",
       "0              0.0000              0.0000           0.0000           0.0000   \n",
       "1              0.0000              0.0000           0.0000           0.0000   \n",
       "2              0.0000              0.0000           0.0000           0.0000   \n",
       "3              0.0000              0.0000           0.0000           0.0000   \n",
       "4              0.0000              0.0000           0.0000           0.0000   \n",
       "...               ...                 ...              ...              ...   \n",
       "1505           0.0000              0.0000           0.0000           0.0000   \n",
       "1506           0.0000              0.0000           0.0000           0.0000   \n",
       "1507           0.0000              0.0000           0.0000           0.0000   \n",
       "1508           0.0000              0.0000           0.0000           0.0000   \n",
       "1509           0.0000              0.0000           0.0000           0.0000   \n",
       "\n",
       "      Industry_financial services  Industry_food and beverage  \\\n",
       "0                          0.0000                      0.0000   \n",
       "1                          0.0000                      0.0000   \n",
       "2                          0.0000                      1.0000   \n",
       "3                          0.0000                      0.0000   \n",
       "4                          0.0000                      0.0000   \n",
       "...                           ...                         ...   \n",
       "1505                       0.0000                      0.0000   \n",
       "1506                       0.0000                      0.0000   \n",
       "1507                       1.0000                      0.0000   \n",
       "1508                       0.0000                      1.0000   \n",
       "1509                       0.0000                      0.0000   \n",
       "\n",
       "      Industry_gaming  Industry_government and military  Industry_hardware  \\\n",
       "0              0.0000                            0.0000             0.0000   \n",
       "1              0.0000                            0.0000             1.0000   \n",
       "2              0.0000                            0.0000             0.0000   \n",
       "3              0.0000                            0.0000             0.0000   \n",
       "4              0.0000                            0.0000             0.0000   \n",
       "...               ...                               ...                ...   \n",
       "1505           0.0000                            0.0000             0.0000   \n",
       "1506           0.0000                            0.0000             1.0000   \n",
       "1507           0.0000                            0.0000             0.0000   \n",
       "1508           0.0000                            0.0000             0.0000   \n",
       "1509           0.0000                            0.0000             0.0000   \n",
       "\n",
       "      Industry_health care  Industry_information technology  \\\n",
       "0                   0.0000                           0.0000   \n",
       "1                   0.0000                           0.0000   \n",
       "2                   0.0000                           0.0000   \n",
       "3                   0.0000                           0.0000   \n",
       "4                   0.0000                           1.0000   \n",
       "...                    ...                              ...   \n",
       "1505                0.0000                           0.0000   \n",
       "1506                0.0000                           1.0000   \n",
       "1507                0.0000                           0.0000   \n",
       "1508                1.0000                           0.0000   \n",
       "1509                0.0000                           0.0000   \n",
       "\n",
       "      Industry_internet services  Industry_lending and investments  \\\n",
       "0                         0.0000                            0.0000   \n",
       "1                         0.0000                            0.0000   \n",
       "2                         0.0000                            0.0000   \n",
       "3                         0.0000                            0.0000   \n",
       "4                         1.0000                            0.0000   \n",
       "...                          ...                               ...   \n",
       "1505                      0.0000                            0.0000   \n",
       "1506                      1.0000                            0.0000   \n",
       "1507                      0.0000                            0.0000   \n",
       "1508                      0.0000                            0.0000   \n",
       "1509                      0.0000                            0.0000   \n",
       "\n",
       "      Industry_manufacturing  Industry_media and entertainment  \\\n",
       "0                     0.0000                            0.0000   \n",
       "1                     1.0000                            0.0000   \n",
       "2                     0.0000                            0.0000   \n",
       "3                     1.0000                            0.0000   \n",
       "4                     0.0000                            0.0000   \n",
       "...                      ...                               ...   \n",
       "1505                  0.0000                            0.0000   \n",
       "1506                  0.0000                            0.0000   \n",
       "1507                  0.0000                            0.0000   \n",
       "1508                  0.0000                            0.0000   \n",
       "1509                  0.0000                            0.0000   \n",
       "\n",
       "      Industry_messaging and telecommunications  Industry_mobile  \\\n",
       "0                                        0.0000           0.0000   \n",
       "1                                        0.0000           0.0000   \n",
       "2                                        0.0000           0.0000   \n",
       "3                                        0.0000           0.0000   \n",
       "4                                        0.0000           0.0000   \n",
       "...                                         ...              ...   \n",
       "1505                                     0.0000           1.0000   \n",
       "1506                                     0.0000           0.0000   \n",
       "1507                                     0.0000           0.0000   \n",
       "1508                                     0.0000           0.0000   \n",
       "1509                                     0.0000           0.0000   \n",
       "\n",
       "      Industry_music and audio  Industry_natural resources  \\\n",
       "0                       0.0000                      0.0000   \n",
       "1                       0.0000                      0.0000   \n",
       "2                       0.0000                      0.0000   \n",
       "3                       0.0000                      0.0000   \n",
       "4                       0.0000                      1.0000   \n",
       "...                        ...                         ...   \n",
       "1505                    0.0000                      0.0000   \n",
       "1506                    0.0000                      0.0000   \n",
       "1507                    0.0000                      0.0000   \n",
       "1508                    0.0000                      0.0000   \n",
       "1509                    0.0000                      0.0000   \n",
       "\n",
       "      Industry_navigation and mapping  Industry_other  Industry_payments  \\\n",
       "0                              0.0000          0.0000             0.0000   \n",
       "1                              0.0000          0.0000             0.0000   \n",
       "2                              0.0000          0.0000             0.0000   \n",
       "3                              0.0000          0.0000             0.0000   \n",
       "4                              0.0000          1.0000             0.0000   \n",
       "...                               ...             ...                ...   \n",
       "1505                           0.0000          1.0000             0.0000   \n",
       "1506                           0.0000          0.0000             0.0000   \n",
       "1507                           0.0000          0.0000             0.0000   \n",
       "1508                           0.0000          0.0000             0.0000   \n",
       "1509                           0.0000          1.0000             0.0000   \n",
       "\n",
       "      Industry_platforms  Industry_privacy and security  \\\n",
       "0                 0.0000                         0.0000   \n",
       "1                 0.0000                         0.0000   \n",
       "2                 0.0000                         0.0000   \n",
       "3                 0.0000                         0.0000   \n",
       "4                 0.0000                         0.0000   \n",
       "...                  ...                            ...   \n",
       "1505              0.0000                         0.0000   \n",
       "1506              0.0000                         0.0000   \n",
       "1507              0.0000                         0.0000   \n",
       "1508              0.0000                         0.0000   \n",
       "1509              0.0000                         0.0000   \n",
       "\n",
       "      Industry_professional services  Industry_real estate  \\\n",
       "0                             0.0000                0.0000   \n",
       "1                             0.0000                0.0000   \n",
       "2                             0.0000                0.0000   \n",
       "3                             0.0000                0.0000   \n",
       "4                             0.0000                1.0000   \n",
       "...                              ...                   ...   \n",
       "1505                          0.0000                0.0000   \n",
       "1506                          0.0000                1.0000   \n",
       "1507                          1.0000                0.0000   \n",
       "1508                          0.0000                0.0000   \n",
       "1509                          0.0000                1.0000   \n",
       "\n",
       "      Industry_sales and marketing  Industry_science and engineering  \\\n",
       "0                           0.0000                            0.0000   \n",
       "1                           0.0000                            1.0000   \n",
       "2                           0.0000                            0.0000   \n",
       "3                           0.0000                            0.0000   \n",
       "4                           0.0000                            1.0000   \n",
       "...                            ...                               ...   \n",
       "1505                        0.0000                            0.0000   \n",
       "1506                        0.0000                            1.0000   \n",
       "1507                        0.0000                            0.0000   \n",
       "1508                        0.0000                            0.0000   \n",
       "1509                        0.0000                            0.0000   \n",
       "\n",
       "      Industry_social impact  Industry_software  Industry_sports  \\\n",
       "0                     0.0000             1.0000           0.0000   \n",
       "1                     0.0000             1.0000           0.0000   \n",
       "2                     0.0000             0.0000           0.0000   \n",
       "3                     0.0000             1.0000           0.0000   \n",
       "4                     0.0000             1.0000           0.0000   \n",
       "...                      ...                ...              ...   \n",
       "1505                  0.0000             1.0000           0.0000   \n",
       "1506                  0.0000             1.0000           0.0000   \n",
       "1507                  0.0000             1.0000           0.0000   \n",
       "1508                  0.0000             0.0000           1.0000   \n",
       "1509                  0.0000             0.0000           0.0000   \n",
       "\n",
       "      Industry_sustainability  Industry_transportation  \\\n",
       "0                      0.0000                   1.0000   \n",
       "1                      0.0000                   0.0000   \n",
       "2                      0.0000                   0.0000   \n",
       "3                      0.0000                   0.0000   \n",
       "4                      0.0000                   0.0000   \n",
       "...                       ...                      ...   \n",
       "1505                   0.0000                   0.0000   \n",
       "1506                   0.0000                   1.0000   \n",
       "1507                   0.0000                   0.0000   \n",
       "1508                   0.0000                   0.0000   \n",
       "1509                   0.0000                   0.0000   \n",
       "\n",
       "      Industry_travel and tourism  Industry_video  \\\n",
       "0                          0.0000          0.0000   \n",
       "1                          0.0000          0.0000   \n",
       "2                          0.0000          0.0000   \n",
       "3                          0.0000          0.0000   \n",
       "4                          0.0000          0.0000   \n",
       "...                           ...             ...   \n",
       "1505                       0.0000          0.0000   \n",
       "1506                       0.0000          0.0000   \n",
       "1507                       0.0000          0.0000   \n",
       "1508                       0.0000          0.0000   \n",
       "1509                       0.0000          0.0000   \n",
       "\n",
       "      LinkedIn: Average Followers Founders  \\\n",
       "0                                4815.0000   \n",
       "1                                6482.0000   \n",
       "2                                   0.0000   \n",
       "3                                5664.6667   \n",
       "4                                   0.0000   \n",
       "...                                    ...   \n",
       "1505                              202.0000   \n",
       "1506                                0.0000   \n",
       "1507                             2646.6667   \n",
       "1508                                0.0000   \n",
       "1509                             2572.0000   \n",
       "\n",
       "      LinkedIn: Average Connections Founders  \\\n",
       "0                                  3914.0000   \n",
       "1                                  6479.0000   \n",
       "2                                     0.0000   \n",
       "3                                  5337.6667   \n",
       "4                                     0.0000   \n",
       "...                                      ...   \n",
       "1505                                204.0000   \n",
       "1506                                  0.0000   \n",
       "1507                               2589.0000   \n",
       "1508                                  0.0000   \n",
       "1509                               2091.0000   \n",
       "\n",
       "      LinkedIn: Min Followers Founders  LinkedIn: Max Followers Founders  \\\n",
       "0                            4815.0000                         4815.0000   \n",
       "1                            6482.0000                         6482.0000   \n",
       "2                               0.0000                            0.0000   \n",
       "3                            2089.0000                         9782.0000   \n",
       "4                               0.0000                            0.0000   \n",
       "...                                ...                               ...   \n",
       "1505                          202.0000                          202.0000   \n",
       "1506                            0.0000                            0.0000   \n",
       "1507                         1176.0000                         5489.0000   \n",
       "1508                            0.0000                            0.0000   \n",
       "1509                         2572.0000                         2572.0000   \n",
       "\n",
       "      LinkedIn: Min Connections Founders  LinkedIn: Max Connections Founders  \\\n",
       "0                              3914.0000                           3914.0000   \n",
       "1                              6479.0000                           6479.0000   \n",
       "2                                 0.0000                              0.0000   \n",
       "3                              1893.0000                           9238.0000   \n",
       "4                                 0.0000                              0.0000   \n",
       "...                                  ...                                 ...   \n",
       "1505                            204.0000                            204.0000   \n",
       "1506                              0.0000                              0.0000   \n",
       "1507                           1116.0000                           5477.0000   \n",
       "1508                              0.0000                              0.0000   \n",
       "1509                           2091.0000                           2091.0000   \n",
       "\n",
       "      Highest Education Bachelor  Highest Education Doctor/PhD  \\\n",
       "0                         0.0000                        0.0000   \n",
       "1                         0.0000                        0.0000   \n",
       "2                         0.0000                        0.0000   \n",
       "3                         0.0000                        0.0000   \n",
       "4                         0.0000                        0.0000   \n",
       "...                          ...                           ...   \n",
       "1505                      0.0000                        0.0000   \n",
       "1506                      0.0000                        0.0000   \n",
       "1507                      0.0000                        0.0000   \n",
       "1508                      0.0000                        0.0000   \n",
       "1509                      0.0000                        0.0000   \n",
       "\n",
       "      Highest Education Master  International Team  Top University  \\\n",
       "0                       1.0000                   0          0.0000   \n",
       "1                       1.0000                   1          0.5000   \n",
       "2                       0.0000                   1          0.0000   \n",
       "3                       2.0000                   1          0.3333   \n",
       "4                       0.0000                   1          0.0000   \n",
       "...                        ...                 ...             ...   \n",
       "1505                    1.0000                   1          0.0000   \n",
       "1506                    0.0000                   1          0.0000   \n",
       "1507                    2.0000                   1          0.2500   \n",
       "1508                    0.0000                   1          0.0000   \n",
       "1509                    0.0000                   0          0.0000   \n",
       "\n",
       "      Studies Abroad Founder  Firsttime Founder Ratio  All Firsttime Founders  \\\n",
       "0                     0.0000                   0.0000                  0.0000   \n",
       "1                     0.5000                   0.0000                  0.0000   \n",
       "2                     0.0000                   0.0000                  0.0000   \n",
       "3                     0.0000                   0.3333                  0.0000   \n",
       "4                     0.0000                   0.0000                  0.0000   \n",
       "...                      ...                      ...                     ...   \n",
       "1505                  0.0000                   1.0000                  1.0000   \n",
       "1506                  0.0000                   0.0000                  0.0000   \n",
       "1507                  0.2500                   1.0000                  1.0000   \n",
       "1508                  0.0000                   0.0000                  0.0000   \n",
       "1509                  0.0000                   1.0000                  1.0000   \n",
       "\n",
       "      Any Firsttime Founder  Researcher Ratio  All Researchers  \\\n",
       "0                    0.0000            0.0000           0.0000   \n",
       "1                    0.0000            1.0000           1.0000   \n",
       "2                    0.0000            0.0000           0.0000   \n",
       "3                    1.0000            0.6667           0.0000   \n",
       "4                    0.0000            0.0000           0.0000   \n",
       "...                     ...               ...              ...   \n",
       "1505                 1.0000            0.0000           0.0000   \n",
       "1506                 0.0000            0.0000           0.0000   \n",
       "1507                 1.0000            0.3333           0.0000   \n",
       "1508                 0.0000            0.0000           0.0000   \n",
       "1509                 1.0000            0.0000           0.0000   \n",
       "\n",
       "      Any Researcher  Executive Ratio  All Executives  Any Executive  \\\n",
       "0             0.0000           0.0000          0.0000         0.0000   \n",
       "1             1.0000           0.0000          0.0000         0.0000   \n",
       "2             0.0000           0.0000          0.0000         0.0000   \n",
       "3             1.0000           0.6667          0.0000         1.0000   \n",
       "4             0.0000           0.0000          0.0000         0.0000   \n",
       "...              ...              ...             ...            ...   \n",
       "1505          0.0000           0.0000          0.0000         0.0000   \n",
       "1506          0.0000           0.0000          0.0000         0.0000   \n",
       "1507          1.0000           0.3333          0.0000         1.0000   \n",
       "1508          0.0000           0.0000          0.0000         0.0000   \n",
       "1509          0.0000           0.0000          0.0000         0.0000   \n",
       "\n",
       "      Few Years Experience Ratio  Decade Experience Ratio  \\\n",
       "0                         0.0000                   1.0000   \n",
       "1                         0.0000                   1.0000   \n",
       "2                         0.0000                   0.0000   \n",
       "3                         0.0000                   0.6667   \n",
       "4                         0.0000                   0.0000   \n",
       "...                          ...                      ...   \n",
       "1505                      0.0000                   0.0000   \n",
       "1506                      0.0000                   0.0000   \n",
       "1507                      0.0000                   1.0000   \n",
       "1508                      0.0000                   0.0000   \n",
       "1509                      0.0000                   0.0000   \n",
       "\n",
       "      Mid Career Experience Ratio  Avg Gaps in Experience  \\\n",
       "0                          0.0000                  0.0000   \n",
       "1                          0.0000                  7.0000   \n",
       "2                          0.0000                  0.0000   \n",
       "3                          0.3333                 15.6667   \n",
       "4                          0.0000                  0.0000   \n",
       "...                           ...                     ...   \n",
       "1505                       1.0000                 13.0000   \n",
       "1506                       0.0000                  0.0000   \n",
       "1507                       0.0000                  2.6667   \n",
       "1508                       0.0000                  0.0000   \n",
       "1509                       1.0000                  0.0000   \n",
       "\n",
       "      Avg Longest Position Duration  Success  \n",
       "0                          150.1667        0  \n",
       "1                           82.2333        0  \n",
       "2                            0.0000        0  \n",
       "3                           99.0556        0  \n",
       "4                            0.0000        0  \n",
       "...                             ...      ...  \n",
       "1505                        56.8333        0  \n",
       "1506                         0.0000        0  \n",
       "1507                       151.8889        0  \n",
       "1508                         0.0000        0  \n",
       "1509                        97.4000        0  \n",
       "\n",
       "[1510 rows x 114 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Number of Founders</th>\n",
       "      <th>Number of Employees</th>\n",
       "      <th>Number of Funding Rounds</th>\n",
       "      <th>Last Funding Amount</th>\n",
       "      <th>Total Funding Amount</th>\n",
       "      <th>Number of Lead Investors</th>\n",
       "      <th>Number of Investors</th>\n",
       "      <th>Number of Acquisitions</th>\n",
       "      <th>X: Followers</th>\n",
       "      <th>X: Following</th>\n",
       "      <th>X: Number of Tweets</th>\n",
       "      <th>X: Account Age Days</th>\n",
       "      <th>X: Tweet Activity</th>\n",
       "      <th>X: Followers Max Growth</th>\n",
       "      <th>X: Followers Max Loss</th>\n",
       "      <th>X: Tweets Max Growth</th>\n",
       "      <th>X: Tweets Max Loss</th>\n",
       "      <th>Months until First Round</th>\n",
       "      <th>Grant Y/N</th>\n",
       "      <th>Made Acquisitions</th>\n",
       "      <th>Project Funding</th>\n",
       "      <th>Startup Funding</th>\n",
       "      <th>Growth Funding</th>\n",
       "      <th>Expansion Funding</th>\n",
       "      <th>Exit Funding</th>\n",
       "      <th>Average Time To Next Round</th>\n",
       "      <th>Average Funding Size</th>\n",
       "      <th>Average Number of Investments by Investors</th>\n",
       "      <th>Average Number of Exits by Investors</th>\n",
       "      <th>Average Number of Lead Investments by Investors</th>\n",
       "      <th>Average Number of Portfolio Organizations by Investors</th>\n",
       "      <th>Investor Country_AUS</th>\n",
       "      <th>Investor Country_GER</th>\n",
       "      <th>Investor Country_SWI</th>\n",
       "      <th>Investor Country_UK</th>\n",
       "      <th>Investor Country_USA</th>\n",
       "      <th>Top Investor Participation</th>\n",
       "      <th>Industry_administrative services</th>\n",
       "      <th>Industry_advertising</th>\n",
       "      <th>Industry_agriculture and farming</th>\n",
       "      <th>Industry_apps</th>\n",
       "      <th>Industry_artificial intelligence (ai)</th>\n",
       "      <th>Industry_biotechnology</th>\n",
       "      <th>Industry_blockchain and cryptocurrency</th>\n",
       "      <th>Industry_clothing and apparel</th>\n",
       "      <th>Industry_commerce and shopping</th>\n",
       "      <th>Industry_community and lifestyle</th>\n",
       "      <th>Industry_consumer electronics</th>\n",
       "      <th>Industry_consumer goods</th>\n",
       "      <th>Industry_content and publishing</th>\n",
       "      <th>Industry_data and analytics</th>\n",
       "      <th>Industry_design</th>\n",
       "      <th>Industry_education</th>\n",
       "      <th>Industry_energy</th>\n",
       "      <th>Industry_events</th>\n",
       "      <th>Industry_financial services</th>\n",
       "      <th>Industry_food and beverage</th>\n",
       "      <th>Industry_gaming</th>\n",
       "      <th>Industry_government and military</th>\n",
       "      <th>Industry_hardware</th>\n",
       "      <th>Industry_health care</th>\n",
       "      <th>Industry_information technology</th>\n",
       "      <th>Industry_internet services</th>\n",
       "      <th>Industry_lending and investments</th>\n",
       "      <th>Industry_manufacturing</th>\n",
       "      <th>Industry_media and entertainment</th>\n",
       "      <th>Industry_messaging and telecommunications</th>\n",
       "      <th>Industry_mobile</th>\n",
       "      <th>Industry_music and audio</th>\n",
       "      <th>Industry_natural resources</th>\n",
       "      <th>Industry_navigation and mapping</th>\n",
       "      <th>Industry_other</th>\n",
       "      <th>Industry_payments</th>\n",
       "      <th>Industry_platforms</th>\n",
       "      <th>Industry_privacy and security</th>\n",
       "      <th>Industry_professional services</th>\n",
       "      <th>Industry_real estate</th>\n",
       "      <th>Industry_sales and marketing</th>\n",
       "      <th>Industry_science and engineering</th>\n",
       "      <th>Industry_social impact</th>\n",
       "      <th>Industry_software</th>\n",
       "      <th>Industry_sports</th>\n",
       "      <th>Industry_sustainability</th>\n",
       "      <th>Industry_transportation</th>\n",
       "      <th>Industry_travel and tourism</th>\n",
       "      <th>Industry_video</th>\n",
       "      <th>LinkedIn: Average Followers Founders</th>\n",
       "      <th>LinkedIn: Average Connections Founders</th>\n",
       "      <th>LinkedIn: Min Followers Founders</th>\n",
       "      <th>LinkedIn: Max Followers Founders</th>\n",
       "      <th>LinkedIn: Min Connections Founders</th>\n",
       "      <th>LinkedIn: Max Connections Founders</th>\n",
       "      <th>Highest Education Bachelor</th>\n",
       "      <th>Highest Education Doctor/PhD</th>\n",
       "      <th>Highest Education Master</th>\n",
       "      <th>International Team</th>\n",
       "      <th>Top University</th>\n",
       "      <th>Studies Abroad Founder</th>\n",
       "      <th>Firsttime Founder Ratio</th>\n",
       "      <th>All Firsttime Founders</th>\n",
       "      <th>Any Firsttime Founder</th>\n",
       "      <th>Researcher Ratio</th>\n",
       "      <th>All Researchers</th>\n",
       "      <th>Any Researcher</th>\n",
       "      <th>Executive Ratio</th>\n",
       "      <th>All Executives</th>\n",
       "      <th>Any Executive</th>\n",
       "      <th>Few Years Experience Ratio</th>\n",
       "      <th>Decade Experience Ratio</th>\n",
       "      <th>Mid Career Experience Ratio</th>\n",
       "      <th>Avg Gaps in Experience</th>\n",
       "      <th>Avg Longest Position Duration</th>\n",
       "      <th>Success</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>30.5000</td>\n",
       "      <td>2</td>\n",
       "      <td>7000000.0000</td>\n",
       "      <td>7000000.0000</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>4.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>40.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>31.5500</td>\n",
       "      <td>7000000.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>182.0000</td>\n",
       "      <td>220.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>4815.0000</td>\n",
       "      <td>3914.0000</td>\n",
       "      <td>4815.0000</td>\n",
       "      <td>4815.0000</td>\n",
       "      <td>3914.0000</td>\n",
       "      <td>3914.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>150.1667</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>30.5000</td>\n",
       "      <td>2</td>\n",
       "      <td>3000000.0000</td>\n",
       "      <td>3000000.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>15.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16.4333</td>\n",
       "      <td>3000000.0000</td>\n",
       "      <td>321.0000</td>\n",
       "      <td>32.0000</td>\n",
       "      <td>49.5000</td>\n",
       "      <td>277.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>6482.0000</td>\n",
       "      <td>6479.0000</td>\n",
       "      <td>6482.0000</td>\n",
       "      <td>6482.0000</td>\n",
       "      <td>6479.0000</td>\n",
       "      <td>6479.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>7.0000</td>\n",
       "      <td>82.2333</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>30.5000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>29.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29.4000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>36.0000</td>\n",
       "      <td>7.0000</td>\n",
       "      <td>19.0000</td>\n",
       "      <td>29.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>5.5000</td>\n",
       "      <td>1</td>\n",
       "      <td>1000000.0000</td>\n",
       "      <td>1000000.0000</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>269.0000</td>\n",
       "      <td>346.0000</td>\n",
       "      <td>248.0000</td>\n",
       "      <td>3389.0000</td>\n",
       "      <td>0.0700</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>-3.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-2.0000</td>\n",
       "      <td>-1.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>5664.6667</td>\n",
       "      <td>5337.6667</td>\n",
       "      <td>2089.0000</td>\n",
       "      <td>9782.0000</td>\n",
       "      <td>1893.0000</td>\n",
       "      <td>9238.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.3333</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.3333</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.6667</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.6667</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.6667</td>\n",
       "      <td>0.3333</td>\n",
       "      <td>15.6667</td>\n",
       "      <td>99.0556</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>4.0000</td>\n",
       "      <td>30.5000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>32.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32.4333</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1505</th>\n",
       "      <td>1506</td>\n",
       "      <td>4.0000</td>\n",
       "      <td>5.5000</td>\n",
       "      <td>2</td>\n",
       "      <td>36000.0000</td>\n",
       "      <td>161000.0000</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>102.0000</td>\n",
       "      <td>438.0000</td>\n",
       "      <td>169.0000</td>\n",
       "      <td>2499.0000</td>\n",
       "      <td>0.0700</td>\n",
       "      <td>4.0000</td>\n",
       "      <td>-2.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-2.0000</td>\n",
       "      <td>4.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11.6333</td>\n",
       "      <td>80500.0000</td>\n",
       "      <td>28.0000</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>8.5000</td>\n",
       "      <td>27.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>202.0000</td>\n",
       "      <td>204.0000</td>\n",
       "      <td>202.0000</td>\n",
       "      <td>202.0000</td>\n",
       "      <td>204.0000</td>\n",
       "      <td>204.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>56.8333</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1506</th>\n",
       "      <td>1507</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>30.5000</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>60000.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>59.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>34.3667</td>\n",
       "      <td>30000.0000</td>\n",
       "      <td>4.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1507</th>\n",
       "      <td>1508</td>\n",
       "      <td>4.0000</td>\n",
       "      <td>30.5000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>4.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>232.0000</td>\n",
       "      <td>482.0000</td>\n",
       "      <td>786.0000</td>\n",
       "      <td>2467.0000</td>\n",
       "      <td>0.3200</td>\n",
       "      <td>29.0000</td>\n",
       "      <td>-42.0000</td>\n",
       "      <td>104.0000</td>\n",
       "      <td>-8.0000</td>\n",
       "      <td>4.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21.6833</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>58.6667</td>\n",
       "      <td>6.0000</td>\n",
       "      <td>23.3333</td>\n",
       "      <td>41.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2646.6667</td>\n",
       "      <td>2589.0000</td>\n",
       "      <td>1176.0000</td>\n",
       "      <td>5489.0000</td>\n",
       "      <td>1116.0000</td>\n",
       "      <td>5477.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.3333</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.3333</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2.6667</td>\n",
       "      <td>151.8889</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1508</th>\n",
       "      <td>1509</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>5.5000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>69.0000</td>\n",
       "      <td>298.0000</td>\n",
       "      <td>71.0000</td>\n",
       "      <td>3456.0000</td>\n",
       "      <td>0.0200</td>\n",
       "      <td>4.0000</td>\n",
       "      <td>-4.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-1.0000</td>\n",
       "      <td>14.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20.3833</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>53.0000</td>\n",
       "      <td>8.0000</td>\n",
       "      <td>27.0000</td>\n",
       "      <td>53.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1509</th>\n",
       "      <td>1510</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>30.5000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>9.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.3667</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2572.0000</td>\n",
       "      <td>2091.0000</td>\n",
       "      <td>2572.0000</td>\n",
       "      <td>2572.0000</td>\n",
       "      <td>2091.0000</td>\n",
       "      <td>2091.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>97.4000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1510 rows × 114 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 90
  },
  {
   "cell_type": "markdown",
   "id": "6c0f1dc5615ec0d6",
   "metadata": {},
   "source": [
    "## Train-Test-Split"
   ]
  },
  {
   "cell_type": "code",
   "id": "209dc639c5059b2f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T19:36:26.735076Z",
     "start_time": "2025-01-15T19:36:26.694024Z"
    }
   },
   "source": [
    "ID = data['ID']\n",
    "X = data.drop(columns=['Success', 'ID'])\n",
    "y = data['Success']\n",
    "\n",
    "# Train-Test Split\n",
    "X_train, X_test, y_train, y_test, ID_train, ID_test = train_test_split(\n",
    "    X, y, ID, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Apply SMOTE on the training data\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "additional_ids = len(y_train_resampled) - len(ID_train)\n",
    "\n",
    "new_ids = np.arange(len(ID_train), len(ID_train) + additional_ids)\n",
    "\n",
    "ID_train_resampled = np.concatenate([ID_train, new_ids])\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_resampled)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ],
   "outputs": [],
   "execution_count": 91
  },
  {
   "cell_type": "markdown",
   "id": "85d6efb7",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ee23bf",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "id": "fab62c93",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T19:36:26.811800Z",
     "start_time": "2025-01-15T19:36:26.767809Z"
    }
   },
   "source": [
    "# Logistic Regression\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning) # No influence on the result and just for a cleaner output\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning) # No influence on the result and just for a cleaner output\n",
    "logistic_model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "logistic_model.fit(X_train_scaled, y_train_resampled)\n",
    "logistic_predictions = logistic_model.predict(X_test_scaled)\n",
    "logistic_proba = logistic_model.predict_proba(X_test_scaled)[:, 1]\n",
    "logistic_auc = roc_auc_score(y_test, logistic_proba)\n",
    "logistic_report = classification_report(y_test, logistic_predictions, output_dict=True)\n",
    "\n",
    "print('Test AUC:', logistic_auc)\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test, logistic_predictions))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test AUC: 0.7910131090399333\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.76      0.84       249\n",
      "           1       0.39      0.70      0.50        53\n",
      "\n",
      "    accuracy                           0.75       302\n",
      "   macro avg       0.65      0.73      0.67       302\n",
      "weighted avg       0.83      0.75      0.78       302\n",
      "\n"
     ]
    }
   ],
   "execution_count": 92
  },
  {
   "cell_type": "markdown",
   "id": "153478c3",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "id": "217fb1c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T19:39:21.629311Z",
     "start_time": "2025-01-15T19:36:26.877777Z"
    }
   },
   "source": [
    "# Gradient Boosting\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'learning_rate': [0.03, 0.035],  \n",
    "    'n_estimators': [400, 450],  \n",
    "    'max_depth': [5, 6],  \n",
    "    'min_samples_split': [8, 10],  \n",
    "    'min_samples_leaf': [6, 7],  \n",
    "    'subsample': [0.6, 0.7],  \n",
    "    'max_features': ['sqrt', 'log2'],  \n",
    "}\n",
    "\n",
    "# Initialize the Gradient Boosting Classifier\n",
    "gb_model = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Define the scoring metric\n",
    "scorer = make_scorer(roc_auc_score, needs_proba=True)\n",
    "\n",
    "# Set up GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=gb_model,\n",
    "    param_grid=param_grid,\n",
    "    scoring=scorer,\n",
    "    cv=10,  # Stratified K-Fold Cross-Validation\n",
    "    verbose=2,\n",
    "    n_jobs=-1  # Use all available cores\n",
    ")\n",
    "\n",
    "# Fit GridSearchCV to the data\n",
    "grid_search.fit(X_train_scaled, y_train_resampled)\n",
    "\n",
    "# Get the best parameters and score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print(\"Best Parameters:\", best_params)\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "best_gb_model = grid_search.best_estimator_\n",
    "test_predictions = best_gb_model.predict(X_test_scaled)\n",
    "test_auc = roc_auc_score(y_test, best_gb_model.predict_proba(X_test_scaled)[:, 1])\n",
    "\n",
    "print(\"Test AUC Score:\", test_auc)\n",
    "print(\"Classification Report on Test Set:\")\n",
    "print(classification_report(y_test, test_predictions))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 128 candidates, totalling 1280 fits\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.7s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.2s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.7s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.2s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   0.7s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   0.7s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.7s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.7s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.2s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.4s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.4s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.5s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.2s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.4s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.2s[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.6s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.2s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.7s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   0.7s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   0.7s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.7s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.4s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.5s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.5s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.2s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.4s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.2s[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.7s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.7s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   0.7s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.7s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.7s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.4s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.4s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.5s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.4s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.2s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.4s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.2s[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.6s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.3s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   0.7s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   0.7s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   0.7s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.7s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.7s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.7s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.2s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.4s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.4s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.5s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.2s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.2s[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.4s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.7s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.7s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.7s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.4s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.4s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.4s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.5s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.3s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.2s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.4s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.2s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.2s[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.7s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.7s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   0.7s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.7s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   0.7s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.7s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.5s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.4s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.5s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.5s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.2s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.4s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.1s[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.7s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.4s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.6s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   0.7s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   0.7s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.7s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.7s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.4s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.5s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.5s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.2s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.4s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.4s[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.6s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.2s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   0.7s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.7s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.5s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.4s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.5s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.2s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.2s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.4s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.2s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.2s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.6s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.4s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.6s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.4s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.4s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.5s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.4s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.4s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.7s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.7s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.4s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.2s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.4s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.6s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.4s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.4s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.7s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.4s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.3s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.6s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.4s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.4s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.7s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.4s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.3s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.4s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.4s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.4s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.4s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.4s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.4s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.7s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.7s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.5s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.4s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.4s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.4s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.4s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.4s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.5s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.7s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.5s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.3s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.4s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.2s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.4s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.4s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.5s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.4s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.9s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/janlinzner/Library/Mobile Documents/com~apple~CloudDocs/Documents/McGill/1_Lectures/Decision Analytics (MGSC 662)/2_Coding/Exercises/venv/lib/python3.12/site-packages/numpy/ma/core.py:2846: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'learning_rate': 0.035, 'max_depth': 6, 'max_features': 'sqrt', 'min_samples_leaf': 7, 'min_samples_split': 8, 'n_estimators': 450, 'subsample': 0.6}\n",
      "Test AUC Score: 0.8320830491778435\n",
      "Classification Report on Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.95      0.93       249\n",
      "           1       0.70      0.53      0.60        53\n",
      "\n",
      "    accuracy                           0.88       302\n",
      "   macro avg       0.80      0.74      0.76       302\n",
      "weighted avg       0.87      0.88      0.87       302\n",
      "\n"
     ]
    }
   ],
   "execution_count": 93
  },
  {
   "cell_type": "markdown",
   "id": "2522117d",
   "metadata": {},
   "source": [
    "### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "id": "116d54aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T20:35:46.704660Z",
     "start_time": "2025-01-15T19:39:21.807202Z"
    }
   },
   "source": [
    "# LightGBM\n",
    "# Define the hyperparameter grid for LightGBM\n",
    "param_grid_lgb = {\n",
    "    'learning_rate': [0.02, 0.025],          \n",
    "    'n_estimators': [300, 350],             \n",
    "    'min_child_samples': [4, 6, 8],        \n",
    "    'num_leaves': [15, 20],                 \n",
    "    'subsample': [0.5, 0.55],               \n",
    "    'colsample_bytree': [0.6],             \n",
    "    'reg_alpha': [0.5, 1.0],                \n",
    "    'reg_lambda': [3.0],                   \n",
    "    'scale_pos_weight': [1.0, 1.5, 2.0],    \n",
    "}\n",
    "# Initialize the LightGBM Classifier\n",
    "lgb_model = LGBMClassifier(random_state=42)\n",
    "\n",
    "# Set up GridSearchCV for LightGBM\n",
    "grid_search_lgb = GridSearchCV(\n",
    "    estimator=lgb_model,\n",
    "    param_grid=param_grid_lgb,\n",
    "    scoring=scorer,  # Use AUC as the scoring metric\n",
    "    cv=5,  \n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit GridSearchCV to the data\n",
    "grid_search_lgb.fit(X_train_scaled, y_train_resampled)\n",
    "\n",
    "# Get the best parameters and score for LightGBM\n",
    "best_params_lgb = grid_search_lgb.best_params_\n",
    "best_score_lgb = grid_search_lgb.best_score_\n",
    "\n",
    "print(\"Best Parameters (LightGBM):\", best_params_lgb)\n",
    "\n",
    "# Evaluate the best LightGBM model on the test set\n",
    "best_lgb_model = grid_search_lgb.best_estimator_\n",
    "lgb_predictions = best_lgb_model.predict(X_test_scaled)\n",
    "lgb_proba = best_lgb_model.predict_proba(X_test_scaled)[:, 1]\n",
    "lgb_test_auc = roc_auc_score(y_test, lgb_proba)\n",
    "\n",
    "print(\"Test AUC Score (LightGBM):\", lgb_test_auc)\n",
    "print(\"Classification Report (LightGBM):\\n\", classification_report(y_test, lgb_predictions))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 288 candidates, totalling 1440 fits\n",
      "\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.6s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.4s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.3s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.4s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.5s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.7s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.4s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.5s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.2s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.4s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.4s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.4s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.4s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.4s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.7s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.4s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.7s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.6s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.4s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.5s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   0.7s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.4s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.4s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.4s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.4s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.4s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.6s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.4s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.4s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.7s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.4s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.5s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.5s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.5s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.5s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.5s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.4s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.4s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.4s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.8s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.4s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.6s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.6s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.4s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.4s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.5s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.5s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.4s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.5s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.4s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.4s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.8s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.5s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.5s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.6s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.5s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.4s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.5s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.5s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.4s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.5s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.4s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.8s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.6s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.5s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.4s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011103 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.5s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.5s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.4s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.4s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.5s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.6s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.7s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.4s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.5s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.4s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.4s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.4s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.7s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.4s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.4s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.4s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.5s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.4s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.4s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.6s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.8s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.4s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.6s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.5s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.5s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.4s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.4s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.4s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.4s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.6s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.6s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.7s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.5s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.5s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.5s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.4s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.4s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010955 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011007 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   3.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005734 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   3.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005245 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   3.5s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004785 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005170 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   3.9s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007932 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007325 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   3.9s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007133 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005224 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004740 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   5.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005395 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.4s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005230 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.0s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010951 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   3.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004659 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   3.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006634 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   3.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006084 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006623 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007055 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004535 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010336 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   3.9s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004843 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006201 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   5.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006010 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012913 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008798 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   3.4s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003846 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   3.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004914 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   3.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015869 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005424 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006800 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.6s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.126775 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.068200 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   3.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036151 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004619 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   5.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012238 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   5.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006349 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   5.8s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.3s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011456 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   3.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013790 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   3.1s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009409 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006544 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006794 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004658 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   3.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.130672 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007826 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   3.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011985 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   5.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007508 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   5.6s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.055454 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   5.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004725 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   5.7s\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006211 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005484 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   3.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007656 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006089 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   3.5s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008808 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.5s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020445 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006460 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   5.0s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005168 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.093167 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   5.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006028 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   5.2s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007547 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   5.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011265 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   3.9s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006194 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   3.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005763 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   3.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005725 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023622 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   3.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009581 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.5s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007279 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009780 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   5.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005887 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   5.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005868 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   5.8s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006010 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   5.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006241 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   5.9s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007355 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010435 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019643 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   3.2s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004999 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   3.8s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019370 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005828 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   3.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007045 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008336 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   3.9s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005815 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   5.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005594 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   5.0s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005602 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   5.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007899 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   5.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005970 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   5.8s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.075499 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011363 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   3.8s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006944 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   3.5s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010625 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   3.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004535 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007844 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006431 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.046241 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011405 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006899 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   5.2s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005211 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   5.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006144 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   5.3s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007849 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   5.8s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010100 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   6.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013727 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   5.3s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007176 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017925 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005294 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009107 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005206 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004843 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   3.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004903 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   3.8s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011920 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   3.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033045 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   3.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011164 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.9s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004899 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   6.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010752 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   5.3s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014058 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035883 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005720 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006762 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005738 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.5s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005284 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007162 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007224 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.5s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023732 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   3.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012854 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017239 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   6.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005662 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   5.1s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008697 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.024380 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008740 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006134 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006192 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006487 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005839 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.4s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007792 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008844 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   5.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020658 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   6.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.134515 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   6.0s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.100696 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   5.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005362 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035106 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034474 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004076 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006049 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013902 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   3.5s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.026847 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005436 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006743 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.5s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007912 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   6.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006527 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   5.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014953 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   5.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020722 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   5.0s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006537 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006143 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005470 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005009 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   3.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006389 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005557 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005510 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.5s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012196 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.034254 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   6.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   5.0s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004054 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   5.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014284 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.5s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006201 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004993 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005546 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005113 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006005 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005308 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005491 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007268 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   6.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004637 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   6.0s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006559 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   5.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007500 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   5.2s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017152 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.016381 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006776 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009915 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007588 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.4s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007365 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007058 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006057 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008511 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   5.9s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005548 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   5.4s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.046540 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   5.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006198 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   5.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006071 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008622 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006574 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.4s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006280 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005315 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020851 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.126682 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007680 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.4s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021354 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   5.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007432 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   6.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005328 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   5.2s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.025268 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   6.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010788 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   6.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004844 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   5.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005654 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   6.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005454 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   5.8s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006189 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   5.5s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017672 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   6.1s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006009 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   3.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007743 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018273 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   3.9s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.085930 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   3.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.049187 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   6.0s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007027 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   6.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004699 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   6.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009534 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   6.4s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.093986 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   5.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005962 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   6.1s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014784 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   5.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004433 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   3.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012829 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005595 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005689 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006132 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   3.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009239 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005442 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   5.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005468 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   5.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007312 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   6.2s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062768 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   5.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.100670 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.122505 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   5.1s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.066210 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004945 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.0s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.049019 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   3.4s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007285 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   3.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010140 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012920 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013080 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   6.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006818 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   6.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005608 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   5.4s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006763 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   6.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.043403 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   5.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.024979 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   6.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004181 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017251 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   3.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004294 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   3.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005400 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   3.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018928 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   3.9s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007911 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   3.8s\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   6.2s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006157 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   6.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032160 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   5.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008050 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   5.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008980 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   5.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009423 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   6.2s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006864 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   3.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005954 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   3.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.038644 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   3.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.038864 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   3.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007004 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006296 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   3.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005834 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   6.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014443 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   6.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.044388 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   5.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006611 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   6.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019646 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   5.0s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.106211 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   5.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008473 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007401 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   3.9s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008686 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006853 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.027608 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015083 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   3.8s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017517 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   5.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005441 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   6.0s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008923 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   6.4s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005167 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   6.1s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005607 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   5.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005423 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   3.9s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008470 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015804 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004641 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013028 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004481 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.151445 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   3.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011059 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   3.8s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008224 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   6.5s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.115537 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   6.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008924 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   6.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005721 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   6.4s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004615 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.049936 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   6.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006714 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006473 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   3.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006270 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.0s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017501 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   3.8s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006014 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005611 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   3.5s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006335 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.4s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006886 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017875 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   3.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014868 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   5.2s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005206 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015053 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016061 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009915 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   5.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006487 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   5.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005594 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   5.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.077224 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004784 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   3.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006119 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   3.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006639 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005696 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   5.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006128 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   5.3s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005412 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   5.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004564 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   5.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006965 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   5.6s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006222 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   5.5s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023866 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004952 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004390 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   3.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034877 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008804 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.0s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005382 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006398 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007045 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   5.2s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009799 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005603 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.6s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005238 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   5.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005137 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   5.8s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019837 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   5.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006219 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005921 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.064736 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   5.1s\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   3.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004635 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   3.8s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006329 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005155 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   5.4s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010892 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   5.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007939 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   5.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007090 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   5.3s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012090 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   5.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005013 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006405 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   5.8s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004676 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018617 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004559 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032244 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   5.2s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005832 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   5.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.052830 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   5.4s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006492 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   5.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.046534 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006770 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   5.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006030 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022703 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005428 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.5s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006231 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.5s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032413 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   5.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005799 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.0s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005513 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   5.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004767 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018597 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   5.3s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007873 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   5.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006146 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.083410 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   5.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005956 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   5.5s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006606 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006723 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004617 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007317 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.5s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008607 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006009 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   5.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005922 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   5.1s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010478 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005747 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004403 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005533 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   5.5s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013154 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   5.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007103 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   5.2s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.029220 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005579 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006358 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005671 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012130 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   5.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.016334 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   5.0s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022203 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   5.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006048 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   5.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018764 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   5.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008106 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   5.4s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007290 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   5.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009362 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.9s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006419 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006368 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031301 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.6s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006789 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005739 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008274 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   5.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010391 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019725 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.5s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005549 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.038653 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   5.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005274 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   6.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005948 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   6.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.079143 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   6.6s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008810 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   6.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005881 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   6.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007594 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   5.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007611 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.169220 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034944 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.079624 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   3.9s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005169 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005715 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   5.2s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005137 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   6.3s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032663 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   6.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005583 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   6.4s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008101 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   6.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006141 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   6.2s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005269 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   6.5s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.028332 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   6.3s\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.5s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005984 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008268 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007169 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.047479 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009635 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   6.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019143 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   5.3s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008776 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   6.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012910 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   6.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005495 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   6.2s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005887 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   6.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012568 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   6.2s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005803 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.0s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006578 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   3.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009307 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   3.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005823 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007483 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.5s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022328 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   6.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004774 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   6.3s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008567 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   6.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005830 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   6.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009727 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   6.6s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005125 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   6.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027846 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   5.5s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009048 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006968 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008055 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013120 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   5.1s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006072 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   5.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005338 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   5.8s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004467 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   5.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005869 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   6.3s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.035614 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   5.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012089 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   5.9s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006086 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   6.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006465 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   6.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019564 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   5.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007272 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004399 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   3.4s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.062059 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005346 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007959 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   5.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007345 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   5.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011193 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   5.9s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005124 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   6.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007158 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   6.4s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006402 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   6.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006124 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   6.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008568 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   6.2s\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.0s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007050 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   3.8s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008933 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006609 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007927 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   6.0s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007506 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   5.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005535 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   6.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006402 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   6.6s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006306 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   6.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005966 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   6.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006178 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   6.5s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006098 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   6.1s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005854 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005700 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006091 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.5s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005496 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   5.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010118 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   6.2s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017530 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   6.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012957 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   6.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.025964 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   6.3s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015615 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   6.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.049757 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   6.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005882 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   6.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011858 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   3.9s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006935 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   3.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013072 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007866 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   3.4s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005495 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   3.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006374 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005050 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.0s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.038096 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005848 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   3.8s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036483 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   3.9s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005954 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   5.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007809 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   5.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.027779 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   5.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008092 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.6s\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   6.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007822 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   5.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006160 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   3.3s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005575 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008192 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006924 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019003 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   3.9s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008972 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   3.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007090 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.0s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006951 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   5.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007224 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   5.5s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006275 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006228 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   3.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005279 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   3.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003965 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.0s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005169 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   3.4s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007256 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006114 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007093 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005487 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027508 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017128 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   5.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013607 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   5.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008552 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   5.3s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007003 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003711 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   3.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005220 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   3.8s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005151 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   3.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047075 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.068123 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.6s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005662 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006097 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006086 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   5.3s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005940 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   5.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012969 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   5.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005992 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   5.0s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004621 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   5.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008254 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   3.9s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005332 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   3.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005322 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006333 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017162 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.6s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021656 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   3.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003234 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   3.6s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005398 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   3.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.016284 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   5.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007135 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   5.3s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007526 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   5.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009293 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   5.5s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007777 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   3.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005296 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   3.6s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007102 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006461 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009014 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006946 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   3.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006408 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.129356 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   5.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005679 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   5.3s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003669 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   5.3s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005586 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   5.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004963 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   5.2s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014791 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   6.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010655 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   3.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.042281 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.0s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011006 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   3.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.034238 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009139 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013260 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   3.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004889 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   3.4s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021328 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006129 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   5.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005886 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   5.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005065 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   5.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.084212 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010121 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   3.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.049529 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   3.3s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005007 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008801 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.0s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002764 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   3.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010529 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   3.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005865 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   5.2s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006626 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   5.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008173 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   5.2s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010465 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   5.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005081 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   5.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007149 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   5.5s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.033194 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   5.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017750 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   5.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006491 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.4s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020188 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   5.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006860 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.0s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007006 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006434 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.122013 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.5s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023053 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   3.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014394 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.0s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017723 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.024542 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   5.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005785 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   5.0s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006664 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   5.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009452 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   5.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012042 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014097 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.0s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007250 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006302 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.120586 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031662 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003342 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015920 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   5.8s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006965 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006520 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029535 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.6s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007596 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.129531 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010416 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.099332 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   3.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005278 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005768 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.6s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.047235 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   3.8s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005247 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006355 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   5.1s\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   5.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006267 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007074 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   5.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007237 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   3.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007807 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007330 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   5.0s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.051941 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009475 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005442 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   3.9s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006652 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005887 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.0s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007591 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   5.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005765 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   5.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009030 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   5.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019509 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   5.2s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005970 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.5s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004588 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   3.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006925 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013826 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017080 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.087895 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004985 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.099433 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   6.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007573 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   5.9s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006337 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   5.0s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006225 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   5.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006842 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   5.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007122 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   5.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012135 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006313 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015638 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.9s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010875 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   3.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005700 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.5s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011996 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010381 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006092 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   5.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007037 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   5.9s\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   5.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012465 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.4s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006535 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   5.2s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005714 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   3.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005939 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.5s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004874 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006456 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   3.9s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007294 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007030 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   3.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005080 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.4s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005272 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   5.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005313 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   6.2s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005981 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021599 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   5.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006164 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.070809 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015032 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007845 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.6s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010104 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.028748 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008546 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005262 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013404 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   5.4s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006239 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   5.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009116 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   5.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006945 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   5.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006004 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   6.0s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015354 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   5.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.023422 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   6.3s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018907 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   6.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007489 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   6.4s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012851 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   6.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008891 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005149 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   3.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005566 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007249 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006639 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007217 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   5.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010950 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   5.1s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005772 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   6.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.023854 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   6.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.033922 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   6.5s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.034134 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   6.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.023564 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010322 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   3.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006374 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.0s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007234 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.0s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006053 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   3.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011566 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   3.3s\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   6.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005782 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   5.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006980 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   6.1s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006870 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   6.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007498 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   6.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006625 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   6.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005436 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   6.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009268 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.033508 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   3.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012272 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   3.4s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005814 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005179 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   3.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005403 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   6.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004995 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   5.6s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005551 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   6.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005929 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   6.1s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007770 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   6.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007160 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   7.0s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005260 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   5.6s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004750 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004538 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   3.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006692 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008114 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006714 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008540 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   5.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021910 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   6.2s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007761 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   6.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022671 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   6.3s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007345 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   6.6s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010330 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   6.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006459 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   3.8s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.105680 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   3.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005956 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   3.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005263 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   3.6s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007235 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   3.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005497 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022189 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015275 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   5.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005680 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   6.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.130585 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   6.5s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006257 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   6.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006120 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   6.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006090 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.026909 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   3.5s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062739 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   3.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.016185 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.0s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008245 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.0s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006037 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   3.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005410 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   3.7s\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   5.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005556 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   6.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010158 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   6.5s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005114 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   6.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012118 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   7.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.016135 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   6.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007334 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004827 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   3.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005978 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   3.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005060 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009575 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   3.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005595 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   3.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005801 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006760 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   6.4s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007341 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   6.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005548 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   6.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007590 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   7.1s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006476 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   3.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008106 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   3.9s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005471 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   3.9s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009467 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   3.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020204 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   3.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005160 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   3.3s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037788 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005438 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   3.3s\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.027406 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   3.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007002 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   3.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034326 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   3.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005500 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   5.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.042712 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   5.8s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006451 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   5.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007916 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   5.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006139 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   5.1s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009437 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   6.5s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005688 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=  20.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012200 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008337 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004723 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   3.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009275 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   5.4s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.029094 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   5.8s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015465 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   6.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004965 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   5.6s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010054 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005832 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   5.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006556 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005136 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=  21.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007105 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   6.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007819 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005345 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.4s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007352 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   3.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005022 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   3.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.025686 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   5.5s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004834 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   5.1s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.027498 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   6.2s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005791 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   5.4s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005548 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005548 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   5.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021950 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.6s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005242 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=  21.2s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008182 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   6.2s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006512 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.1s\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.0s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020567 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   3.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036942 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   5.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032252 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   5.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009248 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   6.1s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005886 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   5.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005377 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   5.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017224 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   6.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006633 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=  21.4s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005017 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   3.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005153 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   6.8s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008189 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002872 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   3.9s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005735 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   3.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008962 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   5.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005891 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   5.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010523 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   5.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008767 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   5.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004382 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.9s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009973 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   5.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011997 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   5.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008197 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=  21.2s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007123 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   6.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011894 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006617 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   3.8s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005434 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   5.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006670 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   5.6s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007049 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   5.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007187 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   5.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006611 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   5.1s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010672 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   6.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003486 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=  21.4s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007727 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   3.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004936 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   6.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004935 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.4s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005075 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   3.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006558 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008155 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   5.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006455 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   5.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008568 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   5.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.046736 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   5.0s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.063496 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008966 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   6.5s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.030743 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=  20.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004904 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017460 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   6.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007640 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   3.5s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006470 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011560 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=  13.6s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   3.8s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004909 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.6s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007226 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   5.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006720 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   5.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004747 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   5.2s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005305 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014304 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   5.8s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004798 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   5.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004856 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=  20.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005802 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   3.4s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006695 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   6.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005809 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007174 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   6.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004717 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006136 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012273 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=  14.2s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005415 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007553 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   5.8s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010392 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time= 1.8min\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005037 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time= 2.2min\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014307 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   6.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.055034 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005354 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=  15.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004340 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   5.4s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036529 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014087 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=  14.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005365 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006211 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   5.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.016693 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time= 1.8min\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004742 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time= 2.2min\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003696 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   5.6s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005836 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   5.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014077 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=  15.0s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003469 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006927 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   5.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.131647 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=  31.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013068 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017769 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006795 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=  14.3s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006519 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005449 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   6.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013669 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time= 1.8min\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014599 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time= 2.2min\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004004 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   6.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004427 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=  16.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007918 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   5.3s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008797 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   5.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005732 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=  31.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012961 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   5.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.4s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005784 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=  13.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008243 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006150 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   6.3s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005553 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time= 1.8min\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008719 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time= 2.2min\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002330 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   6.3s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008944 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=  16.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005547 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   5.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006780 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   5.4s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005282 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=  31.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008137 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   5.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006996 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.5s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006095 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=  13.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005658 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.130800 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   6.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014363 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time= 1.8min\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005310 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time= 2.2min\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014039 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   6.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036035 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=  16.0s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008735 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   5.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032263 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   5.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005833 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=  31.3s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009396 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   5.8s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.025181 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=  14.4s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.016235 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004885 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time= 1.8min\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010650 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.0s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005647 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time= 2.2min\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007696 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   6.3s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030069 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=  14.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011047 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   5.3s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006977 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   5.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006380 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=  30.5s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005521 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017246 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   3.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005710 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002753 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=  14.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.063791 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   6.3s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020551 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time= 1.8min\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005788 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time= 2.2min\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005356 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   5.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008326 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   5.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015947 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=  15.1s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013529 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   5.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005823 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=  31.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011852 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   6.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005864 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   5.3s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010698 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.048263 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004313 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   6.5s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005973 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time= 1.8min\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022551 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time= 2.2min\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004485 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   6.5s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006636 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   5.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 10.242935 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=  15.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007210 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   5.1s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005814 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=  30.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005982 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   6.1s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006486 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   5.5s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004345 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=  31.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007916 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   5.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006656 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   5.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006611 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   3.6s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.045713 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   3.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006073 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   3.6s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007056 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   3.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010866 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016724 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   3.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010052 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024970 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006131 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   6.1s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013302 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   5.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006779 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   3.5s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004974 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   3.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005731 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   3.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004331 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010006 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   3.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007480 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011588 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   3.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.057095 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006299 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   5.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005779 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007498 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   5.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022305 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   3.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017233 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   3.0s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009919 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005401 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008113 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007653 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022302 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006356 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.0s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008032 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   5.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004625 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   5.6s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007731 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   6.0s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008866 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007975 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004939 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   3.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004718 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   3.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005312 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   3.5s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007821 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   3.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004785 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007390 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010023 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006895 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.0s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010687 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   5.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010931 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   5.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007007 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   5.9s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007810 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   5.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005041 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010071 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   3.6s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006874 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   3.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005878 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006458 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   3.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006144 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006317 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010860 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   6.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006624 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.033457 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   6.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006803 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   5.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.082751 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   3.3s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005553 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   3.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006410 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   3.6s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022624 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005903 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.045310 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   3.8s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020918 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   3.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006023 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.028208 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   6.2s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019481 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   5.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062529 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   5.1s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010942 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   5.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.080007 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   3.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010333 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   2.9s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010218 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   3.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011266 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.026938 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010218 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.0s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064804 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   3.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005416 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   6.0s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011394 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   5.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006309 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   6.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004785 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005738 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   5.3s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.045823 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   3.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.070268 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   3.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.052181 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   3.2s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007330 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   3.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.026890 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   3.8s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014918 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   3.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.030515 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011289 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   5.9s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008750 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   5.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.016899 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   6.0s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006580 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   5.4s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008429 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   5.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010798 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010316 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009737 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006271 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   5.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006824 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   5.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.025053 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006362 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008819 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006351 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   8.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014623 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.6s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016272 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013812 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   7.4s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   6.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008241 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   5.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006732 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005939 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   5.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005665 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   5.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011070 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   5.5s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013740 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010979 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.4s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007316 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007802 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   8.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010909 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006074 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.110276 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012309 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   5.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005610 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006819 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   5.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009565 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006532 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011334 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006690 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005475 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   7.9s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005686 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.024873 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.5s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005390 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   8.2s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010735 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   6.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   5.3s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.056684 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007394 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   5.8s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008025 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006289 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007555 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.6s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.030122 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006136 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   7.8s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005405 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005197 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008325 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   8.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007835 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   6.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006920 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   5.6s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008020 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   5.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022527 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   5.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006465 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.025178 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009095 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.9s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016477 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   7.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005716 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005976 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   3.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021002 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006432 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=  10.4s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005674 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   6.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006838 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005736 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   5.2s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004662 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005802 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008002 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.6s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012571 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011354 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   8.0s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019524 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005100 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005403 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   7.3s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.125878 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   7.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007840 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time= 1.0min\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.048966 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   5.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006257 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007650 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   5.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032167 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.9s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005701 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006112 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005915 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   7.9s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013462 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009151 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006754 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006835 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=  10.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007931 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   6.6s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015329 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004764 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   5.1s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012499 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005410 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.023466 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010785 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.039414 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   7.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017801 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.4s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012568 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004603 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   7.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010585 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   6.8s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010000 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time= 1.0min\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007415 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009570 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   7.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.121071 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   5.6s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006114 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=  59.8s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009939 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   5.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005454 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time= 2.5min\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004766 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007724 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   5.6s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005866 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   5.3s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006551 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   3.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.112486 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   3.4s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.044268 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   3.6s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005537 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   8.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007305 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   7.3s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009721 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=  58.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.067950 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   5.6s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004666 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time= 2.5min\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006753 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004842 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.048271 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   6.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017622 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   5.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006054 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004596 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   3.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.016452 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   3.9s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=  59.8s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006730 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   5.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005812 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time= 2.5min\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004534 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   5.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012385 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   6.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005752 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   5.5s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021595 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   3.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009866 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   3.8s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005950 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006248 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005627 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   3.4s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005074 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   3.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.051543 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.023796 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=  59.9s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.029750 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   5.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004484 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time= 2.5min\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004615 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   5.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007704 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   5.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005203 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   5.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008045 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   3.4s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005052 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   3.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020306 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006123 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.039842 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   3.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.038150 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   3.4s\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=  59.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007458 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   5.6s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005177 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time= 2.5min\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007427 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   5.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006722 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   6.1s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010861 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   5.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.057639 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017603 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   3.5s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007280 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   3.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019277 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006607 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   3.8s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014109 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=13.6min\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005667 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=  59.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005227 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010780 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time= 2.5min\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006033 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   5.8s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005454 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   6.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009875 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   5.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002208 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   3.5s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008024 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   3.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006681 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021061 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   3.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005240 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   3.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.083181 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=13.6min\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007068 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   5.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006934 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   5.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004743 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time= 2.5min\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006768 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   5.5s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007743 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   5.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005034 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   3.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005410 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   3.5s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004398 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015976 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024186 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007390 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   3.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008373 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=13.6min\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005782 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   5.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005405 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time= 2.5min\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005124 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   5.6s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005733 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   6.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003828 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   5.6s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.028749 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   3.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012722 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   3.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005512 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005719 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   3.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005757 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007608 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=13.6min\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007626 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009851 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009031 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004957 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   3.6s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005306 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=13.6min\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008945 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   3.8s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017420 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.089243 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005024 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.040787 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004751 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.038563 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time= 1.5min\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004860 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009887 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.5s\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   3.8s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006276 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005419 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=13.6min\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005628 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   3.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005442 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005623 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004667 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005488 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.5s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.127995 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   5.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007280 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time= 1.5min\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010892 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007914 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011779 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006561 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=13.6min\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006318 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004564 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.035501 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.026502 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005847 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020329 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014847 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time= 1.5min\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.026990 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004434 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012881 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time= 1.9min\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006190 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time= 2.0min[LightGBM] [Info] Number of positive: 997, number of negative: 997\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002201 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12978\n",
      "[LightGBM] [Info] Number of data points in the train set: 1994, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Best Parameters (LightGBM): {'colsample_bytree': 0.6, 'learning_rate': 0.025, 'min_child_samples': 6, 'n_estimators': 350, 'num_leaves': 20, 'reg_alpha': 0.5, 'reg_lambda': 3.0, 'scale_pos_weight': 1.5, 'subsample': 0.5}\n",
      "Test AUC Score (LightGBM): 0.8432977191786013\n",
      "Classification Report (LightGBM):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.95      0.93       249\n",
      "           1       0.70      0.57      0.62        53\n",
      "\n",
      "    accuracy                           0.88       302\n",
      "   macro avg       0.80      0.76      0.78       302\n",
      "weighted avg       0.87      0.88      0.88       302\n",
      "\n"
     ]
    }
   ],
   "execution_count": 94
  },
  {
   "cell_type": "markdown",
   "id": "02c3567d",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "id": "afb9fdf4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T20:37:47.005839Z",
     "start_time": "2025-01-15T20:35:46.769980Z"
    }
   },
   "source": [
    "# Neural Network\n",
    "nn_model = Sequential([\n",
    "    Dense(256, kernel_regularizer=regularizers.l2(0.01), input_dim=X_train_scaled.shape[1], kernel_initializer=HeNormal(seed=42)),\n",
    "    LeakyReLU(alpha=0.1),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.1),\n",
    "\n",
    "    Dense(128, kernel_regularizer=regularizers.l2(0.01), kernel_initializer=HeNormal(seed=42)),\n",
    "    LeakyReLU(alpha=0.1),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.1),\n",
    "\n",
    "    Dense(64, kernel_regularizer=regularizers.l2(0.01), kernel_initializer=HeNormal(seed=42)),\n",
    "    LeakyReLU(alpha=0.1),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.1),\n",
    "\n",
    "    Dense(32, kernel_regularizer=regularizers.l2(0.01), kernel_initializer=HeNormal(seed=42)),\n",
    "    LeakyReLU(alpha=0.1),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.1),\n",
    "\n",
    "    Dense(1, activation='sigmoid', kernel_initializer=HeNormal(seed=42))\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "nn_model.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy', 'AUC'])\n",
    "\n",
    "# Train the model with class weights\n",
    "class_weights = {0: 1.0, 1: 3.0}\n",
    "nn_model.fit(X_train_scaled, y_train_resampled, epochs=100, batch_size=32, verbose=0, shuffle=False, class_weight=class_weights)\n",
    "\n",
    "# Predictions with optimized threshold\n",
    "y_probs = nn_model.predict(X_test_scaled).flatten()\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_probs)\n",
    "optimal_idx = np.argmax(tpr - fpr)\n",
    "optimal_threshold = thresholds[optimal_idx]\n",
    "\n",
    "nn_predictions = (y_probs > optimal_threshold).astype(int)\n",
    "nn_report = classification_report(y_test, nn_predictions, output_dict=True)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Optimal Threshold:\", optimal_threshold)\n",
    "print(\"Test AUC Score (Neural Network):\", roc_auc_score(y_test, y_probs))\n",
    "print(\"Classification Report (Neural Network):\\n\", classification_report(y_test, nn_predictions))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m10/10\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step \n",
      "Optimal Threshold: 0.57809067\n",
      "Test AUC Score (Neural Network): 0.7051602636962946\n",
      "Classification Report (Neural Network):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.79      0.84       249\n",
      "           1       0.36      0.55      0.43        53\n",
      "\n",
      "    accuracy                           0.75       302\n",
      "   macro avg       0.62      0.67      0.64       302\n",
      "weighted avg       0.80      0.75      0.77       302\n",
      "\n"
     ]
    }
   ],
   "execution_count": 95
  },
  {
   "cell_type": "markdown",
   "id": "7a7ea580",
   "metadata": {},
   "source": [
    "### Weighted Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "id": "56c8dc27",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T20:37:47.084338Z",
     "start_time": "2025-01-15T20:37:47.049185Z"
    }
   },
   "source": [
    "# Weighted Ensemble \n",
    "ensemble_proba_weighted = (\n",
    "    0.35 * best_gb_model.predict_proba(X_test_scaled)[:, 1] +\n",
    "    0.35 * best_lgb_model.predict_proba(X_test_scaled)[:, 1] +\n",
    "    0.3 * nn_model.predict(X_test_scaled).flatten() +\n",
    "    0.0 * logistic_model.predict_proba(X_test_scaled)[:, 1]\n",
    ")\n",
    "\n",
    "# Optimize Threshold for Weighted Ensemble\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, ensemble_proba_weighted)\n",
    "f1_scores = 2 * (precision * recall) / (precision + recall)\n",
    "optimal_idx = f1_scores.argmax()\n",
    "optimal_threshold = thresholds[optimal_idx]\n",
    "\n",
    "ensemble_predictions_weighted = (ensemble_proba_weighted > optimal_threshold).astype(int)\n",
    "\n",
    "# Evaluation des Weighted Ensembles\n",
    "ensemble_auc_weighted = roc_auc_score(y_test, ensemble_proba_weighted)\n",
    "ensemble_report_weighted = classification_report(y_test, ensemble_predictions_weighted)\n",
    "\n",
    "print(\"Optimized Weighted Ensemble Test AUC Score:\", ensemble_auc_weighted)\n",
    "print(\"Optimal Threshold for Weighted Ensemble:\", optimal_threshold)\n",
    "print(\"Classification Report (Optimized Weighted Ensemble):\\n\", ensemble_report_weighted)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m10/10\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 772us/step\n",
      "Optimized Weighted Ensemble Test AUC Score: 0.8103356823520498\n",
      "Optimal Threshold for Weighted Ensemble: 0.6182950084644212\n",
      "Classification Report (Optimized Weighted Ensemble):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.98      0.94       249\n",
      "           1       0.84      0.51      0.64        53\n",
      "\n",
      "    accuracy                           0.90       302\n",
      "   macro avg       0.87      0.74      0.79       302\n",
      "weighted avg       0.89      0.90      0.89       302\n",
      "\n"
     ]
    }
   ],
   "execution_count": 96
  },
  {
   "cell_type": "markdown",
   "id": "edc7d307",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "id": "a30a19ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T20:37:47.580184Z",
     "start_time": "2025-01-15T20:37:47.161189Z"
    }
   },
   "source": [
    "### Results\n",
    "# Results Comparison\n",
    "results = {\n",
    "    \"Model\": [\"Gradient Boosting\", \"LightGBM\", \"Neural Network\", \"Logistic Regression\", \"Weighted Ensemble\"],\n",
    "    \"Accuracy\": [\n",
    "        classification_report(y_test, best_gb_model.predict(X_test_scaled), output_dict=True)['accuracy'],\n",
    "        classification_report(y_test, best_lgb_model.predict(X_test_scaled), output_dict=True)['accuracy'],\n",
    "        nn_report['accuracy'],\n",
    "        logistic_report['accuracy'],\n",
    "        classification_report(y_test, ensemble_predictions_weighted, output_dict=True)['accuracy']\n",
    "    ],\n",
    "    \"Precision\": [\n",
    "        classification_report(y_test, best_gb_model.predict(X_test_scaled), output_dict=True)['macro avg']['precision'],\n",
    "        classification_report(y_test, best_lgb_model.predict(X_test_scaled), output_dict=True)['macro avg']['precision'],\n",
    "        nn_report['macro avg']['precision'],\n",
    "        logistic_report['macro avg']['precision'],\n",
    "        classification_report(y_test, ensemble_predictions_weighted, output_dict=True)['macro avg']['precision']\n",
    "    ],\n",
    "    \"Recall\": [\n",
    "        classification_report(y_test, best_gb_model.predict(X_test_scaled), output_dict=True)['macro avg']['recall'],\n",
    "        classification_report(y_test, best_lgb_model.predict(X_test_scaled), output_dict=True)['macro avg']['recall'],\n",
    "        nn_report['macro avg']['recall'],\n",
    "        logistic_report['macro avg']['recall'],\n",
    "        classification_report(y_test, ensemble_predictions_weighted, output_dict=True)['macro avg']['recall']\n",
    "    ],\n",
    "    \"F1-Score\": [\n",
    "        classification_report(y_test, best_gb_model.predict(X_test_scaled), output_dict=True)['macro avg']['f1-score'],\n",
    "        classification_report(y_test, best_lgb_model.predict(X_test_scaled), output_dict=True)['macro avg']['f1-score'],\n",
    "        nn_report['macro avg']['f1-score'],\n",
    "        logistic_report['macro avg']['f1-score'],\n",
    "        classification_report(y_test, ensemble_predictions_weighted, output_dict=True)['macro avg']['f1-score']\n",
    "    ],\n",
    "    \"AUC\": [\n",
    "        roc_auc_score(y_test, best_gb_model.predict_proba(X_test_scaled)[:, 1]),\n",
    "        roc_auc_score(y_test, best_lgb_model.predict_proba(X_test_scaled)[:, 1]),\n",
    "        roc_auc_score(y_test, nn_model.predict(X_test_scaled).ravel()),\n",
    "        logistic_auc,\n",
    "        roc_auc_score(y_test, ensemble_proba_weighted)\n",
    "    ]\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Plotting ROC Curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "for model, name, color in zip([best_gb_model, best_lgb_model, logistic_model],\n",
    "                              [\"Gradient Boosting\", \"LightGBM\", \"Logistic Regression\"],\n",
    "                              [\"green\", \"orange\", \"red\"]):\n",
    "    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    plt.plot(fpr, tpr, label=f'{name} (AUC = {auc:.2f})', linewidth=2, color=color)\n",
    "\n",
    "# Add Neural Network to ROC Curve\n",
    "nn_pred_proba = nn_model.predict(X_test_scaled).ravel()\n",
    "fpr, tpr, _ = roc_curve(y_test, nn_pred_proba)\n",
    "auc = roc_auc_score(y_test, nn_pred_proba)\n",
    "plt.plot(fpr, tpr, label=f'Neural Network (AUC = {auc:.2f})', linewidth=2, color='blue')\n",
    "\n",
    "# Add Weighted Ensemble to ROC Curve\n",
    "fpr, tpr, _ = roc_curve(y_test, ensemble_proba_weighted)\n",
    "auc = roc_auc_score(y_test, ensemble_proba_weighted)\n",
    "plt.plot(fpr, tpr, label=f'Weighted Ensemble (AUC = {auc:.2f})', linewidth=2, color='purple')\n",
    "\n",
    "# Add Baseline\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random Chance', linewidth=1)\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve Comparison', fontsize=14)\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Predictions\n",
    "models = [\n",
    "    (\"Gradient Boosting\", best_gb_model, best_gb_model.predict(X_test_scaled)),\n",
    "    (\"LightGBM\", best_lgb_model, best_lgb_model.predict(X_test_scaled)),\n",
    "    (\"Neural Network\", nn_model, (nn_model.predict(X_test_scaled) > optimal_threshold).astype(int)),\n",
    "    (\"Logistic Regression\", logistic_model, logistic_predictions),\n",
    "    (\"Weighted Ensemble\", None, ensemble_predictions_weighted)  # None for model since it's a manual ensemble\n",
    "]\n",
    "\n",
    "for name, model, predictions in models:\n",
    "    # Calculate confusion matrix and normalize to percentage\n",
    "    cm = confusion_matrix(y_test, predictions)\n",
    "    cm_percentage = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
    "\n",
    "    # Display confusion matrix\n",
    "    cmd = ConfusionMatrixDisplay(cm_percentage, display_labels=[\"No-Success\", \"Success\"])\n",
    "    cmd.plot(cmap=plt.cm.Blues, values_format=\".2f\")\n",
    "    plt.title(f\"Confusion Matrix (Percentage): {name}\")\n",
    "    plt.show()\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m10/10\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 723us/step\n",
      "                 Model  Accuracy  Precision  Recall  F1-Score    AUC\n",
      "0    Gradient Boosting    0.8775     0.8023  0.7401    0.7649 0.8321\n",
      "1             LightGBM    0.8808     0.8044  0.7569    0.7771 0.8433\n",
      "2       Neural Network    0.7483     0.6247  0.6692    0.6356 0.7052\n",
      "3  Logistic Regression    0.7517     0.6539  0.7306    0.6659 0.7910\n",
      "4    Weighted Ensemble    0.8974     0.8737  0.7447    0.7878 0.8103\n",
      "\u001B[1m10/10\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 723us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m10/10\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 743us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 97
  },
  {
   "cell_type": "markdown",
   "id": "21fcfe5a660af49",
   "metadata": {},
   "source": [
    "## Model Probabilities Predictions"
   ]
  },
  {
   "cell_type": "code",
   "id": "c6eacecf2034725c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T20:37:47.760825Z",
     "start_time": "2025-01-15T20:37:47.644990Z"
    }
   },
   "source": [
    "pd.options.display.float_format = '{:.4f}'.format  # Ensure non-scientific number formatting\n",
    "gb_train_probabilities = best_gb_model.predict_proba(X_train_scaled)[:, 1]\n",
    "gb_train_predictions = best_gb_model.predict(X_train_scaled)\n",
    "\n",
    "gb_test_probabilities = best_gb_model.predict_proba(X_test_scaled)[:, 1]\n",
    "gb_test_predictions = best_gb_model.predict(X_test_scaled)\n",
    "\n",
    "# LightGBM\n",
    "lgb_train_probabilities = best_lgb_model.predict_proba(X_train_scaled)[:, 1]\n",
    "lgb_train_predictions = best_lgb_model.predict(X_train_scaled)\n",
    "\n",
    "lgb_test_probabilities = best_lgb_model.predict_proba(X_test_scaled)[:, 1]\n",
    "lgb_test_predictions = best_lgb_model.predict(X_test_scaled)\n",
    "\n",
    "# Neural Network\n",
    "nn_train_probabilities = nn_model.predict(X_train_scaled).ravel()\n",
    "nn_train_predictions = (nn_train_probabilities > 0.5).astype(int)\n",
    "\n",
    "nn_test_probabilities = nn_model.predict(X_test_scaled).ravel()\n",
    "nn_test_predictions = (nn_test_probabilities > 0.5).astype(int)\n",
    "\n",
    "# Logistic Regression\n",
    "logistic_train_probabilities = logistic_model.predict_proba(X_train_scaled)[:, 1]\n",
    "logistic_train_predictions = logistic_model.predict(X_train_scaled)\n",
    "\n",
    "logistic_test_probabilities = logistic_model.predict_proba(X_test_scaled)[:, 1]\n",
    "logistic_test_predictions = logistic_model.predict(X_test_scaled)\n",
    "\n",
    "# Weighted Ensemble for training data\n",
    "ensemble_train_probabilities = (\n",
    "        0.3 * gb_train_probabilities +\n",
    "        0.3 * lgb_train_probabilities +\n",
    "        0.4 * nn_train_probabilities\n",
    ")\n",
    "ensemble_train_predictions = (ensemble_train_probabilities > 0.5).astype(int)\n",
    "\n",
    "# Weighted Ensemble for test data\n",
    "ensemble_test_probabilities = (\n",
    "        0.3 * gb_test_probabilities +\n",
    "        0.3 * lgb_test_probabilities +\n",
    "        0.4 * nn_test_probabilities\n",
    ")\n",
    "ensemble_test_predictions = (ensemble_test_probabilities > 0.5).astype(int)\n",
    "\n",
    "training_results = pd.DataFrame({\n",
    "    \"Observation Index\": range(len(X_train_scaled)),\n",
    "    \"ID\": ID_train_resampled,\n",
    "    \"True Class (Train)\": y_train_resampled.values,\n",
    "    \"GB Prediction (Train)\": gb_train_predictions,\n",
    "    \"GB Probability (Success, Train)\": gb_train_probabilities,\n",
    "    \"LGB Prediction (Train)\": lgb_train_predictions,\n",
    "    \"LGB Probability (Success, Train)\": lgb_train_probabilities,\n",
    "    \"NN Prediction (Train)\": nn_train_predictions,\n",
    "    \"NN Probability (Success, Train)\": nn_train_probabilities,\n",
    "    \"Logistic Prediction (Train)\": logistic_train_predictions,\n",
    "    \"Logistic Probability (Success, Train)\": logistic_train_probabilities,\n",
    "    \"Ensemble Prediction (Train)\": ensemble_train_predictions,\n",
    "    \"Ensemble Probability (Success, Train)\": ensemble_train_probabilities\n",
    "})\n",
    "\n",
    "# Create test results DataFrame\n",
    "test_results = pd.DataFrame({\n",
    "    \"Observation Index\": range(len(X_test_scaled)),\n",
    "    \"ID\": ID_test.values,\n",
    "    \"True Class (Test)\": y_test.values,\n",
    "    \"GB Prediction (Test)\": gb_test_predictions,\n",
    "    \"GB Probability (Success, Test)\": gb_test_probabilities,\n",
    "    \"LGB Prediction (Test)\": lgb_test_predictions,\n",
    "    \"LGB Probability (Success, Test)\": lgb_test_probabilities,\n",
    "    \"NN Prediction (Test)\": nn_test_predictions,\n",
    "    \"NN Probability (Success, Test)\": nn_test_probabilities,\n",
    "    \"Logistic Prediction (Test)\": logistic_test_predictions,\n",
    "    \"Logistic Probability (Success, Test)\": logistic_test_probabilities,\n",
    "    \"Ensemble Prediction (Test)\": ensemble_test_predictions,\n",
    "    \"Ensemble Probability (Success, Test)\": ensemble_test_probabilities\n",
    "})"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m63/63\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 337us/step\n",
      "\u001B[1m10/10\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 734us/step\n"
     ]
    }
   ],
   "execution_count": 98
  },
  {
   "cell_type": "code",
   "id": "6ccb72060175a8a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T20:37:47.770906Z",
     "start_time": "2025-01-15T20:37:47.765546Z"
    }
   },
   "source": [
    "test_results"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     Observation Index    ID  True Class (Test)  GB Prediction (Test)  \\\n",
       "0                    0   732                  0                     0   \n",
       "1                    1    37                  1                     0   \n",
       "2                    2   408                  0                     0   \n",
       "3                    3   784                  0                     0   \n",
       "4                    4   135                  0                     0   \n",
       "..                 ...   ...                ...                   ...   \n",
       "297                297   188                  0                     0   \n",
       "298                298   882                  0                     0   \n",
       "299                299  1318                  0                     0   \n",
       "300                300   335                  0                     0   \n",
       "301                301  1045                  1                     0   \n",
       "\n",
       "     GB Probability (Success, Test)  LGB Prediction (Test)  \\\n",
       "0                            0.0468                      0   \n",
       "1                            0.0389                      0   \n",
       "2                            0.0426                      0   \n",
       "3                            0.0240                      0   \n",
       "4                            0.0166                      0   \n",
       "..                              ...                    ...   \n",
       "297                          0.0457                      0   \n",
       "298                          0.0073                      0   \n",
       "299                          0.0090                      0   \n",
       "300                          0.0113                      0   \n",
       "301                          0.0150                      0   \n",
       "\n",
       "     LGB Probability (Success, Test)  NN Prediction (Test)  \\\n",
       "0                             0.1532                     1   \n",
       "1                             0.0643                     0   \n",
       "2                             0.0301                     1   \n",
       "3                             0.0969                     1   \n",
       "4                             0.0375                     0   \n",
       "..                               ...                   ...   \n",
       "297                           0.0433                     0   \n",
       "298                           0.0281                     1   \n",
       "299                           0.0101                     0   \n",
       "300                           0.0528                     0   \n",
       "301                           0.0290                     0   \n",
       "\n",
       "     NN Probability (Success, Test)  Logistic Prediction (Test)  \\\n",
       "0                            0.5221                           0   \n",
       "1                            0.4495                           1   \n",
       "2                            0.6020                           0   \n",
       "3                            0.7031                           0   \n",
       "4                            0.3994                           0   \n",
       "..                              ...                         ...   \n",
       "297                          0.4455                           0   \n",
       "298                          0.5294                           0   \n",
       "299                          0.3842                           0   \n",
       "300                          0.2921                           0   \n",
       "301                          0.3784                           0   \n",
       "\n",
       "     Logistic Probability (Success, Test)  Ensemble Prediction (Test)  \\\n",
       "0                                  0.4354                           0   \n",
       "1                                  0.5796                           0   \n",
       "2                                  0.1860                           0   \n",
       "3                                  0.2957                           0   \n",
       "4                                  0.0418                           0   \n",
       "..                                    ...                         ...   \n",
       "297                                0.1560                           0   \n",
       "298                                0.1721                           0   \n",
       "299                                0.2247                           0   \n",
       "300                                0.1818                           0   \n",
       "301                                0.2302                           0   \n",
       "\n",
       "     Ensemble Probability (Success, Test)  \n",
       "0                                  0.2688  \n",
       "1                                  0.2108  \n",
       "2                                  0.2626  \n",
       "3                                  0.3175  \n",
       "4                                  0.1760  \n",
       "..                                    ...  \n",
       "297                                0.2049  \n",
       "298                                0.2224  \n",
       "299                                0.1594  \n",
       "300                                0.1361  \n",
       "301                                0.1646  \n",
       "\n",
       "[302 rows x 13 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Observation Index</th>\n",
       "      <th>ID</th>\n",
       "      <th>True Class (Test)</th>\n",
       "      <th>GB Prediction (Test)</th>\n",
       "      <th>GB Probability (Success, Test)</th>\n",
       "      <th>LGB Prediction (Test)</th>\n",
       "      <th>LGB Probability (Success, Test)</th>\n",
       "      <th>NN Prediction (Test)</th>\n",
       "      <th>NN Probability (Success, Test)</th>\n",
       "      <th>Logistic Prediction (Test)</th>\n",
       "      <th>Logistic Probability (Success, Test)</th>\n",
       "      <th>Ensemble Prediction (Test)</th>\n",
       "      <th>Ensemble Probability (Success, Test)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>732</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0468</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1532</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5221</td>\n",
       "      <td>0</td>\n",
       "      <td>0.4354</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0389</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0643</td>\n",
       "      <td>0</td>\n",
       "      <td>0.4495</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5796</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>408</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0426</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0301</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6020</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>784</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0240</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0969</td>\n",
       "      <td>1</td>\n",
       "      <td>0.7031</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2957</td>\n",
       "      <td>0</td>\n",
       "      <td>0.3175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>135</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0166</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0375</td>\n",
       "      <td>0</td>\n",
       "      <td>0.3994</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0418</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>297</td>\n",
       "      <td>188</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0457</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0433</td>\n",
       "      <td>0</td>\n",
       "      <td>0.4455</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1560</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>298</td>\n",
       "      <td>882</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0073</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0281</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5294</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1721</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>299</td>\n",
       "      <td>1318</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0090</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0101</td>\n",
       "      <td>0</td>\n",
       "      <td>0.3842</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2247</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>300</td>\n",
       "      <td>335</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0113</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0528</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2921</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1818</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>301</td>\n",
       "      <td>1045</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0150</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0290</td>\n",
       "      <td>0</td>\n",
       "      <td>0.3784</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2302</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1646</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>302 rows × 13 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 99
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Visualizations",
   "id": "2951ca6881aa42e5"
  },
  {
   "cell_type": "code",
   "id": "f6f23d504b67e04c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T20:37:48.670592Z",
     "start_time": "2025-01-15T20:37:47.819644Z"
    }
   },
   "source": [
    "def probability_distribution_plot(df, model_name):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(df[df[\"True Class (Test)\"] == 0][f\"{model_name} Probability (Success, Test)\"], color=\"red\", label=\"No-Success\", kde=True)\n",
    "    sns.histplot(df[df[\"True Class (Test)\"] == 1][f\"{model_name} Probability (Success, Test)\"], color=\"green\", label=\"Success\", kde=True)\n",
    "    plt.title(f\"{model_name} Probability Distribution\")\n",
    "    plt.xlabel(\"Probability\")\n",
    "    plt.ylabel(\"Density\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def calibration_curve_plot(model, X, y, model_name, is_nn=False, is_ensemble=False):\n",
    "    if is_nn:\n",
    "        probs = model.predict(X).ravel()\n",
    "    elif is_ensemble:\n",
    "        probs = model\n",
    "    else:\n",
    "        probs = model.predict_proba(X)[:, 1]\n",
    "    prob_true, prob_pred = calibration_curve(y, probs, n_bins=10)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(prob_pred, prob_true, \"s-\", label=f\"{model_name} Calibration\")\n",
    "    plt.plot([0, 1], [0, 1], \"k--\", label=\"Perfect Calibration\")\n",
    "    plt.title(f\"Calibration Curve: {model_name}\")\n",
    "    plt.xlabel(\"Mean Predicted Probability\")\n",
    "    plt.ylabel(\"Fraction of Positives\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Violin Plot\n",
    "def violin_plot(df, model_name):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.violinplot(x=\"True Class (Test)\", y=f\"{model_name} Probability (Success, Test)\", data=df, palette=\"muted\")\n",
    "    plt.title(f\"{model_name} Violin Plot\")\n",
    "    plt.xlabel(\"True Class\")\n",
    "    plt.ylabel(\"Probability\")\n",
    "    plt.show()\n",
    "\n",
    "# Gradient Boosting\n",
    "probability_distribution_plot(test_results, \"GB\")\n",
    "calibration_curve_plot(best_gb_model, X_test_scaled, y_test, \"Gradient Boosting\")\n",
    "violin_plot(test_results, \"GB\")\n",
    "\n",
    "# LightGBM\n",
    "probability_distribution_plot(test_results, \"LGB\")\n",
    "calibration_curve_plot(best_lgb_model, X_test_scaled, y_test, \"LightGBM\")\n",
    "violin_plot(test_results, \"LGB\")\n",
    "\n",
    "# Neural Network\n",
    "probability_distribution_plot(test_results, \"NN\")\n",
    "calibration_curve_plot(nn_model, X_test_scaled, y_test, \"Neural Network\", is_nn=True)\n",
    "violin_plot(test_results, \"NN\")\n",
    "\n",
    "# Weighted Ensemble\n",
    "probability_distribution_plot(test_results, \"Ensemble\")\n",
    "calibration_curve_plot(ensemble_test_probabilities, X_test_scaled, y_test, \"Weighted Ensemble\", is_ensemble=True)\n",
    "violin_plot(test_results, \"Ensemble\")\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0cAAAIhCAYAAACFYMFwAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAACn+klEQVR4nOzdd3wUdeLG8c/M1vRGBwEp0jsCopyCoIIdu6LioSjW07PAoZ69d8XC2UEUFRs/OxZUBJRepIPSIUACaVtnfn8EIghC+myS530vLsns7Oyzm3GTJ9+Z7xi2bduIiIiIiIjUcKbTAURERERERGKBypGIiIiIiAgqRyIiIiIiIoDKkYiIiIiICKByJCIiIiIiAqgciYiIiIiIACpHIiIiIiIigMqRiIiIiIgIoHIkIiLVWFW4znksZIyFDCIisUDlSESkgl188cW0bduWhQsXHvD2fv36MXLkyFKv/1fr16+nVatW+/xr3bo1Xbp0YfDgwbz//vtle0K7zZw5k1atWjFz5swyb+viiy/m4osvPug6zz77LK1atSr6eu/XYc9z/uCDDwDYtWsXt956K7NmzSpTrpEjR+73Onbu3JlTTz2V5557jkAgUOLnsbfZs2czfPjwQ6731+de0sf5O6FQiAceeIDJkycXLRs5ciT9+vUr87ZFRKoit9MBRERqgmg0yqhRo/jggw/wer3lvv6BjBgxguOOOw4oHBnIy8vjvffeY/To0UQiEc4///xSbdcp55xzDn369DngbXXq1GHixIk0btwYgCVLlvDxxx9z1llnlflxa9euzXPPPQeAZVnk5OQwa9YsXnrpJX766SfeeOMNfD4fAP/9739LtO333nuPVatWHXK9gz33sti6dStvvPEGDz74YNGyq6++mksuuaTcH0tEpCpQORIRqQRJSUmsWLGCMWPGcOONN5b7+gfSuHFjOnfuvM+y3r17s3TpUl5//fUqV47q1atHvXr1Dnib1+vd77mWlwNt+9hjj6VTp05cc801vPrqq4wYMQKAFi1aVEiGgz338ranYIqI1EQ6rE5EpBK0adOGM844g5dffplFixaV+/rFZZombdq0YePGjcCfh6O99tprnHTSSXTq1IlJkyYBsHDhQoYNG0bPnj3p2rUrV111FStWrNhvmytXruTCCy+kQ4cODBgwgHHjxu1z+44dO7j77rvp27cv7du3p0ePHlxzzTWsX79+v22NGTOG3r1706VLF66++mrWrVtXdNtfDy3b296H1c2cObNo5OOSSy7h4osv5q233qJVq1asWbNmn/t9/PHHtGnThk2bNpXgVSzUv39/OnfuzDvvvFO07K+Hu02bNo1zzz2XLl26cOSRRzJixIiikaKRI0fy4YcfsmHDhqLsf/f9+LvnfrDX60CHx+39Oq1fv57jjz8egFGjRhWt+9f7RaNR3nrrLU499VQ6duzIcccdx2OPPUYwGNznsYYOHcqkSZM48cQTad++Paeffjo//PBDiV9XEREnqRyJiFSS//znP6SlpTFq1ChCoVC5r19ca9as2W904Nlnn+WKK67gkUce4eijj2bGjBlccMEFADzwwAPcd999bNq0ifPPP3+/w8AefPBBOnfuzAsvvECfPn247777eOONN4DCw/muvPJKpk2bxs0338wrr7zCtddey/Tp0/c7BG327Nl8+umn3Hnnndx3330sXbqUSy65hNzc3BI9v3bt2nHnnXcCcOedd/Lf//6XU089FZ/Px8cff7zPuh999BFHHXUU9evXL9Fj7HH00UezefNmNmzYsN9t69at4+qrr6Z9+/a88MIL3H///axZs4bhw4djWRZXX301xx57LLVr12bixIlFh0DC/t+PAynr61WnTp2iwwVHjBhR9Plf3XnnnTz44IP079+fF154gYsuuojx48dz9dVX7zORw6JFi3jllVe4/vrrGTNmDC6Xi+uuu46dO3cWK4+ISCzQYXUiIpUkJSWFe+65hxEjRhTrcLmSrv9XlmURiUSKPt+yZQvjxo1j6dKl3HXXXfusO3DgwH3Oz7nuuuto0qQJY8eOxeVyAXDMMccwYMAAnnnmGZ5++umidc8991xuvfXWonW2bNnCSy+9xMUXX0xmZiZxcXHcdtttdO/eHYCePXuydu1aJk6cuE8Gl8vFq6++WnT4WLNmzTjjjDP46KOPGDJkSLGfd2JiYtHhbS1atCj6fMCAAXzyySfccMMNGIbB5s2bmTFjBo8++mixt/1XtWrVAmDbtm00bNhwn9sWLFhAIBDgyiuvpG7dukDh4XHffPMN+fn5NG7cmPT09H0O28vPzwf2/34cSFlfL6/XS5s2bYDCQ+natm273zorV67k/fff59///nfRxBFHH300derU4dZbb+WHH37g2GOPBSAnJ4cPPvigqHjHx8czZMgQZsyYwYknnnjIPCIisUAjRyIilahfv36cdtppvPzyyyxevLjc19/b6NGjadeuHe3ataNDhw7079+fDz74gBEjRnDeeefts+6eX5Kh8Bf0hQsXMnDgwKJiBJCcnEzfvn355Zdf9rnvoEGD9vl6wIABbN++ndWrV1O3bl3efPNNunXrxvr165k2bRrjxo1jzpw5+42Gde3adZ/zatq0acNhhx3Gr7/+WqLn/XfOPvtsNmzYUDSD3UcffURCQgIDBgwo9Tb3jJwYhrHfbZ06dcLn83H22Wdz//338+OPP9K6dWtuvPFGEhMTD7rdvb8ff6eiXy+g6Ht98skn77P85JNPxuVy7TNTYXp6+j4jknuyFRQUlFseEZGKppEjEZFKdvvttzN9+nRGjRpVdH5Pea6/x7XXXlt0qJZpmiQlJdGoUSNMc/+/i8XHxxd9npOTg23bRaMie6tVqxY5OTn7LdtbRkYGQNHhVJ988glPPPEEmzZtIjU1lTZt2uD3+w+47b/KyMhg165dh3imxdOrVy8aNWrERx99xJFHHslHH33EoEGDimaaK40tW7YAFI0M7a1Ro0aMHz+esWPH8v777/Pmm2+SnJzMhRdeyL/+9a8DFqo99v5+/J2Kfr3gz+9h7dq191nudrtJS0vbZ1+Ii4vbZ509z8+yrHLLIyJS0TRyJCJSyVJSUrjrrrtYtmwZzz//fLmvv0fDhg3p0KEDHTp0oF27djRu3PiAxeivkpKSMAyDbdu27XdbZmYmqamp+yz76zkle+6XkZHBrFmzuO222zjhhBP44YcfmDlzJq+//voBZ5Y70LkpmZmZpKenHzJzcRiGwZlnnsmUKVNYtGgRa9asKfNU3z///DNNmjQ5YDkC6NixI88991zR8z766KN58cUX+eKLL8r0uHDo18swDKLR6D637zlsr7hSUlKKtru3cDhMVlYWaWlpJdqeiEisUzkSEXFA//79OeWUUxg7diw7duwo9/XLIj4+nvbt2/P555/v88t1Tk4O33//Pd26ddtn/e+//36frz/99FPq169PkyZNmDt3LpZlcd111xUViGg0ys8//wzsO6owe/bsfUYi5s+fz4YNG+jVq1eJn8PehwPubfDgwezatYuHH36Y5s2b06lTpxJve4/vv/+ehQsXFk1c8Vevv/46ffv2JRQK4fV6Oeqoo7j33nsBimYLLE5Z/TuHer0SEhLIysraZ1a52bNn77ONv3ud9ujRowdQ+D3d26effko0Gt1vXxARqep0WJ2IiEPuuOMOZsyYccARmvJYvyz+/e9/M2zYMIYPH86FF15IOBxm7NixhEIhrrnmmn3WHTduHAkJCbRt25ZPP/2UH3/8kUceeQTDMOjYsSMA99xzD2eddRY7d+7krbfeYunSpUDhSMae828sy2L48OFcddVVZGVl8fjjj3PEEUdw2mmnlTh/UlISUFhgUlJSaN26NQANGjSgd+/e/PTTT9x8883F2lYoFGLevHlA4TlGu3btYtasWbz55pv07Nnzbyc/6NWrF4899hjXXHMNQ4YMweVy8c477+D1eunbty9QeB7Xtm3bmDp1arHOM9rboV6vvn37Mm7cOEaPHs3ZZ5/N8uXLee211/YpRHtep+nTpx+wLLZo0YIzzzyTZ555hoKCAo488kiWLFnCc889R8+ePSvkwrQiIk7SyJGIiENSU1P3mzWuPNcvi6OOOorXXnuNQCDATTfdxB133EHdunV59913OeKII/ZZ97777uOLL75g+PDhzJkzhyeeeILTTz8dKJyZ7s4772Tu3LlcccUVPPTQQzRo0KBo2ui9RzL69+9P9+7dueWWW7jnnnvo0aMHb7zxRqnOCWrZsiWnnHIKb7311n4l6LjjjsPlchVlPJTMzEzOO+88zjvvPM4//3z+9a9/MW3aNK6//npefvllPB7PAe/XunVrXnzxRXJzc7npppu49tpryc7O5tVXX6VZs2ZA4UhWw4YNueaaa/joo49K9BwP9XodffTR3HbbbcyePZsrrriCzz77jOeee26fcpSYmMhll13GlClTuOKKKwiHw/s9zv33388111zD5MmTGT58OG+99RaXXHIJ//vf/8o08iUiEosMe++LFIiIiFRzl19+OT6fjzFjxjgdRUREYowOqxMRkRphzJgxrFmzhp9++okJEyY4HUdERGKQypGIiNQI3377LWvXruXWW2+la9euTscREZEYpMPqRERERERE0IQMIiIiIiIigMqRiIiIiIgIoHIkIiIiIiICqByJiIiIiIgAKkciIiIiIiJADZjKe/v2HJyej88wICMjKSaySNWgfUZKSvuMlJT2GSkp7TNSUrG0z+zJcijVvhzZNo5/M/aIpSxSNWifkZLSPiMlpX1GSkr7jJRUVdpndFidiIiIiIgIKkciIiIiIiKAypGIiIiIiAhQA845EhEREZGay7ZtLCuKZVlOR6lxDAMCgQDhcKjCzzkyTRPTdGEYRpm2o3IkIiIiItVSJBJm584dhMMBp6PUWDt2mJVWTL1eP8nJ6bjdnlJvQ+VIRERERKod27bZvn0zpmmSklILl8td5lEFKTmXyyAardhhI9u2iUYj5OZms337ZurUaVTq77XKkYiIiIhUO5FIGNu2SEmpjdfrdzpOjeV2m0QilTFy5MPlcrFjxxYikTAej7dUW9GEDCIiIiJSbRmGft2tKcrje629RUREREREBB1WJyIiIiI1jGkamGblnX9kWTaWVcHTtUm5UDkSERERkRrDNA3SU+MxXJV3AJUdtdiRnV/sgnTMMd3p3/9E7rrr/n2Wf/bZZF59dSzvvz+51Fl+/XUGr746luXLl+F2u2nfvhNXXDGC1q3blHqb1YnKkYiIiIjUGKZpYLhMAhMmYm/dWuGPZ9Spg//C8zBNo0SjR1OmfMmpp55Bt25HlluWpUuXMHLkv7nmmn8xevTdhEJBJk16l+uvv4o33nib+vUblNtjVVUqRyIiIiJS49hbt2Jt2Fjhj1Pa8an69RvwxBMP8/rrb+PxlP66PXv7+uvP6dGjF4MHn1O07OabRzF79iymTPmKiy8eWi6PU5VpQgYRERERkRhzxRUjyMzMZMKEN/92na1bt3DHHSMZOLAfJ598PE899SihUOhv1zcMk5UrV5KVtWOvZQZPPTWG008/E4BXXnmJa68dvs/9zj77VD77rPBQvkgkwksvjeH000/kxBOP5fbbb2PnzmwACgoKeOSR+xk06HgGDTqehx++n2AwCEBOTg733nsHJ5xwLKeffhJPPvkIweCfF+fds81+/Y7m2muHs3r1qqLHe/jh+zj55OMZMKAPt912I5mZFTfip3IkIiIiIhJjatWqzbBhw3nzzVfZuHHDfreHw2Guv34EgUABzz03lnvueYiff/6J559/5m+3ecopp5OdvYOzzjqVkSNv4v3332HDhvXUq1ef5OSUYuV6+eUX+fzz/2PUqP/y4ouvkZW1g0cffQCAhx66lwUL5vPQQ4/z5JNjWLhwHi+99Pzu2+4hNzeXF154hQcffIwlS37jiSceAWDq1O/45JMPuOeehxk3biIZGRk8+ODdAEyaNJG5c+fwxBNjePnlceTn5/PMM0+U6LUsCZUjEREREZEYdPbZ59OoUWOeeuqx/W6bOfNntm3byh133Evz5i3o1u1IbrrpNj788D3y8/MPuL2mTQ9n7Ng3OO64fsybN4ennnqM8847gzvuGEkgEDjgffZm2zaTJ3/I8OFX06tXbw4/vBk33zyKww9vzq5du/j++2+46aZb6dixM61ateaWW/5DvXr12bBhPT/+OLUoa9u27bntttv5/PP/Izc3l82bN+J2e6hbtx4NGzbiX/+6lWuvvQmATZs24fP5qF+/Pk2aNGX06LsYMmRomV7Xg9E5RyIiIiIiMcjlcnHzzSO5+urL+eGH7/e57fff13DYYY1JTk4uWtahQ0ei0SgbNqzjxRfHsGDB3KLbvv76RwAOP7wZd955L5FIhEWLFjBlyldMnvwhGRm1+Ne/bj5onuzsbHbu3EmrVn/ObHf44c0YNuxKlixZTDQa3WfWu06dutCtWzemTp2KZVmceebAfbZnWRbr16+jf/8TmTTpXc499zTatetAnz7HccoppwNw2mlnMmXKl5x22ol06dKNf/yjL4MGnVKyF7IEVI5ERERERGJUhw6dOPnk03j66ce48MJLipZ7vb791o1GraKPI0feXnS+zx7PPfcUJ544iJYtj8DtdtO5c1c6d+5KQkIC06YVlifD2P/6T9FoFAC3+++rw8Fui0ajJCYm8vLL4/a7rXbt2vh8fiZMmMQvv8zg559/5O23xzF58oe89toEmjVrzvvvT+bnn3/i559/5KWXnuPrr79gzJj/HTBrWemwOhERcYxpGrjdZkz/q8wLRYqIHMiIEdcRCBTwzjvji5Y1btyEdevWsmvXzqJlixcvwOVy0bBhI2rXrkOjRocV/YPCaxztmVhhb4mJSaSmpgLg8Xj2OSwvPz+/aAKHpKTC9VauXF50+4oVyzjzzEHUr98Ql8vFihUrim778cfvueSSC2ncuAm5ubkYhlGUJxgMMmbM04RCYX7++ScmT/6I3r2P4eabR/H66xNYt24tq1at5PPP/49p036gX7/+3H773Tz22LMsWDBvn0klypNGjkRExBFOXIixNEp68UYRqRqMOnUqZZTAqFOnzNtISUllxIjreOih+6hXrz4ARx7ZkwYNGnLvvXdy1VXXsXNnNk8++SgDBpxEUlLSAbdz6aWXc9dd/8Hr9XLCCQPxeNwsWDCfCRPeZPTo/wLQunVbXn75Rb79dgotWrTk1VfHYpquom2cffb5vPzyi9SuXYfU1DSefvpx2rXrQGJiIieddDJPP/0oN988CtM0eeml5zn66KNp2vRwevbszd13386NN96Cabp4+OH7SE5OJikpCcuyGDPmKdLTMzjiiFZMmfIlfr+fww5rzJIli3jhhddISUmlQYOGfP3159SpU5eUlNQyv64HonIkIiKOqOwLMZZGaS/eKCKxy7Js7KiF/8LzKu0x7ahV5veQk08+nU8//YTMzEyg8Hykhx56gieffIThwy8lPj6BE044ieHDr/nbbfTr1x+v18Pbb4/no4/eJxyO0Lx5C0aNupNjjjkWgO7de3DeeRfyyCP343KZnHfeRWzbllm0jSFDhpKTk8Odd44kEonQu3cf/vWvWwC44YZ/89RTj3Hjjdfg8Xjo128AV15ZmOeOO+7hyScf4YYbrsblctGz51HceGPh/Y455h8MG3YVzz77BDt2bKdx46Y8+ODjJCcnM3jwuWzdupV7772TnJxdtGrVhoceehyXy0VFMGzbrtbv9tu25eD0MzQMqFUrKSaySNWgfUZKqiruM263SVpaAgVPPVspF2IsDbNhA+L+dR1ZWXlEIpbTccpVVdxnxFlVbZ8Jh0Ns376JjIz6eDzefW4zTaNSD5m1LLvG/oHF7TYr7f3zYN/zPfvvoWjkSERERERqlJpcVuTgYvtAbxERERERkUqiciQiIiIiIoLKkYiIiIiICKByJCIiIiIiAqgciYiIiIiIACpHIiIiIiIigMqRiIiIiIgIoOsciYiIiEgNo4vAyt9RORIRERGRGsM0DVLT4nGZlXcAVdSyyM7KL3ZBikQivPHGK3zxxWds27aVtLR0+vY9nmHDriQ+PqGC09ZsKkciIiIiUmOYpoHLNJkwdyJbc7dW+OPVSazDhV3OwzSNYpejF154hl9/ncltt42mYcNGbNiwnqeffox169bxyCNPVnDimk3lSERERERqnK25W9mwa6PTMQ7os8/+j1Gj7qR79x4A1K/fgJtv/g/XXHM527Zto1atWg4nrL40IYOIiIiISAwxTYM5c37FsqyiZe3bd2DcuHdJTU3l7LNP5bPPJhfdNmfOLI45pnvR1+vXr+Omm65jwIA+DB58Mu+9907RbUuWLGbEiGEcf/zRnH/+YKZM+bLotvnz5zJs2MX063c0l1xyHt9//03RbZs3b+bGG69hwIA+nHLKAJ588hEikQgAK1Ys56qr/snxxx/NGWcM5LXX/lchr0tl0MiRiIiIiEgMOeecC3j55Rf54Yfv6d37GLp370GPHkdx+OHNDnnfYDDIjTdeS6tWrXjppdfZuHEDd989mgYNGtK2bTtuvPEaTjhhIKNG3cGiRQu5//67aNLkcNLT07n11n8xfPjV9OzZm8WLF3L//XeTlpZOp05deOqpR4iLi+e11yaQlbWD22+/lSZNDmfw4HO4777/0rFjZ+68817Wrv2D22+/ldat23DUUcdUwqtVvlSORERERERiyNChl9OgQUM+/PA9PvnkQz76aBLx8QnccMO/Ofnk0w56319/nUF2dhb/+c9/iY9PoFmz5vzrX7dgmiZTpnxFUlJK0deNGzdl166dBINBPvjgPbp378FZZ50HQKNGh7F8+TLefXcCnTp1YdOmTbRq1Zp69erTqNFhPPro0yQlJQOwefNG+vQ5lnr16tOgQUOeeup56tdvUOGvU0VQORIRERERiTEnnDCQE04YyM6d2cycOYNJkyby0EP30rx5y4Peb+3aPzjssMb7zGq3p1A9/vjDHHHEEZh7zdR3/vlDAHjnnXFMm/YjAwb0KbotEolw2GGNAbjookt44IG7+eGH7+jZszfHH38CRxzRGoCLL76Ml14aw8cff0Dv3sdw4omDyMiomudFqRyJiIiIiMSIlStX8Pnn/8d1190IQEpKKieccBJ9+x7PeeedwZw5v2IY+16jKRqNFn3udv/9r/cHuy0ajXLCCQO55JJ/HvA+J5wwkG7djuTHH7/n559/4o47buOiiy5l+PCrGTJkKP36DeCHH75j2rQfueGGEdx662hOPfWMkj35GKAJGUREREREYkQ0GmXixLdYvnzpPss9Hg9+v5/U1DTcbjf5+XlFt23cuKHo80aNGrNhwzoCgUDRsueee4qnnnqURo0OY9Wqldj2n1OK33nnKCZMeJPDDmvC+vXraNTosKJ/P/44la+++hyAl14aw44dOzjjjLN55JGnuPzyEUyd+i3BYJCnnnoMj8fD+ecP4dlnX+K0087k+++/raiXqEJp5EhEREREapw6iXVi8nFatWpN797HMHLkv7nqquvo0KEj27dv54sv/o9QKMRxx/Vj1qxf+L//+4SuXbuTnZ3NO++ML7p/jx69SE/P4NFH7+eSS4axbt0ffPzxJO6++0E6dOjEyy+/yPPPP8Npp53JwoXz+emnqVx88VCSkpJ5//2JjB37PAMHnsKSJb8xduwYRo26E4C1a3/nyScf4aabbsM0TWbMmEbLlq3w+XwsWDCPrVu3cNVV15Cfn8/8+XPp0+e48nwZK41h710dq6Ft23Jw+hkaBtSqlRQTWaRq0D4jJVUV9xm32yQtLYGCp57F2hCb1xoxGzYg7l/XkZWVRyRiHfoOVUhV3GfEWVVtnwmHQ2zfvomMjPp4PN6i5aZpkJoWj8usvAOoopZFdlZ+sS8CGwgEeOONV/juu2/YunUzfn8cPXr04qqrrqNevXps2rSR+++/i8WLF9K4cVMuueSf/Pe/o/jpp1kA/PHH7zzxxMMsXLiAjIwMLrroEs4442wAFi1awNNPP87Klctp0KAhw4dfzbHH9gPg119n8sILz7JmzSpq1arD+edfWDRBQ1bWDh5//CFmzfqVaDRK795Hc+ONt5Gamsr69et44omHWbRoIS6Xi379+nP99Tfh8/lxu81Ke//8u+85/Ln/HorKUSWoam8m4jztM1JSVXGfUTlyVlXcZ8RZVW2fOdgvyqZpYJrG39yz/FmWXexiVN1UtXKkw+pEREREpEapyWVFDk4TMoiIiIiIiBBD5Wj48OGMHDmy6OvffvuNc845h06dOnHWWWexaNEiB9OJiIiIiEh1FxPl6NNPP2Xq1KlFX+fn5zN8+HC6d+/OBx98QJcuXbjyyivJz893MKWIiIiIiFRnjpej7OxsHnnkETp06FC07LPPPsPn83HrrbfSvHlzRo8eTUJCAl988YWDSUVERESkqqnmc4/JXsrje+34hAwPP/wwp59+Olu3bi1aNn/+fLp161Z09V/DMOjatSvz5s1j8ODBJdq+UXkTkRwyQyxkkapB+4yUVJXeZ4wYzr1XrpjNWEpVep8RR1S1fcbtdgEQCgXxen0Op5HKEAoFAXC73fvtp8Xdbx0tR9OnT2fWrFlMnjyZu+66q2h5ZmYmLVq02GfdjIwMVqxYUeLHyMg49JR9lSWWskjVoH1GSqoq7jNxfi/Ex+gvLv7CqWDT0hIcDlJxquI+I86qSvtMJJJPVlY2LpeBz+djn794SKUIhSrjUWyCwSD5+TvJyEinTp2UUm/JsXIUDAb573//y5133onf79/ntoKCArzefecm93q9hErx6m7f7vxc/IZR+EYSC1mkatA+IyVVFfcZl2v3dY4CIez8oNNxDsgIhIgDsrLyiEar33WOqto+I86qivuMx5OIzxdi584dTkepsUzTxLIq5/0zLi4RjyeRbdty9rttz/57KI6Vo+eee4727dvTp0+f/W7z+Xz7FaFQKLRfiSoO2yZm/gOOpSxSNWifkZKqkvtMDGc29soVqxnLqkruM+KoqrXPGKSkZJCUlEY0GnE6TI1jGIUj71lZeRW+z7hcbkyzcDqFsjyWY+Xo008/Zdu2bXTp0gWgqAx9+eWXnHLKKWzbtm2f9bdt20adOnUqPaeIiIiIVG2maWKa3kOvKOXKMMDv9+PxhKtMoXasHI0bN45I5M8G/9hjjwFw88038+uvv/K///0P27YxDAPbtpkzZw5XXXWVU3FFRERERKSac6wcNWzYcJ+vExIKT3Zt0qQJGRkZPP7449x///2cf/75vPPOOxQUFDBw4EAnooqIiIiISA3g+HWODiQxMZGXXnqJ2bNnM3jwYObPn8/YsWOJj493OpqIiIiIiFRTjl/naI+HHnpon687duzIhx9+6FAaERERERGpaWJy5EhERERERKSyqRyJiIiIiIigciQiIiIiIgKoHImIiIiIiAAqRyIiIiIiIoDKkYiIiIiICKByJCIiIiIiAqgciYiIiIiIACpHIiIiIiIigMqRiIiIiIgIoHIkIiIiIiICqByJiIiIiIgAKkciIiIiIiKAypGIiIiIiAigciQiIiIiIgKA2+kAIiIiAOTn49q0ATMzE2PXTsydOzFydkE4DFELw4qCaWLHJ2DHx2MnJGJlZBCt3wCrbj3w+51+BiIiUsWpHImIiDNycuCbz/G8NR5z5XLM7Oxi3c3Izz/gciujFpGWRxBp1Rq7Vm0wjHIMKyIiNYHKkYiIVJ6CAnyTP8L30SS8P3wPodA+P4isjFpE69bFTk3DTk7GSk4Brw/bZYJpgmVh5OVh5Odh5uRiZm7B3LQJc2c25vZteLdvwzvjZ6z0dCLtOxHu1Bl8PoeerIiIVDUqRyIiUuFcS37DP+41/O9NxNyZ/ecNLVoQPqwx0fRaROs3KNahcXbtwo/RvRfm5+P6Yw3uZUtxrVmNuWMH3h++wzPjZ8KduxDp2h07MbE8n5KIiFRDKkciIlIxbBvPjJ+Jf/pxvN9OKVocPawxgQuGEDnjTFJ6dSPy9HNYGzaW7bHi44m2aUe0TTsIBnEvX4rn118wd2zH+8sMPHNmET6yJ+EevcDjKeMTExGR6krlSEREyp3n+29JeOwhPL/MAMB2uQgNPIWCi4cSPrYvmCZut1kx5wX5fEQ6dCLSviOu1SvxzJyBa+MGvNOn4V68iFC/44k2b6lzkkREZD8qRyIiUm5cixaSePfteKd+B4Dt9RK44GLyr7keq+nhlRvGMIg2b0m0WQtcy5fh/f4bzF078X/0AZGWRxA8YSDExVVuJhERiWkqRyIipWSaBqYZW6MPLte+l6+zLBvLsiv8cY0tW0i877/43n0bw7axPR4K/nkFBdf+q3CabScZBtFWrSk4vBmemdPx/DoT94rlmJs3EzzlNKyGjZzNJyIiMUPlSESkFEzTID01HsMVW9fSTktL2OdrO2qxIzu/4gqSZeF/8zUS7rsLc9dOAAJnnkXeqDsrf6ToULxewn2OJdqyFb7/+xgzOwv/O28RProP4Z5H6TA7ERFRORIRKQ3TNDBcJoEJE7G3bnU6DhgQ5/dSEAjB7h5k1KmD/8LzME2jQsqRa/Eikm6+Hs/sWQCEO3ch96HHiXTtXu6PVZ6sevUouGQovq+/wr1kMd6ffsDclknwpJPBrR+LIiI1mX4KiIiUgb11a9lnWisHhgHE+7Dzg9i7e1CFjWlFo8SNeZqEh+/HCIexEpPIG30ngaGXg8tVUY9avrw+goNOIdq4Md6vv8S9dAlGTg6BM87SeUgiIjVYbB0PIiIiMc1cs5rU0weSeN9dGOEwwZNOJuvnWQSGXVl1itEehkGkQycCZ52L7fXh2rCeuAnjMLKznU4mIiIOUTkSEZFi8b0/kfS+R+P5ZQZWYhK7nnmBXW9MwKpX3+loZWI1aUrBBUOwkpIxs3bgf+ctjKwsp2OJiIgDVI5EROTgAgES/30DyVdfgZGfR6j3MWR9/zPB8y+qNpMY2LVrE7joEqyMWpi5OfjfnYCRrYIkIlLTqByJiMjfMtesJnVQf+LGvYZtGOT9+zZ2TpqM1biJ09HKnZ2YSMG5F2ClZ2Dm5OCf+DbGtm1OxxIRkUqkciQiIgfk+eF70k48Ds+iBVgZGex85wPybxtd9c4tKomEBALn7SlIu/A+/yysX+90KhERqSQqRyIish//q/8j5bwzMbOzCXfrTta30wj3Pd7pWJXCTkgkcO4FWOnpmFlZMGgQ7L6Gk4iIVG8qRyIi8qdIhMRbbyRp5L8xolECZ59H9oefYdVv4HSySmUnJhI46zzspGRYuJDEoRdDOOx0LBERqWAqRyIiUig/n+ShFxL3+ivYhkHu7XeTM2Ys+P1OJ3OEnZJC8IorISEBz/ffknjzDRRdREpERKollSMREcHYsZ3Us0/D99UX2H4/u157i4Lrb6w2s9GVln3YYfDuu9imSdzb44l/8lGnI4mISAVSORIRqeHMDetJPe0kPLN+wUpNJfu9TwgNOsXpWLFj0CDyH3sSgPiH78f7zVcOBxIRkYqiciQiUoOZa1aTeuqJuJcvI9qgIdmffEmkZy+nY8Wc0NBhFFw6DMO2SRpxOeYfvzsdSUREKoDKkYhIDeVavZLUMwbhWr+OSIuWZH/6NdHWbZyOFbNy73uIcLfumNnZJP/zYigocDqSiIiUM5UjEZEayLViOSmnD8K1aSORVq0LZ6Rr2MjpWLHN52PXK+OwatXCs3A+SbfdpAkaRESqGZUjEZEaxrVyReGI0ZbNRNq0I/uDT7Hr1nU6VpVgNWjIrpdewzZN/O+8he/dt52OJCIi5UjlSESkBjHXrSXl7NMwM7cSadeB7A/+D7t2badjVSnhPseSf9toABJH3YL5+xqHE4mISHlRORIRqSGMLVtIOfs0XBs3EDmiFdnvf4KdkeF0rCop//qbCPXqjZmbQ/LVV0Ak4nQkEREpBypHIiI1gJG1g9RzT8e9ZjXRxk3Y+d7HKkZl4XKRM2YsVlIynlm/6PpHIiLVhNvpACIiUrFcoSBJQ87DveQ3rHr1yP1wMuZhjRz/65jL5XSC4jtg1sObkv/4UyQO/yfxjz9MtN/xRB2aBt2ybCxLk0OIiJSVypGISDVlJCVCJELyNcPh15mQmor59dektG/vdLR9GYbTCf6WkZQIlkVyctyBV7jiMpj6DcZbb5F83VUwfz7Ex1duSMCOWuzIzldBEhEpI5UjEZHqyh8Ho0bBpEnYLhehiy7GmvIdTPnO6WQAmK1a4Rt4ArFbjSh8DU2T0NsTiW7ZeuB12nXAn5KCsXIl4VNOJXLaGZUa0ahTB/+F52GahsqRiEgZqRyJiFRTrq+/gjdeAyB44iCicQmwYaPDqf5kVKFZ8qytmVgHee2C/frj/3AS7u+/I9LgMKz69SstW9U5OFFEJPbpPVVEpBpy/b4G95uvAxAedDLRtu2cDVTNRZu3JNK6LYZt4/vyM4hGnY4kIiKloHIkIlLNGNlZ+CZ/hGHbMHQo0QEnOh2pRgj2648dF4+5LRPPjJ+djiMiIqWgciQiUp2EQvg/+gAjGMRq3gJefDGmJzyoVuLjCR4/AADPzOkY2zIdDiQiIiWlciQiUl3YNt7PP8XclomVkEDohhvB53M6VY0SbdWaSPMWGJaFb8pXYGuCBBGRqkTlSESkupg2DffyZdimSfC0MyE93elENY9hEOo3ANvtxrV+Ha4li51OJCIiJaByJCJSDZjr18G33wIQOn4AVsNGDiequeyUFMK9egPg/f47CAQcTiQiIsWlciQiUtXl5+Od/AnYNpG27Yh07Ox0ohov3L0HVlo6Zn4e3mk/Oh1HRESKSeVIRKQqs218X3yKmZsDGRmEBpyoCRhigdtNsP8JhZ/Om4O5ZbPDgUREpDhUjkREqjD3rF9wr16F7XLBOeeA1+t0JNnNatKUSKs2GLaN95uvNTmDiEgVoHIkIlJFmZs24f1xKgDhfv2hbl2HE8lfhY7rh+324Nq4AdeypU7HERGRQ1A5EhGpisJhfJ9PxrAsIke0JtKps9OJ5ADspCTCPXoC4P3hOwiHHU4kIiIHo3IkIlIFeX/8HnPHDqyERII6zyimhY/siZWUhLlrF57ZvzodR0REDkLlSESkijH/+B3PnNkAhE4aBHFxDieSg/J4CPU5rvDTmdMxcnOdzSMiIn9L5UhEpCoJBPB98SkA4U5diB7ezOFAUhzRNm2J1m+AEQ7j+ekHp+OIiMjfUDkSEalCvN9NwczJwUpNJXRsX6fjSHEZBqG+xwPgXrQAc+sWhwOJiMiBqByJiFQRrt/X4Fm8CBsIDjxF03ZXMVaDhkRat8EAPD9MdTqOiIgcgMqRiEhVEArh/eoLACJdu2M1bORwICmN0DH/wDZN3L+vxlz7u9NxRETkL1SORESqAO+0HzF37cRKSiZ0zD+cjiOlZKemFU277p36vS4MKyISY1SORERinLlpE+45swAIDThRh9NVcaFeR2N7vLi2bMa1fJnTcUREZC8qRyIisSwaxfvVZxi2TaRNW6LNmjudSMoqIYHwkT0A8P40FaJRhwOJiMgeKkciIjHMPWcWrsxM7Lg4gn37Ox1Hykm4+5HY8fGYWVm4F853Oo6IiOymciQiEqOM3By8P08DIPSP4yA+3tlAUn68PkK9jgbAM/1nCIcdDiQiIqByJCISs7xTv8cIh4jWb0CkfUen40g5i3TshJWUjJmXi3v+XKfjiIgIKkciIjHJXLcW95LF2EDo+AFgGE5HkvLmdhPuXTh65J05A0IhhwOJiIjKkYhIrLEsfN98DUCkU2esevUdDiQVJdK2PVZqKkZBPp65s52OIyJS46kciYjEGPe8OZjbMrH9fkLHHOt0HKlILhfho44BwPPrTAgGHA4kIlKzqRyJiMSSggK8034EINTnWIiLcziQVLRIm7ZY6RkYgQCe2bOcjiMiUqOpHImIxBDv9GkYwSDR2rWJdOjkdBypDKZJqPfu0aNZv0JBgcOBRERqLpUjEZEYYWTtwD1vDgChY/uBqbfomiLaqjVWrdoYoSCeORo9EhFxin7yiojECO8P32NYFpHDm2E1PdzpOFKZDINQr94AheUoGHQ4kIhIzaRyJCISA8x1a3GvWI5tGISO7et0HHFA9IhWheceBYN4do8giohI5VI5EhFxmm3j/f5boPDCoHat2g4HEkeYJuGeRwHgmfWLrnskIuIAlSMREYe5li3BtWUztsdLqHcfp+OIgyJt2mKlpGAUFOBeON/pOCIiNY7KkYiIk6JRvD8VTt0d7tETEhIcDiSO2nv06NeZEIk4HEhEpGZRORIRcZB78ULM7CzsuHjC3bo7HUdiQKRte6ykJMzcXNyLFjodR0SkRlE5EhFxSiSC5+dpAIR6HQVen8OBJCa43YSP7AmA55fpEI06HEhEpOZQORIRcYh73hzM3ByspGQinbo4HUdiSKRDJ+z4eMxdu3D/ttjpOCIiNYbKkYiIE0JBvDOnAxDufTS43Q4Hkpji8RDuvnv0aOZ0sCyHA4mI1AwqRyIiDvDM+hWjoAArLZ1Iuw5Ox5EYFO7cGdvvx8zOwrVsqdNxRERqBJUjEZHKFgjgmf0rAKGj+4Cpt2I5AK+PcLcjCz+d8TPYtsOBRESqP/1EFhGpZJ45szCCQayMWkRbtXY6jsSwcJdu2F4f5vZtuFYudzqOiEi1p3IkIlKZgnuNGh11NBiGw4Ekpvn9hLt0BcCj0SMRkQqnciQiUok8c2Zr1EhKJNztSGy3B9eWLbh+X+N0HBGRas3RcvTHH38wbNgwunTpwnHHHcfLL79cdNu6desYOnQonTt3ZtCgQfz0008OJhURKQfB4F6jRr01aiTFEx9PpGMnADy/znQ4jIhI9eZYObIsi+HDh5OWlsaHH37I3XffzQsvvMDkyZOxbZtrrrmGWrVqMWnSJE4//XSuvfZaNm7c6FRcEZEy88ydjREIYKVnED1Co0ZSfOFuR2IbBq61f2Bu2ex0HBGRasuxC2ts27aNNm3acNddd5GYmEjTpk056qijmD17NrVq1WLdunW88847xMfH07x5c6ZPn86kSZO47rrrnIosIlJ6oSCeWb8UfnpUb81QJyVip6QQbdUG99Lf8Pz6C8FTTnM6kohIteTYT+c6derw1FNPkZiYiG3bzJ49m19//ZUePXowf/582rZtS3x8fNH63bp1Y968eU7FFREpE8+8ebtHjdKJtmrjdBypgsJH9gDAtWwJxs5sZ8OIiFRTMXFJ9n79+rFx40b69u3LiSeeyAMPPECdOnX2WScjI4PNm0t+KEEsHNK/J0MsZJGqQftMFWIU4/sUieCZXThqFO7ZC8NVsX+XKspjHGBZLIn1fBBTGe169Yg2aYrrj9/xzP6V8PEDdgf7c52SZNT7jJSU9hkpqVjaZ4qbISbK0TPPPMO2bdu46667ePDBBykoKMDr9e6zjtfrJRQKlXjbGRlJ5RWzzGIpi1QN2mdiX5zfC/G+g680ayHk5UFyMr5uXcHlqrA88Xtn8XkA8Ps9h87ohFjPB7GXsc8x8MfveBYuwNP/eIiLA3/hz8u0tIRSbVLvM1JS2mekpKrSPhMT5ahDhw4ABINBbr75Zs466ywKCgr2WScUCuH3+0u87e3bcxy/LIRhFO4UsZBFqgbtM7HP5TJJS0ugIBDCzg/+/YqWhf+naZhAqPuRRIIRIFIhmeLjfeTvlcUMhvEDgUAY62AZHRLr+SAGM9ZriL92HczMrYR+nkHkqN4YgRBxQFZWHtGoVexN6X1GSkr7jJRULO0ze7IciqMTMsybN4/+/fsXLWvRogXhcJjatWuzevXq/db/66F2xWHbsXPNvFjKIlWD9pkq4BDfI9eypZg7s7Hj4gi37wQV9P3c+3CBojz2AZbFkljPBzGY0SB0ZE/8n03GPWc24e49MMqYUe8zUlLaZ6SkqtI+49iEDOvXr+faa69ly5YtRcsWLVpEeno63bp1Y/HixQQCgaLbZs+eTadOnZyIKiJSOraNZ+YMAMJdu8NfDhcWKY1oq9ZYScmY+Xm4f1vkdBwRkWrFsXLUoUMH2rVrx3/+8x9WrlzJ1KlTefTRR7nqqqvo0aMH9evXZ9SoUaxYsYKxY8eyYMECzj77bKfiioiUmGvNalyZW7E9XsKduzodR6oLl4twt+4AeH79BaziH0onIiIH51g5crlcPP/888TFxXHeeecxevRoLr74Yi655JKi2zIzMxk8eDCffPIJY8aMoUGDBk7FFREpMc8vhaNGkU6dC0+cFyknkY6dsH0+zKwdmIs1eiQiUl4cnZChbt26PPfccwe8rUmTJowfP76SE4mIlA9z0yZc69dhmybhbkc6HUeqG6+PcOeueGdOx/3tN06nERGpNnSJdhGRCuCZVXhdo0jrNthJVWcKU6k6Il26YbtcuH5fAz//7HQcEZFqQeVIRKScGTt34lq+FIBI9x4Op5Hqyk5MJNK2XeEXjz7qbBgRkWpC5UhEpJx55szCsG2iTZpi1anrdBypxsJ7yvfHH2OuXOFsGBGRakDlSESkPAUCuBfMB/b6xVWkgtgZtYi2bQe2je+l552OIyJS5akciYiUI8+C+RjhEFat2kSbHu50HKkBIsf1BcD39lsYWTscTiMiUrWpHImIlJdoFPecWQCFM9QZhsOBpCawWrSEjh0x8vPxj3vD6TgiIlWaypGISDlxLV+KmZuDlZBApE1bp+NITWEYcOONAMS98hKEww4HEhGpulSORETKiWd24ahRpHNXcDt6GTmpaS64AKtOHVybNuKb/JHTaUREqiyVIxGRcmBu3IBr8yZsl4twx85Ox5Gaxucj+M8rAIh7aQzYtsOBRESqJpUjEZFy4Nl9rlGkdVtISHA4jdREwcuGYft8eObOwf3LTKfjiIhUSSpHIiJlZOTk4Fq+DIBIt+4Op5Gayq5dh8DZ5wEQ/9IYh9OIiFRNKkciImXknjcHw7KINjpMF30VRxUMvxoA72eTMf/43dkwIiJVkMqRiEhZhEJ4FswDINxVo0birGibtoSO7YthWcS9MtbpOCIiVY7KkYhIGbjmzMYoKMBKTibaoqXTcUQouOoaAPxvvYmRm+NwGhGRqkXlSESktGwb948/ABDu0g1MvaWK80J9+xNpeQRmzi78E8Y5HUdEpErRT3IRkdKaPh1z4wZst5tIh05OpxEpZJoUXDECgLixL0I06nAgEZGqQ+VIRKS0nn8e2D19t9/vcBiRPwXOvQArLQ3X2t/xfvGZ03FERKoMlSMRkVIwMrfCe+8BEOnc1eE0In8RH0/gkn8Cuy8KKyIixaJyJCJSCt63xkEohNW4CVa9ek7HEdlPwT+vwHa78c74Gff8uU7HERGpElSORERKKhrF99orAESOPsbhMCIHZtVvQPC0MwGIe/klh9OIiFQNKkciIiXk/eYrXOvWQno60c5dnI4j8rcKrrgKAN+H72NkZjqcRkQk9qkciYiUkP+1lws/+ec/wet1NozIQUS6HUm4azeMUIi4ca85HUdEJOapHImIlIC5ZjXeb6cUfnHVVc6GESmGgssL91P/ay9DOOxwGhGR2KZyJCJSAnFvvoZh24SPHwDNmzsdR+SQgqedSbROXVxbNuP7v4+djiMiEtNUjkREiqugAP+ENwEIDrvC4TAixeT1Erh097Te/3vR4TAiIrFN5UhEpJh8n3yImZVFtNFhhAec6HQckWIruOSf2B4Pnlm/4J472+k4IiIxS+VIRKSY4l4vnIih4NJ/gsvlcBqR4rPr1iV4+mBA03qLiByMypGISDG458/FM3sWtsdD4MJLnI4jUmJF03p/NAlj61aH04iIxCaVIxGRYvC/XnjR1+CpZ2DXru1wGpGSi3TpRrjbkRjhsKb1FhH5GypHIiKHYGRn4f/gPQAKLtNEDFJ17Rk98r/+CoRCDqcREYk9KkciIofgnzgBo6CASNv2RHr0dDqOSKkFTzmdaN16uLZsxqtpvUVE9qNyJCJyMLaN/83CQ5AKhg4Dw3A4kEgZeL0Ehg4DNK23iMiBqByJiByEZ+Z03CuWY8cnEDz7XKfjiJRZwcWXYXu9eGb9Cr/84nQcEZGYonIkInIQe0aNAoPPxk5McjiNSNnZdeoQPOOswi+efdbZMCIiMUblSETkbxjZWfh2n5cRGHKpw2lEys+eiRmYOBFjyxZnw4iIxBCVIxGRv+F7fyJGIFA4EUOXbk7HESk3kU5dCB/ZE8Jh/G++6nQcEZGYoXIkInIgtk3cuDcAKLj4Uk3EINXOntGjuNc0rbeIyB4qRyIiB+CeMwv3ksXYfj/BszQRg1Q/oVNOgwYNMDO34vvkQ6fjiIjEBJUjEZED8I8vHDUKnnoGdmqaw2lEKoDHA1dfDUDcy5rWW0QEVI5ERPZj5OzC/+H7QOG0xyLV1vDh2D4fnjmzcc/+1ek0IiKOUzkSEfkL3wfvY+TnEzmiFZGevZyOI1JxatcmeObZgC4KKyICKkciIvvZc0hd4CJNxCDVX8HlVwLg++RDzM2bHE4jIuIslSMRkb24F87HM38uttdL4NwLnI4jUuGinToT7nkURiSC/w1N6y0iNZvKkYjIXvzjXgcgOOgU7IwMZ8OIVJI9o0dxb76mab1FpEZTORIR2SMvD9+k9wAIDBnqbBaRShQcdCrR+prWW0RE5UhEZDffJx9i5uwi2qQp4WP+4XQckcrj8RC49J8AxL3yksNhRESco3IkIrJb3O5D6gouHgqm3h6lZim4+DJsrxfP7Fm45852Oo6IiCP0019EBHAtXYJn1i/YbjeB8y5yOo5IpbNr1yZ4+mAA4l4Z63AaERFnqByJiAD+8a8DEDphIHbdus6GEXFIwbDhAPg+moSRmelwGhGRyleqcrRu3bryziEi4pxAAP+7bxd+evGlDocRcU6ka3fCXbthhELE7f6DgYhITVKqcnTSSSdxzjnn8Prrr7Nly5byziQiUql8//cxZnY20UaHETrueKfjiDiqYFjhtN7+11+BSMThNCIilatU5ejHH39k8ODBfPvttxx//PEMGTKECRMmsGPHjvLOJyJS4fzj3wAgcMEQcLkcTiPirOBpZ2LVqo1r00a8n/+f03FERCpVqcpReno6F1xwAW+++SZTp07l5JNP5ocffqB///4MGzaMDz/8kIKCgvLOKiJS7lyrVuD9+Sds0yRw4cVOxxFxns9HwSVDAYh7WdN6i0jNUuYJGTIzM8nMzGTz5s1YlkVCQgLvvvsuxx13HF999VV5ZBQRqTD+8W8CEDp+AFbDRg6nEYkNgUuHYbvdeKdPw7V4kdNxREQqjbs0d1qyZAlffPEFX3zxBRs2bKB3795cdtll9O/fn4SEBACef/557rjjDk444YRyDSwiUm5CIfwT3wIgMGSos1lEYohVvwHBk0/D//EHxL06ltzHn3E6kohIpShVORo8eDDdu3dn6NChnHTSSaSlpe23Trdu3TSrnYjENO+Xn2Fu20a0bj1CA050Oo5ITCkYdiX+jz/A//5E8m6/Czst3elIIiIVrlTl6KGHHmLQoEF4PJ59lodCoaJzj3r27EnPnj3LJaSISEWIe/M1YPdEDO5SvR2KVFuRnr2ItOuAe/FC/BPGU3DN9U5HEhGpcKU652jkyJHk5OTst3zFihXcdNNNZQ4lIlLRzD9+xzv1OwBNxCByIIZBweWF03rHvfY/iEYdDiQiUvGK/afSCRMmcM8992AYBrZtc/TRRx9wvd69e5dbOBGRiuKfsHsihmP7YjU93OE0IrEpMPgcEu65A9faP/BO+YrQiQOdjiQiUqGKXY4uvPBCWrZsiWVZXHrppTzzzDOkpKQU3W4YBnFxcRxxxBEVElREpNxEIvjfLpyIoeDioc5mEYllcXEELrqU+OeeIu7lF1WORKTaK9FB9kceeSQA33zzDQ0aNMAwjAoJJSJSkbxTvsK1eRNWrVqETjrZ6TgiMa1g6DDinn8G79TvcK1YTrSl/ggqItVXscvRqFGjGD16NImJiTz33HMHXffBBx8sczARkYriH/86AIFzLwSv19kwIjHOatyE0AkD8X3xKXGvvETuQ487HUlEpMKU+SKwIiJViblxA94phReoDgy51OE0IlXDnokZfBPfxsjZ5XAaEZGKU+yRo71HgzQyJCJVlX/COAzLInTU0URbtHQ6jki5cblK9/fO4tzP7tuX6BGtcC1fRvx7bxMcPqLEj2NZNpZllyaiiEilKdWFPfLy8njhhRcYPHgwTZs2ZeTIkXz11Ve0bduWRx99lIYNG5Z3ThGRsotG8U8YB2jUSKoPIykRLIvk5LhS3T8tLaF4K95wPVxzDfGv/o/4W24Cs2RlzI5a7MjOV0ESkZhWqnJ01113sXTpUs466ywmT57MV199xQMPPMAXX3zB3XffzdixY8s7p4hImXmmfotr/Tqs1FSCp5zudByR8uGPA9Mk9PZEolu2Fv9+BsT5vRQEQlCcvhIM4vf7MZYvJ3j1tVit2xT/oerUwX/heZimoXIkIjGtVOVo6tSpvPnmmxx++OE8+uij9O3bl0GDBtG2bVvOPPPM8s4oIlIu4sa9AUDgnPMhrnR/ZReJVdbWTKwNG4u9vmEA8T7s/CB2MftKpG17PHNm4fr6KyJJKYe+w246wVlEqopSvV/Zto3H4yEQCDB9+nSOPfZYAHbu3El8fHy5BhQRKQ/Gli14v/wMgMCQoc6GEamiwl26AuBavQojO8vhNCIi5a9UI0e9evXijjvuID4+HtM06d+/P9OnT+fee++lX79+5Z1RRKTM/BPfwohECHc7kmibtk7HEamS7LR0Ik2b4f59NZ65cwj1Pd7pSCIi5apUI0cPPPAAbdu2xev1MmbMGBITE1m2bBnHHnsso0ePLu+MIiJlY1nEjd99SN3FQ53NIlLFRbp2A8C9aAGEQg6nEREpX6UaOUpKSuL222/fZ9nQoUPLI4+ISLnzTPsR1+9rsBKTCJw+2Ok4IlVa9PBmWKlpmNlZuJcsJtKpi9ORRETKTanKUTgc5qOPPmLhwoVEIhHsv5zJqesgiUgs8Y9/HYDgWedCQjGnLRaRAzMMwl264vvuGzxz5xDp2Hn37A4iIlVfqQ6rGz16NPfffz9ZWVn7FSMRkVhibN+O79PJAAQu1rWNRMpDpF0HbLcHc1sm5rq1TscRESk3pRo5+vrrrxkzZgxHH310eecRESlX/nffxgiFCHfsXPgXbhEpO7+fSLv2eObPxTN3NsHGTZxOJCJSLko1cpSUlETdunXLO4uISPmy7aJD6gJDNGokUp6KpvVeuQJj106H04iIlI9SlaMRI0Zw//33s2rVKiKRSHlnEhEpF+6ZM3CvWI4dH0/wrHOcjiNSrdi1ahNt3ATDtnHPn+d0HBGRclGqw+r+97//sXXrVk455ZQD3r5kyZIyhRIRKQ9xb74KQOCMs7CTkh1OI1L9hLt0w7X2DzwL5hE+6mhwl+rXChGRmFGqd7GHHnqovHOIiJQrY8d2fJM/AiBwyWXOhhGppqLNW2AlJWPm7MK99Dci7Ts6HUlEpExKVY569OgBQG5uLmvXrqVFixaEQiESExPLNZyISGn5330bIxgk3L4jkS7dnI4jUj2ZJpHOXfH++D3uubOJtOugab1FpEor1TlHoVCI22+/nR49enD22WezZcsWRo4cybBhw9i5UydliojDbBv/m68Bu0eN9MuaSIUJd+yE7Xbj2rIFc9NGp+OIiJRJqcrRI488wsqVK/nwww/x+XwAXHfddWRlZXHfffeVa0ARkZLyTJ+Ge+UK7PgETcQgUtHi4oi0bguAZ85sh8OIiJRNqcrRV199xejRo2nVqlXRslatWnHvvffyww8/lFs4EZHSKBo1OuscTcQgUgkie6b1Xr4UIzfX4TQiIqVXqnKUl5dHXFzcfsstyyIajZY5lIhIaRnbt+P7v48BCFw81NkwIjWEVbce0YaNMCwL9/y5TscRESm1UpWjfv368cQTT5C711+H1q1bx3333cexxx5bbuFERErKP3ECRihEuGNnIp27Oh1HpMYI7574xL1gHugPpSJSRZWqHN1555243W569uxJQUEBZ511FgMGDCA5OZk77rijvDOKiBSPbeMft9dEDCJSaaItj8BKSMTMy8O1fJnTcURESqVUU3lnZ2dz5pln0q5dO1q1asUff/xBnz59aNasWXnnExEpNs+0H3GvWomVkEhw8NlOxxGpWVwuIp064/35JzxzZxNt09bpRCIiJVaicjR9+nQefPBBVqxYgW3bRcsNw2Dy5MmMHDmS7t27l3tIEZHi8L/5KgDBs87FTkxyOI1IzRPp1BnPjJ9xbdyAuXkzVr16TkcSESmRYh9W99NPP3H55ZfTunVrxo0bx4wZM1i8eDEzZ87k9ddfp1mzZlx22WXMnasTMUWk8hmZmfg+nQxA4FIdUifiBDshkWirNgC452pabxGpeoo9cjRmzBiGDh3KLbfcss/ylJQUevbsSc+ePUlJSeGFF15g7Nix5R5URORg/O+8hREOE+7SlUiHTk7HEamxwl274V6yGPfS3wgd2xfi452OJCJSbMUeOVq6dClnnnnmQdc555xz+O2338ocSkSkRCwL//jXAQhc8k9ns4jUcFa9+kTr1sOIRvEsnO90HBGREil2OQoEAqSkpBx0nbS0NHbs2FHsB9+yZQvXX389PXr0oE+fPjz44IMEg0GgcGrwoUOH0rlzZwYNGsRPP/1U7O2KSM3i+ekH3GtWYyUlEzjjLKfjiNRshkGk6+5pvefNBctyOJCISPEVuxzZto1pHnx1wzD2majhUNu7/vrrKSgo4K233uLJJ5/ku+++46mnnsK2ba655hpq1arFpEmTOP3007n22mvZuHFjceOKSA3if7Nw+u7g2edCQoLDaUQk0qoNdlw8Zs4uXCuXOx1HRKTYSjRb3eeff05iYuLf3p6Tk1Psba1evZp58+Yxbdo0atWqBcD111/Pww8/zD/+8Q/WrVvHO++8Q3x8PM2bN2f69OlMmjSJ6667riSRRaSaM7ZuxfdZ4UQMBRdrIgaRmOB2E+7UGe+Mn/HMnUOobz+nE4mIFEuxy1GDBg149dVXD7le/fr1i7W92rVr8/LLLxcVoz1yc3OZP38+bdu2JX6vkzi7devGvHnzihtXRGqIuLfewIhECHc7kmj7Dk7HEZHdIp0645k5Hde6tRg68kNEqohil6Nvv/22XB84OTmZPn36FH1tWRbjx4+nV69eZGZmUqdOnX3Wz8jIYPPmzSV+HMMoc9Qy25MhFrJI1aB9BkzTwDjUCxCJEPdG4R9tQldcidtd7COFy8zl2v1YRux9n4ryGAdYFktiPR/UmIwV8tySk4m2PAL38mW4f/qhYh9LKoV+NklJxdI+U9wMJTqsriI9+uij/Pbbb7z//vu8/vrreL3efW73er2EQqESbzcjI3YuBBlLWaRqqNH7jGXBIc5z5IMPYOMGqF2bhKFDSPD5KifbXuL8Xoiv/Mf9O/F7Z/F5APD7PTGVsUis54MakTG+Ip9X76Ng+TLcs2dBVhZpaWkV91hSaWr0zyYplaq0z8REOXr00Ud54403ePLJJzniiCPw+XxkZ2fvs04oFMLv95d429u351DMOSIqjGEU7hSxkEWqhpq+z7hcJmlpCQTfnoi1devfrucd8xwuINypM5EXK/f6aq5WrfCedAKBQBgrP1ipj/134uN95O+VxQyG8UNMZdxbrOeD6p/xr/tMuatVF3/t2piZmfDaa2RddiXRqGavq6pq+s8mKblY2mf2ZDkUx8vRvffey9tvv82jjz7KiSeeCEDdunVZuXLlPutt27Ztv0PtisO2cfybsUcsZZGqoabvM9aWrVgbDnyugrF9G64Vy7ENg3CzFtjrK/ecBqNW7aLPY+F7tPfhAkV57AMsiyWxng+qdcYD7jPlziDcpRu+r76AMWPgksux7Rg4vkbKpKb/bJKSq0r7TOUdoH8Azz33HO+88w5PPPEEJ598ctHyTp06sXjxYgKBQNGy2bNn06mTrnovIoU8c+cAEG3eAjv54NdgExHnRNq0w46Ph9Wr8Uz5yuk4IiIH5Vg5WrVqFc8//zxXXHEF3bp1IzMzs+hfjx49qF+/PqNGjWLFihWMHTuWBQsWcPbZZzsVV0RiSSiI+7dFAIS7dHM4jIgclMdDpGcvAHxjX3Q4jIjIwTlWjr755hui0SgvvPACxxxzzD7/XC4Xzz//PJmZmQwePJhPPvmEMWPG0KBBA6fiikgMcS9ejBEKYaWnYzVu4nQcETmE6NF9wDDwfPcNrpUrnI4jIvK3HDvnaPjw4QwfPvxvb2/SpAnjx4+vxEQiUiXYNp65swEId+4aG/ODishB2RkZcMopMHky/lfHkvfAo05HEhE5IEfPORIRKSlz3VrMHduxPR4i7do7HUdEiuu66wDwvzMBIzfH4TAiIgemciQiVcqeiRgibduDr+TT+4uIQ/r3J9ryCMzcHHwTJzidRkTkgFSORKTKMHJ24Vq5HNh9SJ2IVB2GQfCKKwGIe2Vs1ZnXV0RqFJUjEaky3PPnYdg20UaHYdeufeg7iEhMCZ53IVZiEu6VK/BM/c7pOCIi+1E5EpGqIRrFvWA+oOm7RaqspCQC518IQNwrLzkcRkRkfypHIlIluJYvw8zPw0pMJNqipdNxRKSUAsMKZ6r1fvUF5u9rHE4jIrIvlSMRqRL2TN8d6dgZXC5nw4hIqUWbtyTU93gM2ybu9VecjiMisg+VIxGJeeaWzbg2bsA2zcJyJCJVWsHu0SP/hDchP9/hNCIif1I5EpGY5579KwDRI1phJyY6nEZEyip0/AlEmzTFzM7GP+ldp+OIiBRRORKRmGbk5OBeugSAcPceDqcRkXLhclHwz8LRo7iXX9K03iISM1SORCSmuefNwbAsog0bYdWr73QcESkngQsuwo6Px71kMZ4ZPzsdR0QEUDkSkVgWDOKZPxeAcPcjHQ4jIuXJTk0jcNZ5wO7RIxGRGKByJCIxyzXrF4xAACsllWhzTd8tUt3smZjB+9lkzA3rHU4jIqJyJCKxyrJwT/0egHC37mDq7Uqkuom2bUfo6D4Y0Sj+N151Oo6IiMqRiMSozz7DzMzE9vmItO/odBoRqSAFw64EIG7caxAIOJxGRGo6lSMRiU1PPAFAuGNn8HqdzSIiFSZ00iCiDRthbt+O7+MPnI4jIjWcypGIxBzXgvnw3XeFF33t0s3pOCJSkdxuCoYOAyDuFU3rLSLOUjkSkZjje+E5AKKdu2AnJzucRkQqWmDIUGyfD8+8uUUXfRYRcYLKkYjEFHPzJrwfvA9A5Ni+DqcRkcpgZ2QQPPNsQNN6i4izVI5EJKb4X/0fRjgMxxyD3bix03FEpJLsmdbbN/kjjC1bHE4jIjWVypGIxI68POLeeKXw85tucjaLiFSqSKcuhLv3wAiHC2euExFxgMqRiMQM/7tvY2ZlEW16OJx2mtNxRKSSFVxeOK23/41XIRRyOI2I1EQqRyISGyyLuLHPAxC86mpwuRwOJCKVLXjK6UTr1MW1ZTO+Tz9xOo6I1EAqRyISE7xffYF71Uqs5BSCF17sdBwRcYLXS+DSfwIQ98pYh8OISE2kciQizrNt4p9+HIDAZZdDYqLDgUTEKYFLLsN2u/H8MgP3wvlOxxGRGkblSEQc55nxM57Zv2L7fORfMcLpOCLiIKtuPYKnnQGAX9N6i0glUzkSEcfFPfMEAIHzh2DXqeNwGhFxWsGw3RMzfPAexvbtDqcRkZpE5UhEHOVatBDfN19jmyb5V1/ndBwRiQGR7j0Id+qCEQzif+tNp+OISA2iciQijop/7kkAgqefiXV4M4fTiEhMMIyii8LGvf4yRCIOBxKRmkLlSEQcY/6+Bt9HHwCQf+2NDqcRkVgSPOMsrIwMXOvX4f3yc6fjiEgNoXIkIo6JH/MMhmUR6tefaIeOTscRkVji9xMYMhSAuFc0MYOIVA6VIxFxhLlpI/63xwGQf/1NDqcRkVhUMHQYtmni/ekHXEuXOB1HRGoAlSMRcUTcmKcxQiFCvXoT7n2M03FEJAZZDRsRGngKoIvCikjlUDkSkUpnbN1K3JuvAZB/4y0OpxGRWFZw+e5pvd97G2NntrNhRKTaUzkSkUoX/8KzGIEA4a7dCB/Xz+k4IhLDwr2PIdKmLUZ+Pv63xzsdR0SqOZUjEalUxo7txL32MgD5N90KhuFwIhGJaYZRdFHYuFf/B5blcCARqc5UjkSkUsWNfR4jP49w+46EBpzkdBwRqQICZ52LlZKK6/c1eL/92uk4IlKNqRyJSKUxdmYT97/CKXnzb7xFo0YiUjwJCQQuGAJA3Mua1ltEKo7KkYhUmrgXx2Dm7CLSug2hk091Oo6IVCEF/7wC2zDwfjsF16oVTscRkWpK5UhEKoWRtYO4l54HIO+WUWDq7UdEis9qejihAScC4Ne03iJSQfTbiYhUirgXnsPMzSHStj2hk09zOo6IVEF7Jmbwv/0Wxq6dDqcRkepI5UhEKpyxfTvxY18AIO/W/2jUSERKJXxcPyKt22Dm5eIf/6bTcUSkGtJvKCJS4eLHPF04Q12HToQGnux0HBGpqgyDguFXAxD38osQiTgcSESqG5UjEalQxtatxL1aeH5A/m3/0Qx1IlImgbPOxapVC9f6dXg/m+x0HBGpZlSORKRCxT/7JEZ+PuGu3XRdIxEpu7g4Ci4dBkD8i2McDiMi1Y3KkYhUGHPDeuJefxmAvFtHa9RIRMpFwWVXYHu9eGb9gnvWL07HEZFqROVIRCpM/GMPYQSDhHofQ7jv8U7HEZFqwq5Th+DgcwCKLhEgIlIeVI5EpEK4VizH//Z4APJG/1ejRiJSrvJ3T8zg+7+PMdevcziNiFQXKkciUiESHrwXw7IInnQykSN7Oh1HRKqZaPsOhPocixGNEvfyS07HEZFqQuVIRMqde+5sfP/3MbZhkDfqDqfjiEg1VXBl4eiRf/wbkJvrcBoRqQ5UjkSk3CXcdzcAwXPOJ9qmrcNpRKS6CvU/kUjzFpi7duJ/Z7zTcUSkGlA5EpFy5fnuG7w/fo/t8ZB363+cjiMi1ZlpFl0UNn7sCxCNOhxIRKo6lSMRKT/RKIl33Q5AwWWXYzVu4nAgEanuAudegJWaiuv3NXi/+sLpOCJSxbmdDiAilc80DUyz/GeP874zHveSxVgpqYRuHYnbXbq/v7hc+ruNFI+RmoKRkFBx209LK/xYuzambVfY45RFqTMagN+LEQhhHOJudl4edvbO0ofcrUL+205JIjh0GHFPPU78S89hnXpqqTdlWTaWFZvfZxGpHCpHIjWMaRqkp8ZjlPcvKXl58OC9hY9xx+2kNm9c9m1q+m85CCM1Bf8tN2J6fRX+WL4Lz6vwxyir0maMK8Y6VihI4NEnS12QjKREsCySk4vzaKVw843w3NN4fp5G2ppl0LVrqTZjRy12ZOerIInUYCpHIjWMaRoYLpPAhInYW7eW23bdX36OZ9MmrPQMgrYBTz1b+oytWuEbeAKqRnIwRkICptfHxHfvJjPzj4p5jPQM3C1bEFm4CDs/r0Ieo6xKm9EA3G4XkUiUg1WB2rWbcN65/8VISCj96JE/DkyT0NsTiW4pv/edvXk6dsI9ZzaR4VcSHnJJie9v1KmD/8LzME1D5UikBlM5Eqmh7K1bsTZsLJdtGbm5uL+ZAkCo9zFYWzLLtr3atcsjltQQmZl/sHHj8grZthmth7t+ApEtq7BydlXIY5RVaTMagMfjJhyOHLQclSdra2a5ve/8VbhdB9xzZuOaM5tQtyOxk1NKdH8dzCsioPcCESkHnmk/YoTDROs3INqqtdNxRKQGsurWI9q4CYZt45k9y+k4IlJFqRyJSJmYmzfjXjgfgNBx/XSekIg4JnxkTwDcC+ZBIOBsGBGpklSORKT0bBvvd1MwgEibtlgNGzmdSERqsGjTw7Fq1cYIh/HMn+t0HBGpglSORKTUXEuX4NqwHtvtIfSP45yOIyI1nWH8OXo0ZxZEIg4HEpGqRuVIREonFMI79TsAwj17YSclOxxIRAQirdtgJSVh5uXh/m2x03FEpIpRORKRUvH8MgMzNwcrOYVw9x5OxxERKeRyEe52JACeWTMhRi/eKyKxSeVIRErMyM7G8+tMYPckDB6Pw4lERP4U6dgJ2+fD3LED16qVTscRkSpE1zkSkZKxbbzffo0RjRJt3IRoyyOcTiQisi+vj3CnLnh/mYHn15lEW7R0OpHEGNM0ME3Nrir7UzkSkRJxrVqBe/UqbNMkePwJmrpbRGJSpGs3PLN/xbVhPebGDVgNGjodSWKEaRqkpsXjMnUAVWWwbAvTNIhGq8YhripHIlJ8oRDeb6YAhdcTsTMyHA4kInJgdmISkTbt8CxagOfXmQRPH+x0JIkRpmngMk0mzJ3I1tytTsep1uom1eGCzudhGAagciQi1Yxnxs+YObuwkpMJ9+rtdBwRkYMKH9kDz6IFuFYsx9ixAzs93elIEkO25m5lw66NTseo1qriwSUaTxSRYjG2bcMz6xcAQscP0CQMIhLz7IxaRJo1x4Ci9y8RkYNRORKRQ7NtfFO+xLAsIs1bEG2uk5tFpGoI9+gFgHvxQsjLcziNiMQ6lSMROST3wvm41q/D9ngI9evvdBwRkWKzGjYiWr8BRjSKZ84sp+OISIxTORKRgzJyc/BO/Q6A0DH/wE5JdTaQiEhJGAbhI3sC4Jk7BwIBhwOJSCxTORKRg/J+8zVGMEi0Xn0iXbo5HUdEpMSiLY/AyqiFEQrimTfH6TgiEsNUjkTkb7lWLMO9YnnhNY1OHAi6JoSIVEWGQajnUQB4Zv8KoZDDgUQkVuk3HRE5sEAA75Svgd3XNKpdx+FAIiKlF23dBis1FaOgAPeCeU7HEZEYpXIkIgfk/e4bzLxcrLQ0wkcd7XQcEZGyMU3Ce0aPfv0FIhGHA4lILFI5EpH9uFatxLN4ITYQPOlkcOt60SJS9UXatsdKSsbMy8W9aIHTcUQkBqkcici+AgG8X30BQKR7D6yGjRwOJCJSTlyuP2eu+2UGRKMOBxKRWKNyJCL78H77deHhdOnphI7u43QcEZFyFenQESs+AXPXLtxLFjsdR0RijMqRiBRxrVyB57fF2IZReDidx+N0JBGR8uXxEOneo/DTmdPBshwOJCKxROVIRArl5eH78nMAwt17YDVo6HAgEZGKEe7cGdvvx8zKwrV8mdNxRCSGqByJCNg2vi8/wyjIx6pVm7AOpxOR6szrI9ztyMJPZ/wMtu1wIBGJFSpHIoJ7/jzcq1dhu1wETj5Vs9OJSLUX7tIN2+vF3JaJa9UKp+OISIxQORKp4Yzt2/F+/w0AoX8cp4u9ikjN4PcT7tINAM+M6Ro9EhFA5UikZotG8X02GSMSIdqkKZGu3Z1OJCJSacLdjsR2u3Ft3oS5bKnTcUQkBqgcidRg3h+n4tqyGdvvJzjwZDAMpyOJiFSe+HginToD4P7yC40eiYjKkUhNZS5ejGfWLwAETzoZOzHJ4UQiIpUvfGSvwtGj39fAV185HUdEHKZyJFITrV+P9+3xAIS7diPaoqXDgUREnGEnJhLp1KXwizvv1OiRSA2nciRS00QicNFFGHl5ROvWJfSPvk4nEhFxVKhHL2yvF375BffXXzodR0QcpHIkUsP4H3kQfvgB2+cjeMrpmrZbRCQhgcju67vFPXifRo9EajCVI5EaxDvlS+IeexiA8DnnYqelO5xIRCQ2RPr1g4QE3PPn4f3iM6fjiIhDYqIchUIhTjnlFGbOnFm0bN26dQwdOpTOnTszaNAgfvrpJwcTilR95h+/k3T1FYVfjBhBdPfV4UVEBEhMguuvByDhkQfAshwOJCJOcLwcBYNBbrrpJlas+PPq1LZtc80111CrVi0mTZrE6aefzrXXXsvGjRsdTCpShQUCJA+7BDM7u/BaRk8+6XQiEZHY8+9/Yycm4V68EN/kj5xOIyIOcLQcrVy5knPPPZe1a9fus3zGjBmsW7eOe+65h+bNm3PllVfSuXNnJk2a5FBSkaot8T+34FkwDys9ndzXx4HP53QkEZHYk5FB4NrC0aP4B++FcNjhQCJS2Rw9E/uXX36hZ8+e3HjjjXTu3Llo+fz582nbti3x8fFFy7p168a8efNK/BixcE3LPRliIYtUDeW5z/hff4W48W9gGwY5L72K3eiw3RuP4X1yr1zKWHJFeWI4I1D2fMafHyrs6RkH/DS2lEPGg93P2OuTUu9Hsb4vQlHGwIhr8b38Eu7Vq/BPfIvgxUMdjRVLqtvvM0Ys/xysZmLhtS7u4ztaji688MIDLs/MzKROnTr7LMvIyGDz5s0lfoyMjNi5sGUsZZGqocz7zA8/wKhbADDuv5+Us08vuinO74X4GB1B8nkA8Ps9ylhC8XtnidGMRcqaz+8FwO124fFU0I8zl7n7MUyoqMcoqzJmPNRr53a7gDK+Z8T6vghF+1Na43owejTceCNJjz9M0lWXQ1ycw+FiS3X5fcbv9xIfidH9sZrw7/7vKjU1weEkxReT7/QFBQV4vd59lnm9XkKhUIm3tX17juMzchpG4RtJLGSRqqE89hlz3VpSzzoLMxIheMZgci6/Brbl4HKZpKUlUBAIYecHyzd4OTGDYfxAIBDGUsZii4/3kb9XlljMuLey5jMCIeKASCRKOBwp93wARtTCDUQiFnYFPUZZlSWjx+M+5GsXiUQByvSeEev7Ivy5P2Vl5RE9+yLSHn8C1/p15D3yBAXXXO90vJhQXX6f2fNzMBAI7fOeKeUv4C783T07O49IxNlJTvbsv4cSk+XI5/ORnZ29z7JQKITf7y/xtmw7di5XEEtZpGoo9T6Tn0/SJRdibttGuEMndj31PGDA3tuK5f1xr1zKWDx7Hy5QlCfGMu6njPkM+8/NVNTTM/bOWEGPUValzbj3ESYHu5+91yel3o9ifV/kL6+jz0/eLaNIvuFq4p5+nIIhl2InpzgXLsZUl99nqsvzqAqq0mvt+Gx1B1K3bl22bdu2z7Jt27btd6idiByAZZF0/Qg8ixZg1arFrjcmwF7n74nIvqJYBIkQIEIBYfIJk0eIXELkWAXkhfII2mEiWNgxW5GkvAXPOZ9IyyMws7KIe/4Zp+OISCWJyZGjTp06MXbsWAKBQNFo0ezZs+nWrZvDyURiX8KD9+L/5ENsj4ddr47H2jMBg0g1FyTCLiNEDkEKjAj5hMk3whQQocAoLD0hooQMizBRwkQJYWEZByk8O4Hpuy8Iuvtob9M2cGPgxYUPNz7bjR/X7o9u4nCTYHtItL0k4CXR9pCIFw+uCn8NpBy53eSNupOUfw4h/oXnCFw6DKt+A6dTiUgFi8ly1KNHD+rXr8+oUaO4+uqr+e6771iwYAEPPvig09FEYprv7fHEP/04ADlPPEu4V2+HE4mUnwLCrDazWWnsYJWZxR8FYTZN+IK5WT+zw5ND0IiW6+MZNvsdjQpgGTYhbEJY5BIu9hRxPttFsu0jFT8pto9U20+q7SMFP2m2X+UpBoVOPpVwj154fplB/MP3k/vUGKcjiUgFi8ly5HK5eP755xk9ejSDBw+mSZMmjBkzhgYN9Bcbkb/j+ekHkv5deNJw3k23EDzvwLNBisS6HIL8Zm5jkbmVxWYmK8wdrDR3sN7Yhb13EQkBe64fvnu533aRZPuIx0O87SYOD/F4iLM9xOPGa7vw4sKDC69t4tn9uRsDE2P3/9j9/2DWrYfZoS3BGTMJ5+4kikUUmwgWISNKkGjhIXlGhABRgrtHrPKM3YfmGWFyCRExLIJGlEwjn0zy93/SNqTgI8OOp5YdR4YdRy07ngw7nrjY/FFdMxgGuXfdR9qg/vjfHk/BFSOItmvvdCoRqUAx8467bNmyfb5u0qQJ48ePdyiNSNXiWraU5MuGYEQiBM48i/zbbnc6kkixbMrZxE/hBczx/lZUhtaY2X+7forto6WVTgsrncMTD6PZCWcxd+p7hLdlkowPbwWMvpiGicdw4frrj0z7bz7/CxubEFFyCZFtBMk2Auw0AmQbQXZS+DFgRNhJkJ1GkNVk7XP/FNtHXTuBOlYC9ewE6tqJJOEtKnBSsSLdexA47Uz8n3xI4j13sHPih05HEpEKFDPlSERKx9y0kZTzB2PuzCbcvQc5T7/g/JXWRA4ghyBzXJuZbW5ilmsjs38fy4YndhTeuO/VG6hvJdLeqkM7qzatrAxa7C5EtYj7c1SnVgPiul7Oc9N/ZiN5lfxsis/AKDw3CTcZdvwBi1Q+YbYZ+Ww3Coo+bjfy2WWE2GkUlqbl5o6i9eNsN/XtRBpYSTQKQ+NwczyV+JxqmrzR/8X3+f/h/e4bPN99Q7jv8U5HEpEKonIkUoUZu3aScv5ZuDasJ9KiJTvHT4RSTHkvUhG2GnlMN9fzs2sd01zrWGBu3Xfyg0jhqExbox5dghm0t+oUFqJobWpRs2ZYjMdDYzuFxva+00UHiLDFyNv9L5ctRh7bjHwKjAirjWxWm9mQuw5+nka6kUADVzyN7GQaW8mk71UkpWysw5tR8M8riH/peRLvvoOsfxwHLp0jJlIdqRyJVFXBIMmXXoh7yWKideqy850PsNMznE4lNVgm+Ux1/8FU1x9Mc61jubl9v3UOs5LpHm1Ad6s+PZr1odclt+B+4XWi6zc4kDj2+XHTxE6hyV6lKYLFViOPjUYuG40cNnryybLy2GHnscOVxyIyAUiwPRxmJe8uXcnUsuNVlsog/8Zb8L8zAfdvi/BPGEfg4qFORxKRCqByJFIVRSIkX30F3mk/YiUmsfPtSViNmzidSmqYfMJMd63nO9fvfOf6nfmuLfut0zZam6Ojh9HbasTR0cNoYP95dXJXXBv83kQClRm6GnBj0sBO2v1a1sesVY9Qm2as/eVb1gW3sN7YxQYjhzwjzFLXdpZSWFLjbDeH2ck0tgpHqOraNWt0rqzs9AzybxlJ4u0jSXjgboKnnYGdkup0LBEpZypHIlWNZZF0w9X4Jn+E7fWy67XxRDt0dDqV1BBrjGw+d6/kC9dKfnKtI/SX6bPbR2tzXLQpfaKN6RVtRAZxDiWtWeI98bRw1aFZtPCw2ggWG40c1hq7WGfuZL2RQ4ERYbmxo+jcpXjbQ3PSaGqm0NRKJfGvJ37JfgouuwL/m6/hXr6M+MceJu9eXWJEpLpRORKpSmybxNv+jf+9d7BdLnaNfZ3wsX2dTiXVWASLX8wNfO5eyeeuVSx1bdvn9oZWEv2iTekbbcqx0abUtRMcSip7c2P+eQ6TdRhRLDYZuaw1drHW3Ml6Yxf5RpiFbGWheysAdax4DrfTONxK5TA7GTemw88iBnk85N7zIKnnDybulZcIXDyU6BGtnE4lIuVI5UikqrBtEu66nbg3XsE2DHLGjCU06BSnU0k1lE2AKe41fO5aydfuVeww/jzwzWUb9I4exsBoc06MtOAIO13nsVQBLkwa2ck0spPpbTUqGln6w72TlfYONpt5bDXz2Uo+M10bcNsmje1kmllp9I7Wczp+TAn360/wxIH4vvycxDtGsvOdDzRDqEg1onIkUkXEP/og8S88C0DuE88SHHyOw4mkOtlOAf/nXs6H7qV87/qDiGEV3ZZm+zkh0pyTos0ZEGlGKpoRsapzY9LETqEFGfwj0pg8wqwxs1ljZLPGzCLXCBfNhjclaw3vjmnDSQUNOMmsRy+rUY0fVcq9+wG8307B+903eL/+gtAJA52OJCLlROVIpAqIe+5pEh57CIDc+x4icNElDieS6uBghaiVlcHASAsGRlrQ02pY438Zru7i8dDOqk07amNHbTKNfNYY2awys1hn7mLptqUsZSlPxUOq7WNApDkDo83pH2lGeg08r8xq1pyCK68h/rmnSPzPbezocxzE1bzXQaQ6UjkSiXH+V/9H4j13AJD3nzspGH61w4mkKttOAZOLCtHvRPe67lD7aG3OjLTmzEhrjrA1LXxNZWBQx06gjp1AT6shqfWa0LTf6Xz88WN8EVrEDqOA9zy/8Z7nN1y2QS+rEQMjLTgp0pxWdkaNOcwy76Zb8X3wHq61vxP/9GPkj7zD6UgiUg5UjkRimG/CeJJG/huAvH/dTP6/bnY4kVRFeXaQj92Lece9mG9da/YpRB2idYoKUUs73cGUEqviTR/ntjuXU7/eQjhrPb+YG/li9wQdv7kymbb7Ir+3+77jcCu1cMQx2oJjoofhoRpfKDUxkdz7Hibln0OIf/YpgmefT7RFS6dTiUgZqRyJxKoXXyTphsJRovwrriJ/lP4qKcUXxeK7/AW8++FnfLBzInn+UNFtHfcqRC1UiKQEXJgcZTXiqFAj7uY4/jCy+cK9ii9cq5jq+oM1ZjbPe2fxPLNIsX0MiDRjULQFJ0WbU9/p8BUgdPKpBPufgG/KVyTedhM73/9EkzOIVHEqRyIxyP/iGLhjFFBYjPLue1g/cKsQIzUFI6GSp7Q2wPZ5WJC3hrdDv/JueDabN+6CjYU3NzNrcb6nO+d7utPCVadys/0NIy2t8GPt2pi2fYi1D3D/2rXLO5KUUBM7lSvD3bgy3I1cQnzn+p3P3Sv5zLWSbWY+73uW8L5nCa41n9LnjbcYGEhjoFGHZnaa09HLh2GQ+8CjeH/6Ae+PU/F98B7Bs851OpWIlIHKkUiMiX/qMRIeuAeA/OtuJO/2u1SMqhAjNQX/LTdien2V9pgbczYybv44xi0Yx+LcxUXL0+PSOb/d+QzpOIRejXphxOh+5LvwvDLd3/Do4qWxIBEvp0aP4NToEUSxmGVu4nP3Sj51rWCJaxvf//493wO3JRRO+HFypCUDIy3oYTXAVYUn/LCaHk7+jbeQ8OC9JN75H0LHD8BOrSblT6QGUjkSiRW2TfzD95PwxCOFX991F/lX3wg15OTm6sJISMD0+pj47t1kZv5RYY8TtS0WhdYxPbCc38IbsCkceXHjor33MHr4mtO2Xkf8dVsxZ8oEZuf/r8KylJaRnoG7ZQsiCxdh5+eV+P5HtOzJCSdcCR79KIs1Lkx6Wg3pGWrIXRzL2nb1+Ko1fPTN8/wUXskyczvLvNt5wjuDDDuuaGbE46OHk0jVK7v5V1+P7/2JuFcsJ+Gu28l9aozTkUSklPQTRSQW2DYJd99B/PPPAJB35z0k/PcO2JYDJT/aSGJAZuYfbNy4vNy3u5185ru2sNDMJN8IFy1vZCXTxahHy3AavpAbcgNkutfibpBMZMsqrJxd5Z6lrMxoPdz1E0qdr3atxhWQSirC4Z663NDrfK6c6WL7+lV87V7NZ7svMrzdKGC8ZyHjPQvx2i6OizZhYKQFg6ItaGgnOx29eHw+cp54jtTTTiRuwjiCZ55N+Ni+TqcSkVJQORJxmmWROPpW4l4ZC0Du/Q8TGD6CSj5jRWJYiChLzG3MN7ewwcwpWp5ge+hg1aFjtA61iMfjcRMmoj4tMS0VP+dE2nJOpC3hYJSfXev53LWSz9wrWG1m85V7NV+5V3MjX9EpWpdB0RacHGlJJ6tuTE8THunZi8A/ryDulbEk/ft6dkydAZV97qE4yrZtgtEg+ZF8CiL5FITzKYgUEIgGCFthQtEQoWiIsLXnYxjLtgr/YWHbFtHdX5uGicswMQ1X0UfTMPGYHrwuLz6XD6/Lh8/lw+fy43f7SPAkkuBJJNGTiNvUr/ilpVdOxEnhMEn/vh7/O29hGwa5jz5F4JLLYvjHv1SmTUYuc83N/GZmEt59gVbDhhZ2Op2ihSe1V+VzNUQ8uDg22oRjo014MNSPpcb2wgkd3CuYaW5gvmsL811beNA7jQZWEgOjLTg50oJ/RJvgj8FfYXJH34X3y89xrf2DhAfvKZxMR6qFqBUlN5xDTmjPv13khHd/DOWQG84hP5xfdIiz0/wuf2FR8iaS4k0h1ZdGqj+NNF8aKb5UvK6qd/hqZYm9dxaRmiIvj+TLL8H3zdfYLhc5T40heN6FTqcSh4WJ8pu5jTnmZjabuUXL020/naJ1aW/VqZLnZIgcioFBG7sWbcK1uCnci61GHl+5VvGpeyXfutaw0czhFXMur3jmEm976BNtTP/o4fSPNKOFnRYbo0qJieQ89jSp5w8m7n8vEjx9MJEjezqdSoopEAmQFdhBdjCLrGAW2YHCj7tCO8kLF/+8SK/pJc4TT5w7jnh3PD6XH6/Li9f04nF58JpevC4fHtO9e0TIwNxrdMjAwMYmakexrGjRaFLUjhKxwgSjwaJ/od0fA5EAeeFccsO5RO0ogWiAQDTA9sC2A2ZM9CSS5k+ndlwdasfXoU5cHTLiamnECZUjEUcY27aRMuQcPHNmY8fFset/rxM6YaDTscRB28hnrmszC82tBI0oAC7boLVViy5WXRrZybHxy59IJaljJzAk0pEhkY4EiPCD6w8+2z1N+EYzhy/dq/jSvQp80NRK4fhoMwZEDufYaBOSqLzZIv8q3K8/gfMuxD9xAknXXUXWNz/p8LoYEoqGWL5zFes3rOGL1Z+xdtfaoiIUiAYOel+X4SLRm0SSJ4lkb3Lh595kknYvi/ckEOeOc7Rg7Dm0LzecQ144j5xQDjuD2fsUvkA0QO7uIrUuZ23RfQ0M0v0Z1ImvS/2E+jRIbETtuNq4zGp8MecDUDkSqWTmH7+Tct6ZuFevwkpLY+db7xHp3sPpWOKAKBbLzO3MNTez1vxzQoJU20+XaF06WnWJx+NgQpHY4MfNCdHmnBBtzpOcwGIzkymuNXztWs3PrnX8bu4sGlVy24Uz5Q2INKNvtCmdrLq4K/nw09x7H8Tzw/e4V68i8Z47yH34iUp9fIFwNMzqnatYtmMJS3csYVnWUpbtWMLqnauIWJG/vV+iJ5FUXxpp/rTCQ9F2H4aW5E0i3h0fs5dE2MMwDPxuP363n1pxB74WXEGkgOxAFtsD28jM38rWgq1k5m8tGmnaHtjGkh2Fl4Vwmx7qxdejYWIjGiQ2pGFiI/xuf2U+pUqnciRSidwL55N8wdm4tm4helhjdr7zAdGWRzgdSypZNgHmubYw39xSNOOcYUNLO50u0XocbqdqlEjkbxgYtLfq0N6qw7/CPcklxI+utXztWs037jWsMrOY5lrHNNc67mIqybaP3tFG9Ik25h/RJnS06lT4uXp2aho5Tz9P6rlnEPfaywRPHES4X/8KfcyaKhwNs2bnapZl7S5BOwpL0KqdK/+2BCV6kmhftx12FHymv8adixPnjiMuMY76iQ2Kltm2TW44h635W9mav4WNuRvYmLeBYDTI+tx1rM9dBxT+91c3oR5NkprSJLkpDRIbVrtD8arXsxGJYZ4fvid56EWYuTlE2rZn5zuTsOrVdzqWVBIbm1VGFnNcm1llZBVdvirR9tDJqkfnaF2SHTwUSKSqSsTLwGgLBkZbQAhWG1lMca3hG/dqfnKtY6cR5Av3Kr5wrwIg2fZxdLQR/4g2oU+0MR0qqCyFj+tH/uVXEv/ySyTdcDVZP8zATks/5P1M08A0Y/uPIy5X5U8EE7EirNm5mqXbC0vQno8rs1YQtsIHvE+iJ5FW6W1ondGG1ultaJXemtYZbWicchgpKfE89eOzbNi1sZKfSWwyDGP3IYLJNE9tARQWpu2B7UVFaWPuenYEdrA5bxOb8zYxc/N03KabRomH0SS5KS1SW5LmP/Q+HutUjkQqgW/SuyRdPwIjHCZ0dB92vTEBOznF6VhSCYJEWGhuZZZrE1nGn8ezN7VS6BqtTwvNOCdSrprZaQyPpDE80pUoFgvNrfzg+oMfXWuZ5lrPLiPI5+5VfL67LKXYPo6MNqBnoDXHrmrNEQntiXcllkuWvNvvxvv9t7hXriDx1pvIGfsaHOSwLNM0SE2Lx2XG9ntCWlrFnUMVsSKszlrNb5m/sXjrYhZnFv5bum0poWjogPdJ8CTQtnZb2tVpR7vau//VacdhyYcd9DC4WD9EzmmGYVArrha14mrRsXYnAHJCOazd9Tt/7PqdP3J+Jy+cx++71vD7rjVMXf8d6f4Mmqe2oEVqS+onNDjEI8QmlSORimTbxD/yAAmPF07nGjjtTHLGjAWfRgiqu23RHKa4VrNgrwkWfLaLjlZdukbrkU6cwwlFqj8XJp2tenS26nF9uCdRLOabW/jRtXZ3WSocWZriXsOU4BruH/85Bgat09vQvV5PutTpSsfanWiV3gafqxTv2/Hx5IwZS+qg/vg//oBQv/4ELxjyt6ubpoHLNJkwdyJbc7eW4ZlXDMMAv99LIBDCLuOM1aFoiMz8TLbkb2ZL3ha25G1mS/4WMvO3ErWjB7yP1/RSN6EedRPqUS+hHvUS6lM/oR6p/jRMY3ehjMKyzStZtnnl3z52q9qtGNj6BHT0cskleZNoV6sD7Wp12D2ytI0/dv3O6p2rWJezlh2B7ezYvJ1fN88kzh1PpzodOb/zuXgpnz84VAaVI5GKEgiQdMMI/B9OAiD/2n+Rd/tdEON/EZTSs7H5PrKcF985nclZn2DvnuAn3fbTPdqADlYdvNSsWX9EYokLk65Wfbpa9bkh3JMIFovMrcx0beCXxB38krCDNdlrWLLjN5bs+I1xv70GgMf00CajHR1rdaJD7U60r9WB1ultSPImH/IxI126kTfydhLvv5ukkf8m0rU70VatD3qfrblbY/JwL8OA+IiP/PxgscqRbdsEooXTY28PbGdHwfbdJ/xvZ2cw+2/v5zbdZPgzyIirRYa/NrXiapERV4sUb8p+oz0F4SAF4c0leh61Ew48UYGUTOHIUm1qxdWmW90jCUYCrNm1hlXZK1i9cxUFkXxmbJzBC7Ne4IZOtzgdt9hUjkQqgLF1KymXXoBn9q/YbnfhxV0vusTpWFJBCgjzjnsxL3hm81teJiwrXN7MSqV7tAHNNMGCSExy7zWyNCK+AXE3XMfS9auZsWEGs7f8yvzMeSzMnEd2MJsFmfNYkDkPlvx5/4aJjWiV3ppWaYXntByR3opmKc33O++i4Lob8f70A96p35F8xaVkffEdxMdX7pOtIHtO5M/ePV10dmD3x91fB6PBv72v3+Un3Z9BRlwGGf5apMdlkOHPIPkAJUhin8/tp3V64X8LUSvKhtx1BK0AVx95NRz4iMiYpHIkUs5cS34jZci5uNatxUpNZder4wkf8w+nY0kFWLdzHc8UfMKrCT+yY/f5RAl4ubT75aSt3IS1NfYOixGRg6ubUJeTm53Kyc1OBQp/+V+b8wcLMuezMHM+8zPnsmTHb2zO28SG3PVsyF3Pt2un7LONFF8qhycfTtOUw2ma3IymKYdz2J2X0WrEAhqvXkLiHSPJffwZJ55eidi2TUGkgNxwDjmhHHLDOQTsfLLys8kJFS7bFdxJxP77qbGhcGKEPSUo3V+raFSoKkyNLaXjMl00Tm5Ko5QG1EmoQ1ao+BfRdZrKkUg58n7zFUlXXFY4I93hzdg14T2izVs6HUvKkY3NDHMDL27+no+eHlJ4bLwBTawUrgp349LaJ1D/5Nt4bsw/2YjKkUhVZxgGTZILpy0+tfnpRcuzA1ks3X3tnGW7p5BenrWMLfmb2RnMZl7mXOZlzt13YxcWfkgOvE79l76kbv3W1I2vS7o/nTR/Ohnx6RyW0YBlO5aRF8zD4/LiMd14TC9u043LcJWqTFi2RTgaImSFCEV3/7NChKJBAtEABeECCiIFFETyd38sIC+SR24o52/P/9nnNcIgxZdSdE2gNF8aqb5UUvxppHpT8bh0vTapOlSORMqDbRP3vxdIuPM/GJZFqPcx7Hp1HHZ6htPJpJwEifCBeynPe2Yx17UZcguX/8PdkhE5HRgUbYELE9OoHofKiMjBpfrT6FX/KHrVP2qf5XnhPP7Y9Tu/71zDmp2rC2fy2rmaTXkb2Zi7kdxwDrv8sCu6iWXrN5XoMQ0MPC4vLsMEDEzDoOh/hoFNYRGy7ChR28KyokTtKDZlmz0h3h1PkjeJJG8SqfGpxBnxJHqSSPQmkeJLJdmb/OeECCJVnMqRSFkFAiTd8i/8EycAUHDBEHIffQq81f9CcjXBFiOPV9xzedkzl61m4WEBPtvF+cl9uPHCp2g18Uei2RscTikisSLBk0DbjHa0zWh3wNtzC7LJGXEOmUtmsvbwWvxx/RXsMAonLcgO7WBXZCfLt60gJ5RD2AoTjoaKyo2NTegg5/AcimmYeE0vHpcXr+nF6/Lid/mJ88QR544vvDjo7o/x7ngSvUkkehKLLvJpGBAfX/wJGUSqIpUjkTIwN24g+bKL8Mydg+1ykXfXfRQMv/qg17GQqmGeuZnnPbN4372E0O6puOtbiQwPd2VouDP1Wh6Fv14nAvzocFIRqUoS41JJenIi7QYch+vb3wlZM9n59iRwuXC7TdLSEva7OGnUihYWpd3/LNvCti3sPf+zC/9B4bkepmHiMlyYhguXYWIaLrwub6kPyxOpSVSOpEqJpSuHu2ZMJ3HoRZhbt2KlpZP36htEju2LadlYlv6kVhVFsPjEtZwXvLOY7lpftLxHtAEjwt05I9IKj6biFpEystPS2fnGBNJO7o/3+29JeOAe8u64+2/Xd5kuXKYLP/5KTClSM6kcSZVhmgbpqfEYrhg4rnnsWLj2WgiHoUMHzI8+IqlZMwDsqMWO/2/vvuOjqvP9j7/OmZ6ekNB7R6SjWLCBIFIEdK8LlmVXvHL17upd15Wrey2rq2v77eqqa9trXQuyKqirFAvsVQQUpQuGXoIkAdKTaef8/ggMRIoJhDmT5P18POYxyZkzJ+85fJk5nznf8/0WVahAakD2UsmLnhU86/maHWYJAG7b5NJIT24ID2aw1TBn+RaRxBXtfSqlf36CtGnXkPT4n4l070H0yqNPECsi8aHiSBoM0zQwXCZVr83AdmqI5EgEzztv4V70efWv/foTnnwlvPtPAIzmzfFf8VNM01Bx1ACsNQt4yrOMN9yrqTSqh6LNtpKYGunPteEBtLJTHU4oIo1ZcOJPqFizmqS//InUm39FWYf2MO5ip2OJNGkqjqTBsfPzsXbGf+Zwo7QU33uzcOXtxAbC55xH+PQzoHBPbJ0EOKclP8LCZo5rA3/1fMUC99bY8r7R5twQHsxPIqfg11ujiMRJ+e13Ym7ZjP/dd0i++gpY/IXTkUSaNB0BiNSCuW0r/vdnY1RUYPt8BEePI9qlq9OxpA5KCPKKZyXPeJaxySwCwLQNxkW7c0NoMGdZbTFIjOvZRKQJMU1KH38aV95OPF8thTFjCDw6Vd+2iThExZHIsdg2nqVL8Hy2EMO2ieY0Jzh+InZGptPJpJY2Gvt42vMVf/esotQIAZBh+/h5uD/XhQfS3k53OKGINHmBAMUvv0Hm6OG4Nm1i/K3P8MQ9lxFM8jmdTKTJUXEkcjTBKnwf/hP3hlwAwr1PJXThReDRTN+JzsLmE9dmnvYsY65rI/b+E0I9rGbcEBrMpEhvktE8VCKSOOzsbMpmvEX6mJG0XLeNn//hLf737suJeHWoJhJP+h8ncgRGQT7+d9/B3LcP2+UiNGwEkb79NH9RgislyGue1TztWUauuRcAw4ZRkS7cEB7MBdGO6jonIgnL6tYd5swheO7ZdF25jasenMXLt03EcmsKAZF4UXEk8gPuNavxzp+DEYlgpaYRvGQiVqtWTseSY9hk7OMZzzJe8ayixKiePT7N9nF1uA/XhQfRxVY3SBFpIAYN4t0HrmPCb/5K7yUbuPyxD5jxX2OwE2EaC5EmQMWRyAGhEN6P5+NZswqASMdOBEePg6Qkh4PJkdjYfOrawlOer5hzSNe5blYW08KDuDJ8Kqmov76INDw7+3fjlf+ewJT73mbQp2sAVCCJxImKIxHAKCjA/94szL17sA2D8FlDCQ85E0x9ECWaMkK87l7N095lrDcPDqM+MtKZ68ODGR7thKmucyLSwH07pBuv3TqeKx6azaBP12BYNjNuHoulAknkpFJxJE2bbeNetRLvJ/Oru9GlpBAccwlWu/ZOJ5Mf2GwU8axnGS97VlK8v+tciu3lqnAfpoUH0c3OcjihSONm5OQc9+jSRmbmwW3YiTlBtpGTA4DH48LlUAFimtVf7BimgWkarD63F6+ZBlc8OJuBC9fism1ev+USoqa+ABI5WVQcSdMVCuKbPxf3t2sBiHTsTHD0WHWjSyCWbfGxazPPepbxgWtDrOtcFyuTaeFBXBXuQ5q6zomcVCkpWVi2hf/KSSe8Ld8VP62HRCdXSorf6Qj4fR4CgeoRNTeO6MubAS+X/34m/f71Lb5IlFd+cwkhnw7hRE4G/c+SJsnM343vvVnVo9EZBuGh5xE+fYhGo0sQe6NlvL7o//FU6YNsDBTEll8Y6cT14cGMiHZW1zmROAn4UzANkzfffYj87euOaxtGVjPc3boSWbUau6K8nhPWjwMZoxs2YFdWOpKhW+fTGHneFCK5GwhvXR1bvtoN4Sv6M/n15fRc9B3XFr3BC3dcRmVqwJGcIo2ZiiNpWmwb9/Jv8C74GCMarR6NbuwlWG3aOp2sybOxWWbu4jnP17y1ZT1Vm8MApNpeJodPZVp4ED3sZg6nFGm6CvZsIy/vu+N6rhltibtVMpHdG7FKS+o5Wf2IZdyyxrGM2UnZ1T9UVWKXltZ4bF3bAC/8YjBXv7aSTmt3cMP0V/nb7y+nOCfNgaQijZeKI2k6glX45n6I+7v1AES6dCU4agwE9M2bkyoIM9O9luc8X7Pctbt6oQ39W/bn2uKe/KSgNSmasFVEhK2dsnjhL7/gyt/+nZbbCvnlLa/w4h2XsbNrS6ejiTQaGvJEmgQzbyeBl1/A/d16bNMkeP4wghMuU2HkoPXGHm71fkS35Cf4T/+HLHftxme7mBzuzadtfs/X133NVN/ZKoxERA6R36k5f/1/V7O7XTMy9pRyw/S/0/f/vnU6lkijoTNH0rhZFp4vPsezeBGGbWOlpRMcNx6rVWunkzVJIaL805XL3zzfsNC9Nba8k5XB1PAArgr3IZskXIHuGLr+S0TkiIqap/PEI1dz5UPv0nPZJq5+cDbztxYw/4pzsDWSncgJUXEkjZZRVITvg/dw5e0EINKrN8ELR4DP+ZGImprvjD287FnJq+5VFJgVAJi2wcXRLlwbHqi5iURE6qgq2c/zd/6EMS8u4Lx3ljLijUW02bibGb8eS0WaekWIHC8VR9L42DbuNavxfjwfIxzC9voIjhhJtFdvp5M1KRWEme1ezwue5Sxy7Ygtb2Elc3WkL1PD/WlnpzuYUESkYbNdJu9PHcaujjlc9sQcTvlyI/910wu8euslbO2lgYZEjoeKI2lcqqrwzZ+De331cLPRNm0Jjh6LnZ7hbK4mZIX5PS+6V/KmZ01sslbTNrgo2pkp4X5cFO2CB5fDKUVEGo9lw/uQ16k5Vz8wi5y8fVw//VXm/Ow8Fk48HduhCW1FGioVR9JouLZsxjv3A8zSUmzTJHzWUMKnnwFm/D8Y6mt29fqepd0wDNzu6m2eyGz3P7TXKmdm+GteDi/mm+j22PKORjOmeM/gKu8Q2pgZtc+ZmXkwo23XU8r6dbSMRk6OU5FEpAnb1bkFjz36cy57ci4DFq5lzIsL6PXlBt78rzHsaZXpdDw5DoZhJPz0i7YNdoJ+Th8vFUfS8IWCeBd8imflcgCsjEyCY8Y5MuiCkZoClkVaPfX3zsxMrpftHGDZFqZRXRKd6Gz34WiYDzd8yEsrXuL9794nFA0B4HV5mdhzItcOvJZhnYbF/t7x8F3x0xPKGA9Hy2h4NMqeiMRXMMnHa7eMI7dfBy557mM6r9nBr3/1PO9fM4zFF/fXROcNiGEYBPwejAQfYMO2bCqrwo2qQFJxJA2auW0rvjn/xCypnrAvPGAgoXPOB69DB6b+AJgmoddnEN2df/zbMSDg91JZFYJ6er8xcnLwXzmJmZ88xd4087hmqrdtmx3RPSyt2shXwU2U2VWxx9q6sjjd35XTfF1I2e1n3YevsY7Xji/r/pnqjydjvBwtY/duQxg5chp49PYqIg4wDL4c2Y8N/Tpw+aMf0HXVNi7761z6/2stb99wEfnts51OKLVgGGCYRvVnTHmCfg4mJ+PucyqGUX0GqbHQp7c0TKEQ3v9bgOebrwGqh+geNRqrfQdnc+1n5Rdg7cw77ucbBpDkw64I1tsbzoHzN/m7N1Fg1m2m+jJCrDbzWW0WxEabA0i2PfS2cuhjNad5KBkqoYRtnOjc8rGZ6uuQMd6OljEnu72DqUREqu1rkcGz903m7Pe+4uKX/0WX1du5+VfPs+DSIXz807MI+z1OR5RasMvLsUtLnY7RpKg4kgbH3LQR3ysvYRYVARDu15/QeReA1+dssEamkgjfmXtYaxaw1SjG3n9m32UbdLeyONVqTmc7U0Nwi4gkKNs0+Gz8aaw+szsTnplP7yUbGD7zCwYuWMMHU85jxbmnaF4kkR9QcSQNhlFcBLffgu/ppwGwUlMJXTSaaMdOzgZrREJEyTX3stYsYJNRhGUcPG3Vxkqlj9WcnlY2Ab11iIg0GEXN03nxjp9wypJcJjw9n8yCEq585D3Onf0l718zjE19dMZb5AAd4Ujis218775Dyv9Mh927AQj36Uvo/GGa0LUeRLDYaOzjW7OAXHMfEcOKPZZjJXGKlUMvK5tMtK9FRBqytUO68V3/jpwz+yuGzfyCdrnfc/1tr7FuUGfmTz6bbT3bOB1RxHEqjiShmdu2kvLfv8H30bzqBT16ELxwJJFA/Y7i1tSEo2G+i+5mnWs7ueZegkY09lim7ecUK5teVg45dpKDKUVEpL5FfB4+vfxMlo7sy8jXP2fIh9/Qc9kmei7bxPqBnZg/+WxNICtNmoojSUyRCIFn/kryw/djVFRge71U/ddvCNxzF9ZTz8IJDHbQVBXZFby98lWey5vN2g1bCFthDszFmmp7YwVRSzsZQ9cRiYg0auUZybxz/UgWTjyd4TMWMejjVfT4ejM9vt7Mlp5tWHjp6awZ0k2TyEqTo+JIEo5n0Wek/G467jWrAAideTZljzyG0asnAZ8GXaiL3UYZ77tyec/9HQtLthF+5+AZonQCdI9m0MPKpq2dqoJIRKQJ2tsyg5k3jebjn57FsDcXMeiTNXRct5OO979DYasMvhg9kK+G96GinubvE0l0Ko4kYZhbNpPy+zvw/fNdAKyMDMrvvo+qSVeCaaqx1oKNzVqzkLmujXzo3sBic0dslDmAU3JOoX00jfYd+5K9eht2mYYHFRGR6iLpHzeOZu5V53LWP7/mzA++JntXEeP+9xNGvbyQlUN7smRUf/JP6/Kj2zIMI6Hnm030iVXFWTreFMcZpSUkPfYnAk8/gREKYZsmVT/7BeW3/g47W5PV/ZhyQix0bWWuexPzXBvZbtacF2hQtBXjIt0Z32wo/W/4A0++/lsKUpOJGNvra35ZERFpJEqzUph79bl88m9nMHDBWs748BvabtzNoE/XMOjTNexrlcGy805h2QWnUtgm67DnG4ZBwO9J6ALE560+/DUSuYITx6g4EudEo/jfeJXk++/BLMgHIHTuBZTd+0eivU5xOFxi22TsY457I/NcG/k/17YaAyr4bTfnRTswMtqZMZFutLXTADBdLZyKKyIiDUzY72XJqP4suagfbXO/58wPv6HfZ+vI3FXEhW8s4sI3FpHXqTlrzujG6jO6kde5Bew/Y2SYBpFVq7HLy51+GUcUGZABp6DO5HJEKo4k/mwb79wPSX7wvth1RZHOXSj//f2ERo4ioc/FO2QPlfzLtZUF7q0sdG1hg7mvxuPtrTQuinRlVLQL50Tbk4RmPhcRkXpgGOzo3oqZ3Vsx+z9GMOCbzfSes4LuX2+i9eZ8Wm/OZ8Trn7MvJ401Z3Rj7Znd+f60Ltjl5dilCdp1O1jldAJJYCqOJH5sG+9Hc0l6+I94ln8DgJWWTsVvplM59Trweh0OmDgqCPN/bGeOZwMLXFtYYe6uce2Q2zY5K9qWi6JdGBnpQk+7mQZUEBGRkyrs97B6eB+WntmdQHElvb7cQO8lufRYtpnMghKGvreMoe8tozLFz6YO6Wxsk8ymDunkZwf0xac0GCqO5OSzbTyffkzyQ/fh+XpZ9aKkZCqvnUbF9b/CbtbM4YDOqyTMMnMXn7m2s8C9haVmHiEjCofUiz2j2VwQ7cD50Y4MjbYjXZOyioiIQyrSAiwb3odlw/vgDobptnwLvZfk0nvJBlKKK+i9porea6rXLUvysKlDenXB1CGdPVl+FUuSsFQcyclj23gWfkryw3/E8+WS6kWBAJXXXEfFf97UpAdb2Esli107+MK1g0WuHXxt7iJsWDXWaWencV6kAxdEO3JetAMt7RSH0oqIiBxdxOfh2yHd+HZIN96xbbpuK6Tdu/+i8/p8OuwoJaUiTN9vC+n7bSEA5QE3O1qnsr11Cttbp7KjdQqVAXUHl8Sg4kjqXyiEb9ZbJD31ROyaItvvp3LKVCp+9Wvs5s0dDhhfNjZbjWKWuHbyuWs7X5g7+NZVeNh6LaxkzrTacl6kA8OsjvQJtKQyFMLWkHIiItJA2C6THae0ZXNJFxYOao4rYtEur5TOW4vpvLWY9jtLSa6M0GPjPnpsPHj9bGGWn+2tUtnVMpldzZP5vnky5ckqmCT+VBxJvTHy8wm89jL+F/6Ga1ceAHZSEpVXTaHyV7/GatHS4YTxUVhRyOfhNSz1rGaZaxdfufLYY1Qetl43K4uzom05K9qOM6Nt6WRnxK4bMgx0DZGIiDR4UbfJlvbpbGmfzifngCti0Sq/nLZ5ZbTLK6VdXinZe6titwFrCmLPLUnxVBdKLaoLpvzsJAqz/EQ8LgdfkTR2Ko7kxNg2nsWL8L/0v/jem40RDgMQbd6CqmunUTnlGuzMw+dBaCzKCLHS3F1dBJm7+HrL82x+uHpYcnwH1/PYJn2tFpwVbcuZ0bacGW1HDknOhBYREXFI1G2yo3UqO1qnsphWAAQqw7TdVUbbvDJa7S6nZX452fuqSCsLk1ZWRI9NRbHnW0BRuo+C7CQKmgUOuSVRnuTWtUxywlQcxZHLZTod4Zgsy8ayateHy9y5A/+M1/C/8SquLZtjy8ODTqPy51MJTrgMfL5jbOEH2zMNzB+ZMO7A/jNycoj3nrRtm512ESujO1kV3cnK6E5WWjvZZBViHzqVaqT6rrunNYNpwyBXewa5OtDX1Qa/UYfuAQbg92JUhTDqqVudkZNTPxsSERFH/djn5clm7J/P6EiOJ1tlwENu50xyO2fGlnlDUVrkl9Mqv7y6YCqoIKewkqSqCFnFQbKKgzW65QFUBNwUNAtQmBlgX4aPvRl+9mX42ZfhozTFi63CSWpBxVEcmKYBlkVmZrLTUY7JjlrsLao4aoFkFBbie382vllv4fnic4z9F8NYySkEJ1xK1S+uJdK3f53/rmkaZGQm4TJrV/L4r5xU579RF+WhctYVrmNNwRpWfL+CFbtXsPz75eyp3HPE9Vuntub0NqdzeuvTOa3NaQxuPZgMf0a9ZAnUy1Z+wK3/9iIiDZHh9YJt4/M5ey1OIFCbqTdOrBAJeV1sb5vG9rZpBxfaNskVEXL2VJCzp/LgrbCCjOIgSZUROuwopcOOw+dXCrsMitKrCybXNxFY76JLaD2GL0RxsxRKM1OINpbueraNN2zhC0Vxhy08EQt35PB72zCwDLANsA0jdm+ZBlU+F1V+d+w+muBf8NcnHSXFgWEYYJoEX5+BtTvf6ThHZDRvjv+Kn2KaRo3iyNy+De+8D/F9+AGez/+FEY3GHgudfQ5VP72C4NjxkHL8I6mZpoHLNHntmxnklx19/ximgd/nIbJqFXZFxXH/vQOCdpjvI0V8Hy1iV7SI7yPV93utsiPnxKCFK5227ma0cWXSxt2MNu4sUk0/FAKFuazfsZeN3TZWzwxecfwzgxuA2+0iEolSX+MxdO82hJEjp4Grkbz5i4g0NW4PGAbRVauxyo//M+ZEeNwuwpHoUR83mmXj7tYFA+rt8+vgxg3Kkz2UJ1dfw1QjVzhKs71VNC+sIKuoisyiKrKKgmQWVZFeEsQTtcnZW0XO3irYtAQ+XMLoH2y+LD2JkqwUipulUpKVTHlaEhWpASpS/VSkBShPDexf5ifk9xL21X83Plc4iq8iSCAYJj1q4dq8F19RKb5gFH8wijcUxR+MEKiK4gtG8Aerfz/03lcVwVXPOz/kNqnyu6j0uylJ8VKS6qO0WQrrM9uxqX3jGn1YxVEcWfn5WDvznI5xRLHvA4JBPIsW4V3wCd5PPoqNNndAuN8AguMvJTh+Ila79vWaIb8sn50lR98/pmkQCHgJ795Y61m3LWyKqGKfUcVeozJ2v9eopNgIHvV5SbaHbDtAjp1Mi/23bDsJd40OfWWUUsahScxoS9ytkons3ohVWlKrjEdiAB6Pm3A4Um8fLjnZ9fvvJSIizrDLK2r9OVifDACPG47x2WQkOdNLJuxx8X2L6sEbfsi0bNJLqgulzKIgPZPb0dvdkl2rFuPL30Pa3jLcEYuU4gpSiitovbl2X2RbBoR9HsI+DyG/h5Cv+mabBrZpYBnV99VnZaqLKFckijsSxR2O4opU39zhKO5QBF9lCE/46IVnXVkGhN0mEbd52H31mSAbwwbDrnnvsmx8wSiB/cUWgDdi4S2zSCsL06LwwCBT+Zy6sZiHn7mu3jInAhVHTV0wiLkrD/eqFfDBe2R89hlG5cGR1WzTJHz6GYQuGk3w4jFYnbs4GPbIbGxKCO0vfirZGyuEKikiiHWMi3aSbQ/ZdtL+WyD2cxIaPlRERKQxsExj/7VH1ZOnR4aMpPcl/80/Fj3Jtn07wLZJKqkkfW8ZaXtKSdtbRtreMpJLKkkqrSSppJLk0uqfk0sqCZRXf7lq2uCrCuOrCkNx/WYO+TwEk30EDYsqj0HQ5ybocxH0uqq7uvmqu7wF999X+g/53V/9eMhjnvCZLePQQqkqQlJlhLSyEGmlIdKCNlvGnFVPrzhxqDhqSiwLo6gI8/s8XDt3YubtxCwsiF07BNXfCkWbtyB83gWEzruA0PCR2M2aOZeZ6sEQKiIVlIVLqCgtY29kE0WuEoqMKoqNIMVUET1GAeS2TTJtP5n4ybID+29+mqkIEhEREcOgIj2JivQkdnX68bkYjaiFNxjGGwzjCUbwVoXwVoXxVoXxBMOYto1h2QfPyOz/Gdsm6nYR8bgO3ntcRDxuIm4XwSQvwSQfwYAXPK7q3jKLlzhylvAA2zSoCripChxeMhipqXjO6AGVIQeSnTwqjhqrigrMwnzMgoL9t3zMPYUYkchhq1pp6Vhdu+K+5hcU9x9MqFuvuA6Fads2eyr2sK1kGxv2bqAkVExxsJjiUDEl++8jVrjmk35w2YxpG2TgJ8v2k7m/+MmyA2TaAdLwas4gERERqRe2y6wuYpJqPypvXTWd4Q8Sj4qjhioSwSgpwSwuwiguxigpjv1sFhfV6Bp3KNvtxsppjtW6DdE2bbBat8FOScVs0xr3f/4n1r5yiFhxexmbijZw6bvjyCvb+aPrpnhSyQpkklYWIS1skmH7Sbd9pNt+0vFhqgASERERkROg4ihRWRZGWenBYqe4GKO4CLO4uPrnstJjlgI2YGdkVBdC2TnVt5zm2BkZUMshs+OhJFTC7vLvAUjzppHiSSXNl066N510Xzpp3nTSfRmkelPxuj0HTzFXOXeKWUREREQaJxVHTrHt6q5vJYcWPQfvjZISDOvYZ3Bstwc7PR0rPR07PSN2b6enY2Vkgrc28xA4q3/zgWyatoOcrHSeWfy3Y45WJyIiIiJyMqk4igNz3bfwxb/wzJoNO3cePPsTCR/zebZpYqel1Sh8Yvdp6ZCUVP/j6zswyVeGK52AJ4BhGsecWdvpGcFFRESk8aiehjIxjy0SNVdToOLoZLNtUkePhKJ9h+1sG7BTU6vP9KRVn/GpcQYoJSVuXeCM1BSwLNLSAnH5e0fi93niMuu2iIiINGHu6pFqPR53LY87nKRjnnhTcXSyGQZVt0wnadU3hAv3YJmugwVQahq4E+SfwB8A0yT0+gyiu2s3+Vl9MXJy8F85iciqVYR3bzz6eidz1m0RERFpGszqIW+j27cTXr/M4TBHpmMe5yTIkXnjFrzhlyRlJhN57HGsHYl9TY2VX4C1M74ZD5wbsyuOPeO3U7Nui4iISONjB4OOziF0LDrmcU7iDFsmIiIiIiLiIBVHIiIiIiIiqDgSEREREREBErw4CgaD3H777QwePJihQ4fy/PPPOx1JREREREQaqYQekOGhhx5i9erVvPTSS+Tl5TF9+nRat27NqFGjnI4mIiIiIiKNTMIWRxUVFcycOZPnnnuO3r1707t3b3Jzc3n11VdVHImIiIiISL1L2G5169atIxKJMGDAgNiyQYMGsWLFCizLcjCZiIiIiIg0Rgl75qigoIDMzEy83oMzF2dnZxMMBikqKiIrK6tW2zFNsB2ePcvYP7mx2bo1eD3OhjkKs3lO9X3rVtje+DYLo1k2AK079cWbfvR/VyM9HVdaG6JdI9hVVfGKVyf1mdHtMolE6++LgOw23QFo3aoLAZ/dZPbjyXK0jLH93K43Ho8/rpl+2GYSfT+eaL547OtE34dwYhlr8z5TH/u5se/H+vJj+zoRMv5Ym0mEjD8mJ6cDAK1b98ATScwpVhvCfsTvx53WnqA/fNSD7ezk6uNLw6g+JnfSgePxH13Ptp0uHY5s1qxZPPbYY3z66aexZdu3b+fCCy9k4cKFtGzZ0sF0IiIiIiLS2CRstzqfz0coFKqx7MDvfn98v40VEREREZHGL2GLoxYtWrBv3z4ikUhsWUFBAX6/n7S0NAeTiYiIiIhIY5SwxVGvXr1wu90sX748tmzZsmX06dMH0+lOiyIiIiIi0ugkbJURCASYMGECd999NytXruSjjz7i+eef52c/+5nT0UREREREpBFK2AEZACorK7n77ruZN28eKSkpTJ06lZ///OdOxxIRERERkUYooYsjERERERGReEnYbnUiIiIiIiLxpOJIREREREQEFUciIiIiIiKAiqN6EwwGuf322xk8eDBDhw7l+eefP+q6a9eu5d/+7d/o168fl112GatXr45jUkkUdWkzCxYsYPz48QwYMIBx48bx8ccfxzGpJIq6tJkDduzYwYABA1iyZEkcEkqiqUubWb9+PZMnT6Zv376MGzeOxYsXxzGpJIq6tJn58+dz8cUXM2DAACZPnsyaNWvimFQSSSgUYuzYscf8rGkox78qjurJQw89xOrVq3nppZe46667eOKJJ5gzZ85h61VUVHDdddcxePBg3n77bQYMGMC0adOoqKhwILU4qbZtZt26dfzyl7/ksssuY9asWUyaNImbbrqJdevWOZBanFTbNnOou+++W+8vTVht20xpaSnXXHMNXbt25b333mPEiBH88pe/ZM+ePQ6kFifVts3k5ubym9/8hmnTpjF79mx69erFtGnTqKysdCC1OCkYDHLzzTeTm5t71HUa1PGvLSesvLzc7tOnj7148eLYsieffNK+6qqrDlt35syZ9rBhw2zLsmzbtm3LsuwRI0bYb731VtzyivPq0mYefvhhe+rUqTWWXXPNNfaf/vSnk55TEkdd2swBs2fPtidNmmR37969xvOkaahLm3nppZfsCy+80I5EIrFll156qb1gwYK4ZJXEUJc288ILL9gTJ06M/V5aWmp3797dXrlyZVyySmLIzc21L7nkEnvcuHHH/KxpSMe/OnNUD9atW0ckEmHAgAGxZYMGDWLFihVYllVj3RUrVjBo0CAMwwDAMAwGDhzI8uXL4xlZHFaXNjNx4kRuueWWw7ZRWlp60nNK4qhLmwHYt28fDz/8MPfcc088Y0oCqUubWbp0KcOHD8flcsWWvfXWW5x33nlxyyvOq0ubycjIYMOGDSxbtgzLsnj77bdJSUmhffv28Y4tDlq6dClDhgxhxowZx1yvIR3/up0O0BgUFBSQmZmJ1+uNLcvOziYYDFJUVERWVlaNdbt27Vrj+c2aNTvmqUhpfOrSZrp06VLjubm5uXzxxRdMmjQpbnnFeXVpMwAPPPAAEydOpFu3bvGOKgmiLm1m+/bt9O3blzvuuINPPvmENm3aMH36dAYNGuREdHFIXdrM6NGj+eSTT7jiiitwuVyYpskzzzxDenq6E9HFIVdccUWt1mtIx786c1QPKisra7yRALHfQ6FQrdb94XrSuNWlzRxq7969/OpXv2LgwIEMHz78pGaUxFKXNrNo0SKWLVvGDTfcELd8knjq0mYqKip49tlnycnJ4bnnnuO0005j6tSp7Nq1K255xXl1aTP79u2joKCAO++8kzfffJPx48dz22236To1OaKGdPyr4qge+Hy+w/5xD/zu9/trte4P15PGrS5t5oDCwkKmTJmCbdv85S9/wTT137cpqW2bqaqq4s477+Suu+7S+0oTV5f3GZfLRa9evbjxxhs55ZRT+O1vf0vHjh2ZPXt23PKK8+rSZh555BG6d+/OlVdeyamnnsq9995LIBDgrbfeilteaTga0vGvjq7qQYsWLdi3bx+RSCS2rKCgAL/fT1pa2mHrFhYW1lhWWFhI8+bN45JVEkNd2gzA7t27ufLKKwmFQrz88suHdaGSxq+2bWblypVs376dG2+8kQEDBsSuHfj3f/937rzzzrjnFufU5X0mJyeHzp0711jWsWNHnTlqYurSZtasWUPPnj1jv5umSc+ePcnLy4tbXmk4GtLxr4qjetCrVy/cbneNi8qWLVtGnz59Dvt2v1+/fnzzzTfYtg2Abdt8/fXX9OvXL56RxWF1aTMVFRVce+21mKbJ3//+d1q0aBHntJIIattm+vbty7x585g1a1bsBvCHP/yBm266Kc6pxUl1eZ/p378/69evr7Fs06ZNtGnTJh5RJUHUpc00b96cjRs31li2efNm2rZtG4+o0sA0pONfFUf1IBAIMGHCBO6++25WrlzJRx99xPPPP8/PfvYzoPpbl6qqKgBGjRpFSUkJ9913Hxs2bOC+++6jsrKSiy++2MmXIHFWlzbzzDPPsG3bNh588MHYYwUFBRqtrompbZvx+/106NChxg2qv7Vr1qyZky9B4qwu7zOTJk1i/fr1PP7442zdupXHHnuM7du3M378eCdfgsRZXdrM5ZdfzptvvsmsWbPYunUrjzzyCHl5eUycONHJlyAJpMEe/zo6kHgjUlFRYd966612//797aFDh9ovvPBC7LHu3bvXGMd9xYoV9oQJE+w+ffrYP/nJT+w1a9Y4kFicVts2c9FFF9ndu3c/7DZ9+nSHkotT6vI+cyjNc9R01aXNfPXVV/bEiRPtU0891R4/fry9dOlSBxKL0+rSZt5880171KhRdv/+/e3Jkyfbq1evdiCxJIofftY01ONfw7b3n98SERERERFpwtStTkREREREBBVHIiIiIiIigIojERERERERQMWRiIiIiIgIoOJIREREREQEUHEkIiIiIiICqDgSEREREREBVByJiIiIiIgAKo5ERCQOhg0bRo8ePWK33r17M2rUKF588cXj2t7bb7/NsGHDTijP22+/fcTHduzYQY8ePdixYwcAPXr0YMmSJYc9r6ysjFmzZh13BhERSTxupwOIiEjTcPvttzN69GgAIpEIixcv5ne/+x0ZGRlMmDDB2XCHaNWqFZ999hlZWVmHPfaPf/yDpKQkAF588UWWLFmSUNlFROTE6MyRiIjERWpqKjk5OeTk5NCqVSsmTpzImWeeybx585yOVoPL5SInJweXy3XYY1lZWfj9fgBs2453NBEROclUHImIiGPcbjcej4err76ae++9l+HDh3P++edTVlbG999/z0033cTpp5/OkCFD+MMf/kAoFKrx/D/96U8MHDiQc845h1deeSW2PBQK8cc//pFzzjmH3r17M2zYMGbMmFHjubm5uUyYMIE+ffowdepU8vLygMO71R3qQLe6t99+myeeeIKlS5fSo0cP3n33XYYMGUIkEomtO3fuXM4//3wVUSIiDYiKIxERibtwOMy8efP4/PPPGT58OFB9HdHDDz/ME088gdfrZcqUKVRWVvLKK6/w6KOPsmDBAh566KHYNnbu3Mn69euZMWMGN998Mw8++GDs2qBnn32WBQsW8PjjjzNnzhwmTJjAvffeS2FhYez5r7/+Otdeey1vvfUWkUiE6dOn1zr/6NGjueaaaxgwYACfffYZw4cPp6qqisWLF8fW+fDDD7n44osxDONEd5eIiMSJrjkSEZG4uOuuu7j33nsBqKqqwu/3M2XKFC655BJmzpzJ+eefz8CBAwH4+OOP2b17N2+++Sbp6ekA3HnnnVx//fX8+te/BsDn8/HAAw+QmZlJt27dWLp0KW+88QZDhgyhZ8+enHHGGfTv3x+A//iP/+DJJ59ky5YtZGdnAzB58mTGjh0LwH333cfw4cPZuHEjPp/vR1+L3+8nKSkJj8dDTk4OABdccAFz5sxh6NChVFZWsnDhwhpns0REJPGpOBIRkbi48cYbGTlyJFBd2Pzwup42bdrEft64cSMdO3aMFUYAAwcOJBKJsG3bNgDatWtHZmZm7PFTTjmFmTNnAnDhhRfy+eef88ADD7Bp0ybWrl0LQDQaja3ft2/f2M9t27YlIyODTZs20atXr+N6fWPHjuV//ud/uPvuu1mwYAHNmzfn1FNPPa5tiYiIM9StTkRE4qJZs2Z06NCBDh060LJly8MGPDj0jM2Rzt4cKGwO3JtmzY8wy7LweDwA/PnPf+a3v/0tbrebCRMmHHa9EXDY3z/0+cfj3HPPJRqN8uWXXzJ37lwuvvji496WiIg4Q8WRiIgknE6dOrFlyxaKiopiy5YvX47b7aZ9+/YAbN++ncrKytjjK1eupHPnzgC88cYb3HHHHdxyyy2MHj06tt6hgyN89913sZ+3bNlCSUkJnTp1qnXGH15L5PV6GTFiBPPnz+fzzz9nzJgxtX/BIiKSEFQciYhIwjn77LNp164dt956K+vXr2fx4sXce++9jB07lrS0NACCwSDTp08nNzeXN954g7lz5zJlyhQAMjIy+PTTT9m+fTtfffUVt956K0CN0e5eeOEF5s2bx7p167jtttu44IIL6NChQ60zBgIB8vPza4xqN3bsWP7xj3/QsmVLunXrVh+7QkRE4kjFkYiIJByXy8Vf//pXAC6//HJuvvlmhg8fzj333BNbp1evXrRo0YLLL7+cZ599lvvvvz92jc/999/Pt99+y5gxY7jtttsYNWoUffv25dtvv409/xe/+AWPPvool19+Oc2aNeP++++vU8YRI0ZgWRZjxoxhz549AAwZMoTk5OTYZLciItKwGLYmYBAREakXZWVlnH322bz//vu0a9fO6TgiIlJHGq1ORETkBNm2zdy5c5k3bx4DBgxQYSQi0kDpzJGIiEg9GD58OC6Xi6eeeoouXbo4HUdERI6DiiMRERERERE0IIOIiIiIiAig4khERERERARQcSQiIiIiIgKoOBIREREREQFUHImIiIiIiAAqjkRERERERAAVRyIiIiIiIoCKIxEREREREQD+P4jShofA1PxyAAAAAElFTkSuQmCC"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m10/10\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 996us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1MAAAIhCAYAAACrJeDiAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAACjuElEQVR4nOzdeXxU5d3//9eZLfueEAg7AcIumywCgruyKOKuVavV1rbaftv6a+/WLrZ3l7vWbtZqXWrr1rqLKKviBgiC7FuAQICEhJCE7Nss5/z+GAgiIGFIcmaS9/PxyINk5kzmE2Am532u6/pchmVZFiIiIiIiInJGHHYXICIiIiIiEokUpkREREREREKgMCUiIiIiIhIChSkREREREZEQKEyJiIiIiIiEQGFKREREREQkBApTIiIiIiIiIVCYEhERERERCYHClIiIiJwxy7LsLkFExHYKUyIincD//M//kJOTc8qPSZMm2V3iGcnJyeFvf/vblx5z4YUX8j//8z8hP0dxcTGDBw/ml7/85SmP2bJlCzk5Obz22mu88cYb5OTkUFhY2OLn+Nvf/kZOTk7z17feeiu33npryDUfdbJ/4xEjRjBjxgyeeuopTNMEoLCwkJycHN54440z+v6PPfYY//znP8+6ThGRSOeyuwAREWkfGRkZPProoye9z+12t3M14a9bt26cd955LFy4kAceeACX68RfmXPnziUuLo7p06fT2NjIyy+/TJcuXUJ+zl/84hdnU/Jxrr32Wq677rrmrxsaGliyZAkPP/ww1dXV/OAHPwj5e//1r3/l3nvvbY0yRUQimsKUiEgn4fF4GDlypN1lRJRrrrmG5cuXs3z5cqZNm3bcfT6fj3feeYfp06cTGxtLbGwsqampZ/V8/fv3P6vHf17Xrl1P+PeeOHEie/bs4cUXX+Q73/lOqz2XiEhnpWl+IiJynFtvvZUHHniAJ598kmnTpjF8+HBuvPFGNm3a1HxMY2MjDz74IOeffz7Dhg3j8ssvP2HaV2VlJT//+c8577zzGD58ONdffz0rV6487picnBz++9//8j//8z+MGTOGcePG8etf/5rGxkZ+//vfM2HCBMaPH88DDzxAU1PTcY+tra3l/vvvZ9SoUUycOJFf//rXNDQ0nPLnampq4qGHHmLq1KkMGzaMWbNmsWDBgi/9u7j44otJTk7m7bffPuG+jz76iIqKCq699lqAk07zW7FiBTfffDNjxoxh/Pjx/OAHP6C4uPiUz/fFaX45OTm8+OKLPPDAA4wbN45Ro0bx3e9+l7Kysi+t+8sMGzaMuro6qqqqTnr/3r17+c53vsOkSZMYOXIkt956K2vXrj2uJoBHH330uCmKIiKdkcKUiEgn4vf7T/rxxWYCixcvZunSpfz0pz/lT3/6E2VlZdx3330EAgEAfvvb3/Lxxx/zox/9iH/+859cdNFFPPTQQ7z++utAMLjcfvvtLF26lO9973s8+uijdO3albvuuuuEQPWHP/wBj8fDo48+yuzZs3n++eeZPXs2xcXFPPzww9x666289tprPP/888c97vnnn6euro6//OUvfOMb3+DVV1/l/vvvP+nPbVkW3/72t3nppZe44447ePzxxxk1ahTf+973mDt37in/vjweD7NmzWLp0qXU1dUdd9/cuXMZMGDAKUf75s6dy5133km3bt3405/+xI9//GPWr1/PDTfcQHl5+Smf84v+/Oc/Y5omf/rTn/jhD3/IBx98wG9/+9sWP/6L8vPziYuLIy0t7YT78vLymDNnDoWFhfz0pz/l4YcfxjAMbr/9dlavXg3Ayy+/DASnER79XESks9I0PxGRTuLAgQMMHTr0pPf98Ic/5Gtf+1rz136/n3/+85/Ex8cDUFdXx49+9CO2b9/OsGHDWL16NZMmTWLGjBkAjB8/ntjY2OYT9Lfeeovc3FxeeeUVzjnnHADOP/98br31Vh5++OHm0AXBqW2/+tWvABg3bhyvvvoqPp+Phx9+GJfLxeTJk1m8eDHr1q07rubs7Gz+/ve/43A4mDp1KoZh8Nvf/padO3cycODA44795JNPWLZsGX/+85+ZPn06AFOmTKGhoYGHH36YmTNnnnRNFARDw/PPP897773HVVddBUBFRQUffvjhKcObaZo8/PDDTJ48mT/+8Y/Nt48ePZrp06fzz3/+kx/+8IcnfewXDRw4kN/97nfNX2/atIlFixad9nGmaeL3+4FgmCwrK+Ptt9/m/fff56677sIwjBMe8+ijj+LxeHjuueea/+2nTZvGzJkzeeihh3jttdeaw+PJphGKiHQ2ClMiIp1ERkYGjz/++Env69at23Ff9+/fv/lkGiAzMxOgeRrd+PHjeemllzh48CBTp05l6tSpfPvb324+fuXKlWRkZDB06NDmE3qACy64gIceeoiqqiqSkpIAGDVqVPP9TqeTlJQUhg4dely4SU5Opqam5rgaL7/8chyOYxMsLr30Un7729+yZs2aE8LUypUrMQyDqVOnHlfPhRdeyLx589i1axeDBw8+6d/NoEGDGDp0KG+//XZzmJo/fz4AV1555Ukfk5+fT2lp6QlNHnr16sWoUaOaR3la4ouBpWvXrl86nfGoxx57jMcee+y426Kjo7nhhhu47777TvqY1atXc8EFFxz3b+9yuZgxYwZ///vfqaurIy4ursW1i4h0dApTIiKdhMfjYfjw4S06NiYm5rivj4aWoy21H3jgAbp27cq8efP43//9X/73f/+XUaNG8eCDDzJo0CAqKyspLS095UhYaWlpc5j6/In7UbGxsaetMSMj47ivj46KVVdXn3BsZWUllmUxevTok36vQ4cOnTJMQbARxW9/+1vKy8tJS0tj7ty5XHTRRadsOFFZWQlAenr6Cfelp6ezbdu2Uz7XF53s36Ilezxdf/31XH/99QAYhkFcXBw9evT40s6NVVVVp6zZsixqa2sVpkREPkdhSkREzpjH4+Gb3/wm3/zmNykqKuKDDz7gscce4wc/+AHz588nISGBPn368PDDD5/08T169DjrGo4GlqNKS0sBTroWKCEhgdjYWJ577rmTfq/evXt/6XPNmjWL3//+9yxcuJCJEyeyefNmvvvd757y+OTkZICTNoooLS0lJSXlS5+vNXTp0qXF4fmopKSkU9YMtEvdIiKRRA0oRETkjDQ2NnLZZZfxzDPPAJCVlcUtt9zCjBkzKCoqAoJrn4qLi0lLS2P48OHNHytWrODpp5/G6XSedR0ff/zxcV/Pnz8fwzAYN27cCceOGzeO+vp6LMs6rp6dO3fy97///bipfyeTmJjIJZdcwuLFi1m4cCFZWVlfutFx3759ycjI4J133jnu9oKCAjZs2HDKETK7nXvuuXzwwQfU1tY23xYIBJg/fz7Dhw/H4/EAHDe9UkSkM9PIlIhIJ+H1etmwYcMp78/JyTlhStnJREdHM3ToUB599FHcbjc5OTnk5+fz5ptvctlllwEwZ84cXnjhBe644w7uueceunXrxieffMJTTz3FV77ylVbZJHjz5s088MADzJw5k82bN/PII49w7bXX0qdPnxOOnTp1Kueeey7f+ta3+Na3vkV2djabNm3ikUceYcqUKS3aH+qaa67hrrvuori4mDlz5nxpoHA4HHz/+9/nxz/+MT/4wQ+48sorqaio4NFHHyUpKYk77rjjbH70NnPvvffy8ccfc9ttt/H1r38dt9vNCy+8QEFBAU8//XTzcYmJiaxbt441a9YwduzYkzazEBHpDBSmREQ6idLSUm644YZT3j937twvXTf0eb/61a/4y1/+wjPPPENpaSlpaWlce+21zVPfYmNjefHFF/njH//IH/7wB2pqaujevTs/+MEPuPPOO1vl5/n2t7/Nli1buOeee0hISOCuu+7i3nvvPemxDoeDJ598kr/+9a888cQTlJeXk5mZyR133HFc44wvM3HiRLp27UphYSFz5sw57fFz5swhLi6OJ554gm9/+9vEx8czZcoUvv/975+w3itcDBgwgP/85z/NrdwNw2DEiBE899xzjB07tvm4e+65h8cee4y7776bBQsWkJWVZWPVIiL2MayWrGIVERERERGR42jSs4iIiIiISAgUpkREREREREKgMCUiIiIiIhIChSkREREREZEQKEyJiIiIiIiEQGFKREREREQkBApTIiIiIiIiIVCYEhERERERCYHL7gLCTXl5DdrGWERERESk8zIMSEtLOO1xClNfYFkoTImIiIiIyGlpmp+IiIiIiEgIFKZERERERERCoDAlIiIiIiISAoUpERERERGREChMiYiIiIiIhEBhSkREREREJAQKUyIiIiIiIiFQmBIREREREQmBwpSIiIiIiEgIFKZERERERERCoDAlIiIiIiISAoUpERERERGREChMiYiIiIiIhEBhSkREREREJAQKUyIiIiIiIiFQmBIREREREQmBwpSIiIiIiEgIXHYXICIiIiL2Wb78Q3bt2nna43r37suFF17SDhWJRA6FKREREZFOqr6+npdeegHTNE977Jo1qxg9eizJySntUJlIZFCYEhEREemkNm/egGmaXNjdzfhM9ymP21DmZ+F+Lxs3rmfq1AvbsUKR8KY1UyIiIiKd1IYNawE4J91FvNs45ceINCfG544XkSCFKREREZFOqKmpiW1bt5AZY5Ae/eWnhIkeB73iHezatYPa2tp2qlAk/ClMiYiIiHRCmzatx+f3MTS1Zas+hqa6ME2TdevWtHFlIpFDYUpERESkE1q+/CMARqW3LEydk+7CYcCKFR+1ZVkiEUVhSkRERKSTKSkpZteuHfRPcpJ6mil+R8W7DQYnOyko2M/+/XvbtkCRCKEwJSIiItLJrFixDIBzu5xZY+ejxy9f/nGr1yQSiRSmRERERDqRpqYmVq5c1jzSdCayk5ykRBmsWbOS+vq6NqpQJHJERJjyer3MnDmTTz/99JTHfPjhh1x11VWMGjWKWbNmsXTp0nasUERERCQyLF/+EXV1dYzv4sLpMM7osQ7DYGKmm6amJj78UOdaImEfppqamvj+97/Prl27TnlMbm4u9957L9dccw1z587lxhtv5Lvf/S65ubntWKmIiIhIePP5fLz37kKinAYTu556k94vc24XF3Fug/ffX0JDQ0MrVygSWcI6TOXl5XH99dezf//+Lz3unXfeYcKECdx222307t2bW265hfHjx7Nw4cJ2qlREREQk/H3yyTKqqquYkOkixnVmo1JHeZwGk7q6qK+vZ9myD1q5QpHIEtZhavXq1YwfP56XX375S4+7+uqruf/++0+4vaampq1KExEREYkoPp+PJUvm43EYTApxVOqoCZluYlwG7723iMZGjU5J53VmLVza2c0339yi47Kzs4/7eteuXaxcuZIbb7zxjJ/TCO0ijYiIiEhYe++9RVRUVDA1y02c++xOeKKcBlO6ulhSWMuiRe9w9dXXtVKVIuGhpZkgrMNUKA4fPsx9993H6NGjueiii8748WlpCW1QlYiIiIh9ysrKWLz4HRI9BlOzzm5U6qjzurn5rNTP0qVLmD79Mrp3794q31ckknSoMFVWVsYdd9yBZVk88sgjOBxnPouxvLwGy2qD4kRERERs8vTTz+D1+rgqO4ooZ+tMw3E7DKb39vDCziaefPJp7r33exia4iMdhGG0bJClw4SpkpISbrvtNgCee+45UlNTQ/o+loXClIiIiHQYO3ZsZ+3aNfSOd3BO2pntK3U6g5KdDEhysm3bFjZt2sCIEaNa9fuLhLuwbkDRUvX19dx11104HA5eeOEFMjMz7S5JRERExHYNDQ288PwzOAyY2cfT6iNHhmEwo7cHpwH/+c+z1Naq+Zd0LhEbpkpLS2lsbATgiSeeYP/+/fz+979vvq+0tFTd/ERERKRTe/XV/1B+uJxpWW6y4lp3VOqojBgHl/b0UF1dzX/+8xyWpvhIJxKxYWry5MksWLAAgMWLF9PY2Mh1113H5MmTmz9+85vf2FyliIiIiD3Wr1/LqlUr6BHnYForNZ04lfO6uuiX6GDDhuBzinQWhqXLB8cpK1MDChEREYlsVVWV/PrXP8PbUM+9w6NJj2776+eVTSZ/29KI5fTwwAO/JD29S5s/p0hbMQxITz99A4qIHZkSERERkRP5fD6efPLv1NXVMb2Xu12CFEBylIOr+nhoamriySf/TlNTU7s8r4idFKZEREREOgjLsvjvf58jP383I9NdnNulfRs3j0hzMT7TRWFhAc899zSmabbr84u0N4UpERERkQ5i6dLFrFq1gp7xDmb3bf3ufS0xo5eHfokO1q9fy4IF89r9+UXak8KUiIiISAewZcsm3nzzVZI8BrcMiMLtsGcDXafD4KYB0aRGGSxYMI9169bYUodIe1CYEhEREYlwe/fu4Zl/Po7LgFsGRpHgsfcUL9ZlcGtONFFOg2f//RQ7d+6wtR6RtqIwJSIiIhLBCgr287e//RGvt4nrsz10b6P9pM5UlxgHNw+Iwgz4efzxv5Cfv9vukkRancKUiIiISIQqLi7ib488TGNDA9f2i2JIavs2nDid/klObhoQhc/bxKN/+xMFBfvsLkmkVSlMiYiIiESgQ4dKeOSvf6C2rpbZfT2ckx5eQeqowSkursuOorGxgb898keKig7YXZJIq1GYEhEREYkwJSUH+etf/0BVdRUze3sY28Vtd0lfakSaizn9PNTW1fLII3+gsLDA7pJEWoXClIiIiEgE2bs3nz8+/FsqKg5zeS8PE7uGd5A6anSGm6v6eKiprubPf/o/8vJ22l2SyFkzLMuy7C4inJSV1aC/EREREQlH27Zt4cknH8Xv83JVn/AfkTqZTeV+XtvdhOF0ceed9zBy5Gi7SxI5gWFAenrC6Y9TmDqewpSIiIiEozVrVvHss0/jwOTG/lEMTgnPNVItkVcV4MVdTfhMuPnm25k06Xy7SxI5jsJUiBSmREREJJxYlsXSpYt5441XiHYZ3DYwit4J4dH+/GwcqAvw7I4m6nwWM2ZcxRVXzMLh0AoUCQ8KUyFSmBIREZFw4fU28eKLz7JmzSoSPQZfzYkmM7bjBI6yRpNnc5s43GQycuQYbrvtTqKjY+wuS0RhKlQKUyIiIhIOysvLePKJRyko3E/veAc3DYgiwdNxgtRR9T6Ll/Ia2V1t0rVrN+655zt06ZJpd1nSySlMhUhhSkREROy2Y8d2/vn049TW1TKui4sZvT24HIbdZbWZgGWxZL+X5Qf9xETHcOfXvsHQoSPsLks6MYWpEClMiYiIiF0sy+KDD97ljTdewbBMZvXxcG4EduwL1cYyP2/kNxGwYObMq7nsshlaRyW2UJgKkcKUiIiI2KGmpprnn/8XW7ZsJMFtcPOAKHp1gEYTZ+pAXYD/7Gyi0muRkzOY22+/i+TkFLvLkk5GYSpEClMiIiLS3rZt28Jzzz1NdXU1A5KcXNPP0yHXR7VUvc/ijfwmtlcEiIuL4ytfuZNzzhlld1nSiShMhUhhSkRERNqLz+dj3rzXWbp0CU4DLuvpYWJXFw6j466PainLslhzyM+C/T58psWUKdO45pob8Hii7C5NOgGFqRApTImIiEh7OHiwmGee+QeFhQVkxDi4PttDVlznm9Z3OocaTF7Oa+JgfbDb3513foMePXrZXZZ0cApTIVKYEhERkbYUCARYunQJ8995E5/fz7guLq7o5cHj1GjUqfhNiyUFXlYc9ON0Orn88plcdtkMXC6X3aVJB6UwFSKFKREREWkrhYX7ef75Zygo2E+822B2Xw+DUxQIWiqvKsCbe4LNKbK6decrt95Jnz597S5LOiCFqRApTImIiEhr8/l8LFr0NosXL8A0TcZkBEejYlwajTpTTQGLxQVePi3xYxgGF154KbNmzdZaKmlVClMhUpgSERGR1rRnTx4vvPAvDh4sJiXKYHbfKPonaW3U2dpbHeDN/CbKGi3S0zP4ylfuYODAQXaXJR2EwlSIFKZERESkNdTV1TJv3pssX/4hWBYTu7q4pIfWRrUmn2nxfqGP5Qd9mBZMnDiZ2bOvJSEh0e7SJMIpTIVIYUpERETOhmmarFy5nLlzX6Wuro4uMQ6u7uvplBvwtpcDdQHm7vFSVG8SExPDrFlzmDJlGk6n/s4lNApTIVKYEhERkVDt25fPSy+9wL59+UQ5DS7q7mZCpgunQ6NRbc08si/Vu4U+GvwW3bv35MYbv0J29gC7S5MIpDAVIoUpEREROVO1tbXMm/c6K1Z8jGVZjEx3cXlPNwkeh92ldTp1vmAb9bWlfixg/PjzuPrq60hMTLK7NIkgClMhUpgSERGRlgoE/Cxf/hFvv/0m9fX1dI11MKuPhz6a0me7gtoAb+/1cqDOJDo6munTr2Tq1Itwu912lyYRQGEqRApTIiIicjqWZbF58wbefPNVSkoOEu00uLiHm3GZLpyGpvSFC9OyWFvqZ0mBj3q/RXp6OrNnX8eoUWMx9O8kX0JhKkQKUyIiIvJl9u/fy+uvv8yuXTtwGDC+i4sLunuIc+vkPFw1+C0+LPKx8qCPgAV9+2ZzzTU30K9ff7tLkzClMBUihSkRERE5mcOHDzNv3uusXr0SgMEpTi7v6SE9RuuiIsXhRpMlBV42Hw4AMHr0WGbPvpb09C42VybhRmEqRApTIiIi8nkNDfUsWbKQ95cuxuf3kxXnYHovD30TtS4qUu2vCbBwv5f9tSZOp5Np0y7isstmEh8fb3dpEiYUpkKkMCUiIiIAXq+Xjz5ayuLF86mvryfJY3BpTw8j0pw4tN4m4lmWxdbDARYXeDncZBEdHc3FF1/OhRdeSnR0tN3lic0UpkKkMCUiItK5BQJ+VqxYxsKF86iqqiLGZTA1K7hflFv7RXU4fjO4P9UHRT7qfBbx8fFcfvkspkyZps5/nZjCVIgUpkRERDon0zRZu3Y1b7/9JmVlpXgcBpO6uZjc1U20SyGqo2sKWHxy0MeyYj9NAYuUlFRmzryKcePOw+nUlM7ORmEqRApTIiIinYtlWWzZspF5897gwIFCnAaMz3QxNctDvDr0dTr1fouPi3ysLPHhNyEzsytXXjmHkSPHqJ16J6IwFSKFKRERkc7Bsiy2b9/K22+/yb59+RjA6AwXF3R3kxKlDn2dXbXX5IMDPj4r9WNa0LNnL2bMuIrhw0cqVHUCClMhUpgSERHp2CzLYseObbz99lzy83cDMDzVyYU9PHRRm3P5gvJGk/cP+NhY5scCevXqw4wZVzFs2AiFqg5MYSpEClMiIiId186dubz99pvs3r0LgKEpwRDVNVYhSr5caYPJBwe8bCoPYAG9e/dl1qyrGTx4qEJVB6QwFSKFKRERkY5n164dvPPOXHbt2gEEN9y9qLubbnFqLCBn5lC9yfsHjm3827dvNrNmzSYnZ4hCVQeiMBUihSkREZGOY9euHcyf/xY7d+YCMCjZyYU93HRXiJKzdLDe5P1CL1srgqGqX7/+zJhxFYMGKVR1BApTIVKYEhERiWyWZbFzZy4LFsxrHokamOTkoh5uesQrREnrKq4LsPSAj+0VR0eq+jF9+lUMGTJMoSqCKUyFSGFKREQkMlmWRW7uNhYsmNe8JmpQspMLuitESdsrrgvwwQFf80hV7959mD79SoYNO0ehKgIpTIVIYUpERCSyWJbFtm1bWLBgXnN3vsEpwRCl6XzS3krqg40qthwONqro2bMX06dfxYgRaqkeSTpUmPJ6vcyZM4ef/exnjB8//qTHbNu2jV/84hfs3LmT/v3788tf/pJhw4ad8XMpTImIiESGo5vtLljwNvv25QMwLNXJtCw1lhD7Hao3+bDoWPe/7t17Mn36LM45ZzQOh7pHhrsOE6aampr4wQ9+wLvvvstzzz130jBVX1/PpZdeyqxZs7j22mv573//y8KFC3n33XeJjY09o+dTmBIREQlvpmmyadN6Fi58m4KC/RgEQ9QF3T1kqsW5hJnSBpMPi47tU9WtWxZXXDGL0aPPVagKYx0iTOXl5fGDH/zgyOZ6O04Zpl577TUef/xx3nvvPQzDwLIsLrvsMu655x7mzJlzRs+pMCUiIhKeTNNk/frPWLjgbYqKD2AA56Q5mdpdm+1K+CtvNPmoyMf6Mj+mBZmZXbn88pmMHTsep1MjqeGmpWHK1Q61hGz16tWMHz+e733ve4wcOfKUx23cuJExY8Y0z0M1DIPRo0ezYcOGMw5TIiIiEl4CgQBr165m4cK3KSk5iMOA0Rkupma5SY9WiJLIkBbtYE6/KC7o7uajIh/rDh3k2WefZv78t7j88pmMHz8RpzOsT83lJML6X+zmm29u0XGlpaX079//uNvS0tLYtWvXGT+n1gWKiIiEh0DAz+rVq1i48B1KSw/hNODcLi7O7+YmVSFKIlRKlIPZfaO4IMvNsmIfa0pLeeGFf7FwwTwuu3wGEydOxuUK61P0TqGlmaBD/Es1NDTg8XiOu83j8eD1es/4e6WlnX44T0RERNqOz+fj448/5s033qDk0CFcDhifGQxRyVEKUdIxJEU5mNknivOz3Cwv9rH60GH+85/nWLx4PldffTUXXnghbrfb7jLlNDpEmIqKijohOHm9XqKjo8/4e5WXa82UiIiIHfx+PytXLmfRovkcPlyOywHndXUxpZubRI9ClHRMiR4H03tHcX6WxfJiH5+WHObpp5/mtVdf49LLpjN58lSFKhsYRssGWTpEmMrMzKSsrOy428rKyujSpcsZfy/LQmFKRESkHfl8Pj75ZBlLlsynoqICt8Ng0pEQlaAQJZ1EvNvg8l4epnRzs+Kgj5UlVbzyyn9YvGg+l1x6BZMnT8XjibK7TPmCDhGmzjnnHJ566iksy2ru5rdu3Truueceu0sTERGRU/D5fCxf/hHvLllAZVUlbofBlG5uJndzE+/WIma77KkKsPygj8ld3fRLUpe59hbnNri0p4fJXY+Gqmpee+0lFi9ewCWXXM6UKRcQFaVQFS7CujX65+Xk5BzXGr20tJSEhASio6Opra3lkksuYcaMGdx444289NJLLFq0iCVLlmifKRERkTBzdCRq0aK3qaqqwuM0mNDFxeRubuIUomz39LYG8mtM+iY4uGtIjN3ldHoNfisYqg76aQxYJMQncOllM5gyZdoJPQOk9bS0NXrEjp1PnjyZBQsWABAfH88TTzzB2rVrmTNnDhs3buTJJ5884yAlIiIibcfv97N8+Yc8+OD/8PLLL9BQU83ULDf/3zkxXNbLoyAVJpoCx/8p9opxGVzcw8P9I2O4sLsbX0Mtr7/+Er/4+Y/44IP38Pl8dpfYqUXMyFR70ciUiIhI6woEAnz66ScsXDCP8sPluB0GEzKDa6IUoMLP3zc3UFRvkhXr4NvDNTIVbo6OVH1y0E9TwCI5OYXLL5/JeedNUUv1VtTSkSmFqS9QmBIREWkdgUCAzz77lPnz36KsrDTY4ryLi/OzPFoTFcYUpiJDvc9i+ZHpf17TIiUllenTr2TChPO0+W8rUJgKkcKUiIjI2bEsiw0b1jFv3uuUlBzE5YBzM1ycn6UW55FAYSqy1PkslhX7WFXix2dapKdnMGvW1YwZMw6HQ6+3UClMhUhhSkREJHQ7dmxn7tzX2LcvH4cBYzNcTMtyk6TNdiOGwlRkqvGafFzs49MSPwELevToyVVXXcuQIcMwDI0EnymFqRApTImIiJy5/fv38tZbr7N9+1YARqQ5ubiHh7RohahIozAV2SqbTJYW+lhf5scCBgzI4aqrrqFfv/52lxZRFKZCpDAlIiLSciUlB3n77TdZt24NAAOSnFza001WnPYnilQKUx1DSb3Ju4VetlcE2zKOGDGKK6+cQ1ZWd5sriwwtDVNanSYiIiJnrKammnfeeYsVKz7CNE16xju4tKeHfokKUSLhIDPWwVcGRrO/JsDiAi+bNq1n8+YNTJgwiSuvnENSUrLdJXYIClMiIiLSYn6/n48+WsqC+fNoaGwgI8bg0h5RDE5xal2GSBjqleDkrsHR7KwKsKTAx8qVy1m3bg2XXTaTiy66FLfbbXeJEU1hSkRERE7Lsiw2b97A66+/TGnpIWJcBrN6ezg304VTIUokrBmGQU6yiwFJTtaV+llS6GXevNdZvvxD5sy5nlGjxupiSIgUpkRERORLHThQyGuv/ZcdO7bjMOC8ri4u7O4hxqWTL5FI4jAMxnZxMyzVxUdFPlYcLOfppx8nO3sA1113E7169bG7xIijBhRfoAYUIiIiQXV1tcyb9wbLl3+EZVnkJDu5opeHjBh16OvI1ICi8zjcaLK4wMuWwwEMA8aPn8TVV19HQkKi3aXZTg0oREREJCSWZfHZZ5/y2qv/paa2hi4xDqb38jAgWacNIh1JarSDmwZEk18dYP4+L6tWrWDTpvXMmXMDEydO1tS/FtDI1BdoZEpERDqz0tJDvPTS82zfvhW3w+DiHm4mdtW6qM5EI1Odk2lZrD7kZ0mBj6aARf/+A7n55tvp2rWb3aXZQvtMhUhhSkREOqNAwM+77y5m4YK38Pn95CQ7mdXHQ0qUpvR1NgpTnVu112T+vuDUP6fTyWWXzeCyy2Z0uq5/muYnIiIiLZKfv5sXX/g3RcUHSHAbXNs/iqGpanUu0hkleoJT/3Ir/Ly918uCBfP47LNPufnm2xk4cJDd5YUdhSkREZFOKhDws2DBPBYtmg+WxfguLi7t6SFaXfpEOr1BKS76JjpZWujlk4Ml/OUvD3HRRZdx5ZVzOt0o1ZdRmBIREemEDh0q4V//epJ9+/JJjXJwbXYUvROcdpclImEkymkwvXcU56S7eDXPy9Kli8nN3cYdd3ydrKzudpcXFrRm6gu0ZkpERDoyy7JYuXI5r7zyIl6vl9HpLmb28RDl1GiUBGnNlJyMN2CxaL+XTw/5cbtczLnmBs4//8IOOx1Ya6ZERETkOLW1tfz3v8+yfv1aYlwGN/aPYniaTgVE5PQ8ToMr+0YxMNnJG/leXn75RbZs2cytt95BYmKS3eXZRi16REREOoGCgn387re/YP36tfRLdHDfsGgFKRE5Y4NSXHxneAwDk5xs3bqJ3/zm5+Tn77a7LNsoTImIiHRw69ev5Y8P/47Kygou7eHmjkHRJKnluYiEKN5tcFtOFDN6e6irreHPf/49n3660u6ybKFLUiIiIh2UZVksWvQOb7/9Jh6nwVcGRjEoRb/6ReTsGYbBeV3dZMY4+G9eE88++xTFxQe48so5OByd52JN5/lJRUREOhGv18u//vUEb7/9JilRBvcMiVaQEpFWl53k5J6h0aRHO1iyZAFPPvkojY0NdpfVbhSmREREOpj6+jr+8peH+Oyz1fRJcPDNoTFkxupXvoi0jfRoB/cMjWZAkpNNmzbwxz/+jpqaarvLahd6ZxURIDgd6OWXX+D5559BOyaIRK6Ghnr+9rc/snfvHkanu7hjUDRx7o7ZulhEwkeMy+DWnCgmZLo4cKCQv/7lD9TW1thdVptTmBIRAJqamvjoo/dZuXI5VVWVdpcjIiFoaGjg0Uf/xL59exmb4eLqfh5cDgUpEWkfTsNgZm8Pk7q6KCo+wF//+gdqa2vtLqtNKUyJCAA+n/dzn/tsrEREQtHY2Mjf//5n8vP3MDrDxVV9PTg66GaaIhK+DMPgil4eJh4ZofrbIw9TX19nd1ltRmFKRIDgYvWTfS4i4c/r9fLYY39hz548Rqa7uFpBSkRsZBgGM3p7GNfFRUHhfh555I8dtimFwpSIAMFpfsc+b7SxEhE5U6+++h/y8nYyIs3JNf0UpETEfoZhMKuPh7EZLvbv38uLLz7bIddkK0yJCHB8gGpsVJgSiRSffrqSFSs+JivOwTX9ohSkRCRsOAyDK/t66JPgYO3a1Sxb9qHNFbU+hSkRAY4PUxqZEokMxcVF/Pc/zxLtNLipf5SaTYhI2HEaBjf0jyLObfDaq/9h//69dpfUqhSmRAQIdgE7qqPOaxbpSJqamnj66cfw+rxc089DarR+pYtIeEr0OLg+O4pAIMDTTz9GQ0O93SW1Gr3ziggA9fX1n/tcYUok3C1a9A7FxUVM6upiSKrL7nJERL5U/yQn07q7KSsrY968N+0up9UoTIkIwHFXiTpyC1ORjqCqqooP3n+XJI/BJT09dpcjItIiF3R3kxZtsHz5h5SVldpdTqtQmBIR4PiRqY40/C7SES1a9DZen5cLurtxa52UiEQIp2FwcQ8PgUCA+fPfsrucVqEwJSIA1NXVnfRzEQkvZWWlLF/+EenRBqMzNL1PWk+9z+K9Qi+HGkwAqr0m9b6O18pa7DUs1Um3WAerV6+kqOiA3eWcNYUpEQGOn9qnaX4i4WvJkoUEAgEu7uHBqTbo0kqaAhZPbW/ggwM+/EfyU60fntreQFNAgUpaj8MwuKSnG8uyWLToHbvLOWsKUyICfG40ynBoZEokTJmmyYYNn5HgNhia6rS7HOlAPjjg41DDiaHpUIPFBwd8NlQkHdnAJCfp0QZbNm/E54vs/18KUyICQH19LTg84IzRyJRImNq7dw+1tbUMSnFqc15pVXuqAyHdJxIKwzAYlOKisamRXbt22F3OWVGYEhHgyMiUMwqc0dTW1tpdjoicxKZNGwAYnKJRKWldVd5TT+X7svtEQjU4Ofg+tnnzBnsLOUsKUyICHAlTjigMZxQNDfVYln55ioSbzZs34HZAv0SFKRGJbD0THMS6jOaLRJFKYUpEME2TxsYGDGcUOKKwLIvGxka7yxKRzzFNk5KSg3SPc6gduohEPKdh0DPeQUXF4Yg+51CYEhEaGxuCnzijgh9orymRcFNXV4tpmiS4FaREpGM4+n5WU1NtcyWhU5gSkWNXhBweDIf7+NtEJCxUVVUBkOBRmBKRjiH+SJiqrq6yuZLQKUyJSHNwMhzuYEc/PjdaJSJh4ejJRrxGpkSkg0hQmBKRjqCp6ejIlDv4ATQ1NdlYkYh8kdcbfE1qvZSIdBSeI710IvmcQ2FKRI5tmGe4wOE6/jYRCQsJCUkA1PrUaVNEOoaaI+9niYlJNlcSurAOU01NTfzkJz9h7NixTJ48mWeeeeaUx7777rtcccUVjBo1iptuuomtW7e2Y6Uikc3n8wY/cTjBcB5/m4iEheTkZABqtOePiHQQR9/PkpKS7S3kLIR1mHrooYfYsmULzz77LL/4xS949NFHWbRo0QnH7dq1ix/84Ad84xvf4K233mLw4MF84xvfoKFBaz5EWsLvP7K7veHEOBKmmm8TkbBw9MptjUamRKSDOPp+lpSkkalWV19fz6uvvsoDDzzA0KFDueSSS7jrrrt48cUXTzh2xYoV9O/fn9mzZ9OrVy++//3vU1paSl5eng2Vi0Qe0zwanBxgOL5wm4iEA7fbTXxcPOWNpt2liIi0isONFi6Xi9jYOLtLCVnYhqnc3Fz8fj+jRo1qvm3MmDFs3LgR0zz+F0lycjJ5eXmsXbsW0zR54403iI+Pp1evXu1dtkhEan5NGQYQXNweCChMiYSb7P4DONxkUdmkQCUika3eZ1Fcb5KdPQCHI2wjyWm57C7gVEpLS0lJScHj8TTflp6eTlNTE5WVlaSmpjbfPn36dN5//31uvvlmnE4nDoeDJ554IqQhQ0NNkqQTsqzgiZmB43MvAkuvB5EwM2jQEDZuXE9eVYCxXSL35ENEZHd1AAsYPHhIWJ5vtLSmsA1TDQ0NxwUpoPlrr/f4hfEVFRWUlpby85//nHPOOYf//ve//PjHP+bNN98kLS3tjJ43LS3h7AoXiUDx8dEnvS09Xa8HkXBy3nnjePnlF4+EKbfd5YiIhCyvKjgDZsKEcyP6fCNsw1RUVNQJoeno19HRx5/4PfzwwwwcOJBbbrkFgP/93//liiuu4PXXX+frX//6GT1veXkNltb2SidTU3Nknynj87c1UFZWY09BInJSbnc8qSmp5FVX4DctXNpzSkQikGlZ7KoKEBcXR0JCeliebxhGywZZwjZMZWZmUlFRgd/vx+UKlllaWkp0dDSJiYnHHbt161ZuvfXW5q8dDgeDBg2iqKjojJ/XslCYkk4ueHKm14JIODIYM3Y87767kK2HA5yTHra/xkVETmlnZYAqr8WU8eMwDEdEn2+E7YTrwYMH43K52LBhQ/Nta9euZfjw4ScsUuvSpQu7d+8+7rb8/Hx69OjRHqWKiIi0mylTpmEYBp8e0sbaIhKZPi3xAzB16gU2V3L2wjZMxcTEMHv2bB588EE2bdrEe++9xzPPPMNtt90GBEepGhuDU5Ouv/56XnnlFebOncu+fft4+OGHKSoq4uqrr7bzRxCJGFYkXxIS6WTS0zMYOnQ4+2pMDtarq5+IRJbyRpNdVQH69x9IVlbkD3yEbZgC+PGPf8zQoUO5/fbb+eUvf8l9993HpZdeCsDkyZNZsGABEOzm97Of/YwnnniC2bNns27dOp599tkzbj4h0lkd227AwdG3hS9uQSAi4WPq1AsBWHlQo1MiElk+LfFhcex9LNKF9WTrmJgYfv/73/P73//+hPt27Nhx3NfXXXcd1113XXuVJtKhHNtn6lhrdG3aKxK+Bg8eRmZmV9YdOsj5WW7SosP62qiICABVTSafHvKTkpLKOeeMtrucVqF3XxHB7w/OXcYwgoEKbdorEs4cDgdXXXUNpgXvFnhP/wARkTDw/gEffhNmzbq6ucFcpFOYEhF8viMnY4YbHMG9a764NYGIhJdzzhlN37792Hw4wIE6XfwQkfBW2mCyttRPVrfujBs30e5yWo3ClIgcC04OFxjBK0VNTU02ViQip2MYBlddFZzevni/V41kRCSsLSnwYgFXzb72hM7ckazj/CQiErKGhobgJw4PhsMDQGNjg40ViUhLDByYw7Bh57C72mTLYY1OiUh42lHpZ1tFsIPfsGEj7C6nVSlMiQi1tcGdxw1XNLiiAairq7WzJBFpoeuvvxm32807+7w0+DU6JSLhxRuwmLfXi9Pp5KabbsU40uiqo1CYEpHmMIUzBhxRgEFNTY2tNYlIy6SnZzBz5mxqfRaL1YxCRMLM0kIvlU0Wl146nW7duttdTqtTmBIRqqoqweHGcLiDV4xcsVRWVtpdloi00IUXXkqPHj1Zc8jP3mpN9xOR8HCgLsCKg366dMnk8stn2l1Om1CYEhHKy8vBndD8teFOoKLisDbuFYkQTqeTW275KoZh8Ea+l6aApvuJiL18psXre4JNJ26++XbcbrfdJbUJhSmRTq6xsYH6+joMd+KxG90JBAJ+qqur7CtMRM5I7959ufTS6ZQ3mizYp+l+ImKvdwu8lNSbTJt2EQMHDrK7nDajMCXSyR06VAJwXJgy3EkAlJQctKUmEQnNjBlX0atnbz4r9bPtsN/uckSkk9pdFZze17VrN2bPvs7uctqUwpRIJ1dcXBz8JCql+TbjyOcHDxbbUZKIhMjlcvHVO76O2+3mzXwvNV5N1RWR9lXvt3htTxNOp5M77vg6Ho/H7pLalMKUSCd38GARAEZUavNtClMikatr125cc82NR05ovJjazFdE2ollWczd00S112LWrKvp2bO33SW1OYUpkU7uwIFC4PgwhScFMCgqKrSnKBE5K1OmTGPEiJHkVQX4uMhndzki0kmsKvGztSLAwIGDuPjiy+0up10oTIl0cgcOFIArHsMZ3Xyb4XCBJ5nCwgIsXdUWiTiGYXDrrXeSmprKe4U+8tUuXUTaWGFtgIX7vSQkJHDHHd/A4egcMaNz/JQiclL19XVUVBzGiE474T4jOp2GhnoqKg7bUJmInK24uHi+9rVvYjgcvLy7iVqfLoyISNto8Fu8lNeEicGdd95DUlKS3SW1G4UpkU6ssLAAACMq/YT7jKi0444RkcjTt282V199PTVei1fzGrV+SkRanWVZvLGniYomi+nTryQnZ7DdJbUrhSmRTqw5TEWfJEwdue3AAYUpkUh24YWXcM45o8irNllaqPVTItK6lhX72FYRYNCgIVxxxSy7y2l3ClMindjRoHR0FOrzjo5WaWRKJLIZhsFtt32NjIwufFjkI7dC+0+JSOvYUxVgSYGPlOQU7rjj651mndTndb6fWESaFRcfAMMJnpPMbXbFgjM6eIyIRLSYmFjuvvvbuN1uXt3j5XCj9p8SkbNT5TV5aXcTDqeTu+7+FgkJiXaXZAuFKZFOyrIsiouLwJOMYZz4VmAYBoYnhUOHSvD7dSVbJNL16NGTm2/+Ko1+i//sasIb0PopEQmN37R4aVcTdT6L6667mb59s+0uyTYKUyKdVFVVJU1NTc0b9J6MEZWCaZqUlh5qx8pEpK2MHz+R88+/gOJ6k3l7vdr6QERCsmC/l/21JuPGTWTKlGl2l2MrhSmRTqqsrBQAw/0l7UuP3FdeXtYeJYlIO7j22pvo2zeb9WV+Pj2kUWcROTPrS318WuKne/ce3HzzbRiGYXdJtlKYEumkmsOU59RznI/eV15e2i41iUjbc7lc3H33t0hISGD+Pi/7arShr4i0TFFdgLl7vcTGxPKNb9yLxxNld0m2U5gS6aSaN+N1J5z6oCP3HT6sjXtFOpLk5BTuvvvbYDj4764mqr1qSCEiX67+yHrLgAV33PkN0tO72F1SWFCYEumkqqurATCcsac8xnDFAFBTU90uNYlI++nffyDXXHMDNT6Ll/OaCJhaPyUiJ2daFq/kBTfmnTnzaoYOHW53SWFDYUqkk2oOSEcC00kdCVoKUyId07RpF3PuuRPYW2OyqMBrdzkiEqbeP+BjV1WAESNGctllM+wuJ6woTIl0UnV1tcFPnNGnPMZwuMDhOnasiHQohmFw8823k9WtO58c9LOxTA0pROR42yv8fHDAR0ZGF2677a5OuTHvl9Hfhkgn1dTUBIbrpHtMHcfhCR4rIh1SVFQUX//GvURHR/NmvpeSeq2fEpGg8kaT13Z78bg9fP3r9xIbe+qlAZ2VwpRIJ9XU1AgO9+kPNNw0Nja2fUEiYpsuXTL56lfvxmdavLiriUa/1k+JdHbeQLDhRGPA4pavfJXu3XvYXVJYUpgS6aS8Xi84XKc/0OEKHisiHdqIEaO47LIZlDeavJnfpA19RTq5t/d5OVhvMnXqRZx77gS7ywlbClMinZTP5wPDedrjDMOJz+9rh4pExG4zZ85m4MBBbDkcYGWJ1k+JdFafHfKxrtRPnz79uOaaG+wuJ6wpTIl0Un6/v0VhCsNJwK+TKpHOwOl0cued3yApMYmF+73s14a+Ip1OUV2At/f5iIuL4667vonL1YJZLJ2YwpRIJ+UP+DFaFKYcBAIBTfkR6SQSE5P42l3fDG7om9dEnU+vfZHOotFv8d9dXgKWxVe/+nVSU9PsLinsKUyJdFIBf6DFI1MAgYCuUIt0Fv37D+Sqq66l2mvx+p4mTF1MEenwLMtibn4Th5tMLr98pjbmbSGFKZFOyLIsAgE/nK4tOjSHKb/WTYl0KhdddClDh45gR2WATw5qqq9IR7em1M/mwwH69x/I9OlX2V1OxFCYEumEfL4j3fla2M0PUEc/kU7G4XBw++1fIykpicUFXgpqNTot0lEdrDeZf2Sd1J13fgOnswUzVwRQmBLplJo34W3JPlNHjvF6tXGvSGcTH5/AnXfeg4XBS3lNNGj/KZEOxxuweGlXE37T4vbb7yY5OcXukiKKwpRIJ9QcpozThynjSJjSxr0indOAATnMmHEVlU0W8/bqoopIR7Ngv5fSRpOLL76cYcNG2F1OxFGYEumEamtrATBc0ac/2Bk8pq6uri1LEpEwdvnlM8nOHsCm8gAbyrR+SqSj2HbYz5pDfnr27MWVV86xu5yIpDAl0gnV1tYEP3G2PEzV1NS0YUUiEs4cDgdf/erdREdHM2+vl4om0+6SROQsVXtN3sz34na7ueOOb2g/qRApTIl0QtXVVQAYztjTHmu4Yo88prItSxKRMJeWls4NN9xKU8Di1Ty1SxeJZKZl8cYeL/V+i2uuuZGuXbvZXVLEUpgS6YTKykqDn3gST3us4U488piytixJRCLAuHETGDt2PPtqTZYXa7sEkUi15pCfXVUBhg07hylTptldTkRTmBLphI6GqaNB6Us1h6nStixJRCKAYRjceONXSEpM4r1CH4fqNd1PJNIcbjRZtD/YBv0rX/kqhmHYXVJEU5gS6YRKSoqD+0e54k57rOH0gDOGgweL26EyEQl3sbFx3HzLVwlY8NqeJgKa7icSMYLT+5rwmhY33PAVEhOT7C4p4oUUpi644AIefvhhtm3b1tr1iEgb8/l8FBUdwIhKb/HVKCM6g7KyQzQ01LdxdSISCYYPP4cJEyZxoM5keZGm+4lEik9L/OTXmIwaNYYxY8bZXU6HEFLbjv/5n/9h0aJF3HLLLWRmZjJ9+nRmzJhBdnZ2a9cnIq2suLiIQCCAIzqjxY8xotOx6vZTULCfgQMHtWF1IhIprr32JnK3b2XpgUoGp7roEqPJLpHuN7/5zUlv/92DP23nSqQtVDSZLC7wER8Xz4033qrpfa0kpHe+yy67jD//+c+sXLmS7373u+Tn53PzzTdz5ZVX8uSTT1JYWNjadYpIK9m9excARnSXFj/GiM4EYM+evDapSUQiT2xsLDfdfBsBC97KV3c/kXBmWRbz9nrxmRbXXX8zCQktWDMtLXJWDeWjo6O57LLLSE5OJjU1lddee41///vfPPbYY4wePZqf/exn9O3bN+Tv39TUxC9/+UuWLFlCdHQ0d955J3feeedJj92xYwcPPvggW7dupXfv3jzwwANMmDAh5OcW6ah27twOgBHXvcWPMeKyANixYzuXXz6zTeoSkcgzfPhIRo0ay/r1n7Gu1M/YLm67S5Kz8MADD5z09ni3RjAi3ZbDAXZWBhg8eBhjx463u5wOJaSRKdM0+eSTT/j5z3/O5MmT+X//7//R1NTEP/7xD5YvX87y5ctJSUnhm9/85lkV99BDD7FlyxaeffZZfvGLX/Doo4+yaNGiE46rqanhzjvvpH///rz99ttccskl3HvvvZSXl5/V84t0NKZpsnPnDvAkYbgTWvw4wxkN0Rns3p2Hz6f1ESJyzHXX3UR0dDSLCnzU+jQ6JRJuGvwW7+wLbs57002a3tfaQgpTEydO5Nvf/jY1NTX86le/YtmyZfz6179m4sSJOBwO4uPjueSSS/D7/SEXVl9fz6uvvsoDDzzA0KFDueSSS7jrrrt48cUXTzj2zTffJDY2lgcffJDevXvzne98h969e7Nly5aQn1+kI8rL20VDQz2OuF5n/FhHXE/8fh+5uVvboDIRiVTJySnMnn0dDX6Lhfua7C5HRL5gSYGXWp/FjBmzSU9v+XppaZmQpvn99Kc/5aKLLiI2NvaE+w4fPkxqaiqXX345l19+eciF5ebm4vf7GTVqVPNtY8aM4R//+AemaeJwHMuBq1ev5qKLLsLpdDbf9vrrr4f0vArr0pFt3LgWACOh3xk/1kjoB+Xr2LhxHSNGjGzlykQkkk2ZMpVVq5azYW8+4zID9E5wnv5BItLmiusCrDnkp1u3LC6++BKd556Blv5dhRSmfvjDH7JixYoTwtSBAweYOXMm69evD+XbHqe0tJSUlBQ8Hk/zbenp6TQ1NVFZWUlqamrz7QUFBYwYMYKf/exnvP/++3Tv3p0f/ehHjBkz5oyfNy2t5VOfRCKJaZps2rQewxmNEZt1xo83ortguOLZtGkDyckxuFxnteRSRDqYr3/9bn7yk5/wzj4v3xwajUNnbSK2sqzg9D4LuPvuu8jMTLG7pA6pxWdDc+fO5Y033gCC/zjf/va3cbuPX2h66NAhMjJaZ/iwoaHhuCAFNH/t9XqPu72+vp4nn3yS2267jaeeeor58+fzta99jYULF9KtW7czet7y8hrUkEg6oh07tlNeXo6RPATDOPMZvoZhYCRmU3t4Ix999AnnnDPq9A8SkU4jJaUrEyZMYtWqFWpGIRIGthwOsLfGZOTIMXTr1oeyshq7S4oohtGyQZYWh6lLLrmkueX56tWrGTlyJHFxcccdExsbyyWXXHKGpZ5cVFTUCaHp6NfR0dHH3e50Ohk8eDDf+c53ABgyZAgrVqzgrbfe4p577jmj57UsFKakQ1q5cgUAjqTQ94lyJA3CPLyRlStXMGKEwpSIHO+qq65h/frPWFLoZViqi2iXRqdE7OAzLRbt9+JyuZgz53qd27ahFoepuLg47r33XgC6d+/OjBkzThg5ak2ZmZlUVFTg9/ubpxOVlpYSHR1NYuLxvfEzMjLo1+/4NSB9+vShuLi4zeoTiSQNDfWsX78Ww5OEEdM15O9jRKdDdAabN2+gurqKxMSkVqxSRCJdUlIyV1wxi7lzX2NZsY9LerbdeYKInNqqgz4qvRaXXXaZmk60sTOa5jd9+nQ8Hg+GYbBgwYJTHjt79uyzLmzw4MG4XC42bNjA2LFjAVi7di3Dhw8/rvkEwMiRI1mzZs1xt+3Zs4eZM7UfjgjAqlWf4PU24egy+qxbojqSh2Ae/IgVKz7miitmtVKFItJRTJt2MR988C4rDlYzsatbexSJtLMGv8VHxX7i4uK49NIr7C6nw2txmHrkkUeYOnUqHo+HRx555JTHGYbRKmEqJiaG2bNn8+CDD/Lb3/6WQ4cO8cwzz/C73/0OCI5SJSQkEB0dzY033sgLL7zA3/72N6688krmzp1LQUEBV1111VnXIRLpLMvio4/fB8OJI2nIWX8/R9JAzEMrWbbsQy69dPpxXTRFRDweDzNmzOY//3mWDw54mdUnyu6SRDqVZcU+GvwW11w1k5iYEztvS+tqcZh6//33T/p5W/rxj3/Mgw8+yO233058fDz33Xcfl156KQCTJ0/md7/7HXPmzKF79+48/fTT/OY3v+HJJ58kOzubJ598kszMzHapUyScbd++lUMlBzGSBmG4ok//gNMwHB4cSTlUVmxm48b1jB49thWqFJGOZOLEybz33iLWHCphUlc3qdEhbWspImeo2mvyyUE/yckpnH/+hXaX0ykYltWyJWlfnEZ3ym9oGM3T8iJRWZm6+UnH8uijf2Lbti24+l6PEd0686YtbyX+3S/Sr19/7r//J63yPUWkY1m3bg1PP/04YzJczOmn0alw97t19dT6Tn4CFO82+PFojXBEgvn7mvjkoJ9bbvkqkyadb3c5Ec0wID29Fbv53XrrrS18YoPt27e39NuKSBsqKipk27YtGLHdWy1IARieZIz4vuzZk0d+/m769s1ute8tIh3DyJFj6Nq1GxtKirmwu5vkKI1OibSlOp/FmkMB0lLTmDDhPLvL6TRaHKZyc3Pbsg4RaQNLly4BwJE6stW/tyNtJIHafJYuXcxdd32r1b+/iEQ2h8PBZZfN5Nlnn2J5sY+ZWjsl0qY+OejDZ1pccul0nM4Wn+LLWWrx33RRURHdunXDMAyKioq+9NisrKyzLkxEzk5VVRWrV68CTwpGfO9W//5GTDeM6C6sX7+WsrJStV4VkROMHTuOd95+g89KDzOtu6XOfiJtpNFvsarET0JCIhMnTra7nE6lxWHqwgsvZMWKFaSlpXHhhRdiGAafX2519GtN8xMJDx99tJRAwI8zY+RZt0M/GcMwgqNTB5bw/vvvcv31N7f6c4hIZHM6nVxy6XReeul5VpX4uLiH9p0SaQtrSv00BiyuuPgy3G633eV0Ki0OU0uXLiU1NbX5cxEJX16vl4+XfQCuGIykgW32PEZCNrgT+OSTZcycOZvYWC1QFpHjTZgwiXnzXmfNoQamZblxOTQ6JdKaTMvi0xIfHo+HyZOn2l1Op9Pi1aDdu3dvvrrdvXt3unfvjtfrZfv27eTl5WGaZvPtImKvNWtWUV9XhyN5GIaj7eZNG4YDR8pwvN4mVq5c1mbPIyKRy+PxcN5551Prs9h6OGB3OSIdzs7KABVNFuPHn6d9pWwQ0llWcXExP/zhD1mzZg1JSUlYlkVNTQ0XXnghv/nNb0hOTm7lMkWkpSzL4v333wXDgSNlaJs/nyN5CGbZGj74YCkXXHAJDoc6donI8c4//wKWLl3MqhIf56RrYbxIa1pV4gdg6lTtK2WHkM56fvrTn+J0Olm6dCmffvopq1evZuHChVRUVPDzn/+8tWsUkTOwe/cuiosPYCT0x3DFtfnzGc4oHIk5HD5cxvbtW9r8+UQk8qSnZzB06Aj215oU12l0SqS1lDea7KoKMGBADllZPewup1MKKUytWbOGn/70p8dN6evTpw8///nP+fjjj1utOBE5cytWBF+D7TEqdZQjZQgAy5fr9S8iJ3d0LceGMr/NlYh0HEdfT9qg1z4hhans7Gx27tx5wu0FBQVaMyVio/r6etauXRNshx7Trd2e14jOwIjuwqbNG6iqqmq35xWRyDFkyDDi4uLYWB7A/Fw3YBEJjWVZbCz34/F4OOec0XaX02m1eOLy3Llzmz+fMGECDzzwANu2bWP48OE4nU527NjBv//9b+644462qFNEWmDjxrX4/T4cGYPapB36lzGSB2Me/Ih161ZzwQWXtOtzi0j4c7lcjB49jmXLPiC/2iQ7yWl3SSIRrbDOpLzRYty4MURFaVNsu7Q4TD3yyCPHfZ2SksKCBQtYsGBB820JCQm8/vrrfOtb32q9CkWkxdauXQOAI7F/uz+3I6Ef5sGPWbt2jcKUiJzUuHETWbbsAzaU+xWmRM7SxiNT/MaNm2hzJZ1bi8PU+++/35Z1iMhZqqurJTd3G0Z0JoYnsd2f33DFYsT1YM+ePCoqDpOSktruNYhIeOvXL5uUlFRyKyowLQtHO4+gi3QUlmWxvSJAXFwcOTmD7S6nUwu5P+nhw4fJz8/HNE0g+I/q9XrZtm0bX//611utQBFpmdzcbZimiSOhr201GPF9seoK2LZtixbDisgJDMNg2LBzWLbsAwprTXolaHRKJBSHGiwqvRbnnhNcbiP2CSlMvfLKK/zqV7/C7/djGAbWkYWkhmEwYsQIhSkRG+TmbgPAEdfLthoc8T0xS4K1KEyJyMkMGzacZcs+YEdlQGFKJEQ7KoNT/IYNG2FzJRJSN79//OMf3HPPPWzatIm0tDQ++OAD3nnnHQYPHswll2ithIgdcnO3gTMaotPtK8KdBO4EcnO3NV9kERH5vIEDB+NyudhRqf2mREK1szKAYRgMHjzM7lI6vZDC1KFDh5g9ezYej4ehQ4eyYcMG+vfvz09+8hNeffXV1q5RRE6jtraW8vIyjJjMdu/i93mGYWDEdKWurpbDh8ttq0NEwldUVBQDBuRQXG9S79NFF5Ez5TMt9tWa9OnTl/j4eLvL6fRCClOpqakcPnwYgH79+rF9+3YAMjMzKSkpab3qRKRFCgv3A8H9nuxmHBkZKyjYb3MlIhKusrMHAFBQp9EpkTN1oM7EtKBfvwF2lyKEGKauuOIKfvSjH7Fu3TqmTJnCG2+8weLFi/n73/9O7969W7tGETmNoqJCAIwoG6f4HXE00B2tSUTki/r06QdAQa1pcyUikefo66Zv32ybKxEIsQHF/fffT0JCAhUVFVx00UVcc801/OIXvyA5OZnf/e53rV2jiJzG0ZFiO1qif5HhDtZQUXHY5kpEJFwpTImErqAmOKLbr5/CVDgIKUy53W7uvffe5q+/973v8b3vfa/VihKRM1NZeSS4uMJg7rQrDlCYEpFTi42NJTOzK4VlJViWZetaT5FIU1hnkpycTHJyit2lCGexz9SaNWt46aWX2L17N263m+zsbG6//XYGD9bGYSLtrbq6GjCC3fxsZjhc4Iw6UpOIyMl169adkpKD1Pshzm13NSKRwRuwqPJaDOrX3e5S5IiQ1ky98MIL3HnnnXg8Hq699lpmzZqF3+/n+uuvZ/78+a1do4icht/vA4crfK7uGs5gTSIip9ClSxcAyho11U+kpQ43BTtgHn39iP1CGpl66qmn+N///V9mz5593O1jx47lT3/6EzNmzGiN2kSkhfz+ABghXRtpI45gTSIip5CRkQlAeaNJb23eK9Ii5UcuPnTpkmlzJXJUSGdftbW1DB8+/ITbx44d27wQXkTaUzju1RKONYlIuDh6MljeqPcKkZY6GqYyMjQyFS5CClNf+cpX+MMf/nDcmoimpiYeffRRrr/++lYrTkRaJjo6BgJeLCtMTkpMLzExMXZXISJhLCkpGYBabdwr0mJHXy9JSWo+ES5aPM3vwgsvbF6PYVkWRUVFnH/++fTs2ROHw8H+/ftpampSAwoRG8TExAIWWD4wPLbWYlkWmF6io2NtrUNEwltsbPA9ojGgMCXSUo1HZtAfff2I/Vocpu677762rENEzkJ8/JGW6P568Ngbpgg0Ap+rSUTkJI6OXjf4FaZEWuro60VhKny0OExdffXVJ9zW0NDAvn37ME2TXr166eRJxCZpaekAWL4aDE+yrbVYvuD036M1iYicjNPpIioqioaAOn+KtFSDP7gvW3S0ptKHi5C6+fl8Pv7whz/wn//8h0AggGVZuFwuZs2axS9/+Us8dl8ZF+lkmoOLtwbi7K0FhSkRaSG3240/4LW7DJGIEbDA6XTicIRTB9/OLaR/id///vd88MEHPP7446xZs4bVq1fz97//nc8++4w///nPrV2jiJzG0a5YlrfS3kIAqylYgzoNicjpmKaJM1z2xxOJAA4j+LqR8BHSyNQ777zDX//6V8aPH99829SpU4mKiuL+++/nRz/6UasVKCKn17VrNwAsr/1bE1jeCgC6dcuyuRIRCXdmwMQR0pmISOd0NExZltXcGE7sFdLIlGVZpKWlnXB7amoqdXV1Z12UiJyZmJhYkpJTsJoq7C4Fq+kwUVHRJCerbauIfLmAGcCh80GRFjt64h42W6FIaGFqwoQJPPzww9TW1jbfVl1dzZ/+9KfjRqtEpP1069oNfNVYpt+2GizLBG8l3bp10xUzEflSpmni9/tx6a1CpMVcR64+eL1NNlciR4U0uP6Tn/yE2267jSlTptC3b18A8vPz6dmzJ48//nirFigiLZOZ2Y3c3G3grYRom5o/+GrACpCZ2c2e5xeRiFFbW4NlWcS7tZBepKXi3cEwVV1drY5+YSKkMJWQkMA777zDxx9/zJ49e4iKiqJv375MmjRJ3UVEbNK1a1cguGbJsClMHV0vpTAlIqdTVVUFQIJHQ1MiLXX09VJdXdXcfErsFVKYmjlzJo8++igXXXQRF110UWvXJCIhSE0NrmO0fLWnObIN+YJrJk+2plJE5POqqysBSHArTIm01NHXS1VVpb2FSLOQhpEcDgc+nzbZEwknSUlHGj747WsCYx157uZaREROoaIiOJKtMCXSckdfL5WV9jeckqCQRqamTZvGHXfcwQUXXED37t1P2KT33nvvbZXiRKTlEhMTAbD89SF/D7OuEPPwRhyp5+CI63Hm3+DIcyclJYZcg4h0DsXFBwDIiNHyAJGWOvp6KSo6YHMlclRIYWrHjh0MHTqUQ4cOcejQoePuUwcvEXs4nc6z/h5m2Rqs+iJM0xtSmLIItmp1OM6+FhHp2IqKDmAAXRSmRFosLdrA5VCYCidnFKbeeust3n33XdLT07nooouYOXNmW9UlImfINM9+zwnL9B3355kyMLDQ/hci8uUsy+JAYQGp0QYepy7CirSUwzDoEu2guPgApmmq8VsYaPG/wLPPPstPfvITGhsbaWho4Mc//jF/+tOf2rI2ETkDPp83+Ilh46iQEXxL8Xq99tUgImGvurqK2rpaumpUSuSMZcY68Hq9lJYeOv3B0uZa/C720ksv8Zvf/Iann36af/zjH/zxj3/kxRdf1BVokTBx6FAJAIYnyb4ijjx3aWmJfTWISNjLy9sJQM94hSmRM3X0dXP0dST2avG7WEFBARMnTmz++sILL6ShoeGENVMiYo+SkmIADE+ybTUYnmAXv4MHi22rQUTC386duQD0TdT6SpEz1e/I6+bo60js1eIw5ff7cbmOLbFyuVxERUVpOo9ImNi9excARpQ9G/YGnzvtuFpERE5m585copwGWXEamRI5U+nRBglug507czVDLAzoXUykA2hqamLz5o0YnhSMqGTb6jDccRjRmeTmbqO21sbNg0UkbFVVVVJScpA+CQ4c6gAscsYMw6BvooOqqsrmKf5inzPq5rdw4ULi4+ObvzZNk3fffZfU1NTjjps9e3arFNfU1MQvf/lLlixZQnR0NHfeeSd33nnnlz6msLCQWbNm8Y9//IPx48e3Sh0i4W7z5o34fD4c6f3tLgUjsT/moRI2bPiMyZOn2V2OiISZzZs3ADAgSVP8REI1IMnJpvIAmzdvIDPzcrvL6dRaHKaysrJ45plnjrstLS2NF1544bjbDMNotTD10EMPsWXLFp599lmKior40Y9+RFZWFpdffur/NA8++CD19aFvWioSaUzTZPHi+YCBI3Gg3eXgSOyPWbqSd99dzMSJk3E6Q9rOTkQ6qPXr1wIwJEVhSiRUg5JdOAwv69ev5eKLFabs1OKznPfff78t6zhBfX09r776Kk899RRDhw5l6NCh7Nq1ixdffPGUYWrevHnU1dW1a50idlu7djUHDhRgJA2ydYrfUYY7HkfyMEpLN7FixTLOP/8Cu0sSkTBRX1/Hjh3b6RnnIClKKw1EQhXrNuib4GB3/m4qKytITk6xu6ROK2zfyXJzc/H7/YwaNar5tjFjxrBx40ZM0zzh+IqKCv7whz/wq1/9qj3LFLFVY2Mjb731BhhOnBnj7C6nmSN9DDjczF8wj/p6XeAQkaBNmzZgmiZDUzUqJXK2hqYGx0Q2blxncyWdW9jOvyktLSUlJQWPx9N8W3p6Ok1NTVRWVp6wTuv//u//uPrqqxkwYMBZPa/WwkqksCyLl156jsOHy3CkjcFwJ9hdUjPDFYsjfSw1h1by/PPP8I1v3IuhF5dIp7dmzSoAhqSG7emHSMQYkuLk7b2wevUqpk27yO5yOpyWnraE7btZQ0PDcUEKaP76i+3YP/nkE9auXcs777xz1s+blhY+J6QiX+a9995j9epVGDHdcGSca3c5J3CkjsKqK2TjxvWsWvURs2bNsrskEbFRaWkpubnb6JPgIC06bCfGiESMBI+DgclOduTvprGxih49ethdUqcUtmHqZHtYHf06Ojq6+bbGxkZ+/vOf84tf/OK420NVXl6DWvZLuNu1awf//OczGM5onN0vxTDCb8qMYRg4sy4hkP8yL7zwAomJaQwdOtzuskTEJgsXLsGyLMZkuO0uRaTDGJPhYkdlgPnzF3PNNdfbXU6HYhgtG2QJ2zCVmZlJRUXFcZsFl5aWEh0dTWJiYvNxmzZtoqCggO985zvHPf7uu+9m9uzZZ7yGyrJQmJKwlpe3k0cf/Qv+gImz5xUY7vjTP8gmhisGR/fLCBTM4x//+Bv33PMdhgwZZndZItLOTNPkk0+WE+U0GKYpfiKtJifZSZzb4NNVK7jqqjnqoGuDsB1nHzx4MC6Xiw0bNjTftnbtWoYPH47DcazsESNGsGTJEubOndv8AfDrX/+a7373u+1ctUjb2r17F48++me8Ph/O7pfjiOtpd0mn5YjthrPHDPwBi3/842/k5m61uyQRaWe5udsoLy9jeKoTj1PrJ0Vai8thMDLNSU1tDRs3brC7nE4pbMNUTEwMs2fP5sEHH2TTpk289957PPPMM9x2221AcJSqsbGR6OhoevfufdwHBEe20tLS7PwRRFrVunVr+Nvf/nQsSCX0sbukFnPE9cDZcwb+gMljj/2VVatW2F2SiLSjDz98D4DxmbpqLtLaxmW6MYAPPnjX7lI6pbANUwA//vGPGTp0KLfffju//OUvue+++7j00ksBmDx5MgsWLLC5QpG2Z5omc+e+ytNPP47Xb+HsfgWOhL52l3XGHHE9cfacScBy8txz/+Tll18kEPDbXZaItLFDh0rYsmUTfRIcZMWF3/pOkUiXHh1sRLF79y4KCvbZXU6nY1iWVgh9XlmZGlBI+KitreVf/3qC7du3YnhScPa4AiOq7Tbm8+W/Ao2lEJ2Bu2/bLGS1vFUEChdiNZWTnT2Au+76FklJSW3yXCJiv1deeZEPP1zKTQOitF4qjP1uXT21vpOfAMW7DX48OradK5IzsavSz793NDFhwiRuu+1rdpfTIRgGpKefvgFFWI9MiXRWlmXx2Wef8qtfPRAMUvF9cfa5tk2DVHsxPEk4+1yDkTiA3bt38atfPcDKlcvQdR2Rjqe+vp6VK5eT7DEYnKJRKZG20j/JSUaMwWdrVlFVVWV3OZ2KwpRImDl8uJzHHvsrzzzzBLX1DTi6TAqOSDk9p39whDAcbpxZl+DoOpWGJj/PP/8vHnnkYQ4dKrG7NBFpRR9//AFNTU1M7OrGqY27RdqMYRhM6urGHwho7VQ7U5gSCRN+v5/331/Cr371U7Zu3YQR1wtX35twpo3E6IAnIYZh4EwZhqvfTRjxfdmxYzu//vXPWbRoPj6fz+7yROQseb1ePvjgXaJdBud20fQ+kbY2Kt1Fgttg2ccf0NDQYHc5nYbe3URsFggE+PTTT1iwYB6HD5cHN+LNugQjcUCHDFFfZLjjcfWcjlm9m0DJMubNe52PPlrKFVfM4rzzpjTvMycikWXVqhXU1FQzLctNlNqhi7Q5l8PgvK4uFhc0sHz5h1xyyRV2l9Qp6CxFxCamabJ27WreeectSktLwHDiSB2JI20Mhiva7vLanSMxGyOuB2b5eqoqNvHSS8+zZMlCZsy4knHjJuJ0ar2FSKQIBAK8994iXA6Y2NVtdzkinca4Lm4+KvLz/tIlTJt2MW63Xn9tTWFKpJ35/X7Wr/+MRYvmU1x8AAwHjpThwRDljrO7PFsZziicXSbgSD0Hs3wdhys28/zzz7B48Xwuu2wGY8eO1y8GkQiwbt0ayspKGZ/pIt6tUSmR9hLtMhif6eKjoipWrVrBlCnT7C6pw1OYEmknlZUVLF/+EcuWfUhNTTVgYCQPwZk+BsOdaHN14cVwxeDMnIQjdSRm+VoOlW7l+eef4Y03XmHy5KlMmTKN1FRtyi0SjkzTZOHCt3EYcH43XfwQaW+Turr55KCfJYvnc955k3E6dbrflvS3K9KGLMtiz548PvxwKevXf4ZpmuCMwpE6CkfKMAyPQtSXMdxxOLuejyNtNGbFFuort7F48XyWLFnAiBGjmDbtIgYOHNQp1paJRIpNm9Zz8GAxozNcJEepz5VIe4tzG5zbxcknB8tZs+ZTJkyYZHdJHZrClEgbqK6uYu3a1axcuZzCwoLgjVHpOFOHBxtLOHS19kwY7nicXSZgpZ+LVZOHeXgTGzeuY+PGdXTrlsXEiZMZO3Y8ycmRvw+XSCSzLItFi97BAKZqVErENpO7uvm0xM/ixfMZN24iDocubLQVhSmRVtLY2MDGjetZs2YV23O3YZkmGA6MxP44UkZgxHTVCMpZMhxOjKQcHEk5mA0lmBWbKT6YxxtvvMIbb75KzsBBnHvuBEaNGkNMTKzd5Yp0Otu3b2X//n2MSHOSHqOTNxG7JEU5GJXu4rOSg2zcuI5Ro8baXVKHpTAlchYCAT/btm1hzZpVbNy4vnl/JCOmK47EgTgS+2O4YmyusmNyxGTiiMnEypyMVb0bs3onO3ZsZ8eO7bz00vMMH34O5547gaFDR6hphUg7WbToHQCmZnWcTcZFItX5WW7WlvpZtOgdRo4cowu6bURhSuQMNTU1sX37VjZv3sDGTRuor6sN3uFJwZExEEfiQK2FakeGMxojZSiOlKFYvhrM6l0Eqnayfv1a1q9fS0xMLMOHn8OIESMZMmQY0dEKtyJtIS9vJ3l5OxmU7KRrrEalROyWFu1gRJqTjQX72bZtC0OHDre7pA5JYUqkBaqqKtm8eSObNq0nN3cbfr8/eIcrLrg3VNJAiErXVR+bGe4EnGmjIW00VmM5ZvVOGqp3sXr1SlavXonT6SInZxDDh49kxIiRpKSk2l2ySIexePECAKZmaSRYJFycn+VhY3kDixa9ozDVRhSmRE7CsiyKiw+wadMGNm5cz759+cfujErHkdwHR3w/iFaACldGdBrO6Ik4MiZAUzlmTT5m7V62bdvCtm1bePnlF+jZszcjRoxkxIhR9OjRU/+WIiE6cKCQrVs30TfBQa8EbbAtEi66xjoYlOwkd/cudu/OIzu7v90ldTgKUyJH1NbWsmPHNnJzt7Ft2xYqKg4H7zAcGHE9MeL74Ejoi+FOsLdQOSOGYUB0Os7odMg4F8tXi1m7F6smn4LCQgoK9jF//lskJaUwePAQBg8eyqBBQ0hI0FRNkZZaunQxAFM0KiUSds7PcpNbGWDp0sUKU21AYUo6Lb/fz549u9m+fQu5udvYt38vWFbwTmc0RuKAYHiK64XhjLK1Vmk9hjseZ8owSBmGFfBi1RVg1uZTVVfAqlUrWLVqBQA9e/Y6EqyGkp09QE0sRE6hqqqSNWtW0SXGwcAkjUqJhJte8Q56xDnYuHEdpaWHyMjoYndJHYrClHQalmVx8GAxublb2b59Kzt37sDrbQreaTgxYrIw4nrhiOup6XudhOH0YCRm40jMxrKs4HTAugKsugIKCg9QULCfJUsW4na7GTAgpzlcZWV11/8PkSM+/HApgUCASV09el2IhCHDMJjczc1LeU28//673HDDLXaX1KEoTEmHZVkWhw6VsHNnLrt25bJz5w6qq6uOHRCViiN1UHAKX2yWNtLt5I6bDpg2Csv0Y9UXY9UV4Kvb37zWCiA+PoGBA3MYMGAQAwfm0LVrlk4ipVPyer0sW/YBcW6Dc9J1SiESroakOkmOMlj5yTKuvPJq7cXYivTOJx2GZVmUlh5i587t7Ny5g507c48PT644jMSBOOJ6YsT1wHDH21eshD3D4cKI7wnxPXFyHpa/PjglsK6Q2vpC1q37jHXrPgOOhqtBDBw4iAEDcujatZvClXQK69evpb6+nmlZbtwO/Z8XCVdOw2BCFxeLCrysXr2KqVMvtLukDkNhSiLW0ZGnvLyd7NyZy86duVRVVR474Gh4iu2OEZcF7iSd4ErIDFcsRlIOjqQcACxvNVb9Acz6A9TWH2DdujWsW7cGgISExOZgpXAlHdmKFR9hAGO76HRCJNyNynDzbqGP5cs/4vzzL9DvpVaidz+JGKZpUlRUSF7eLvLydrBr105qaqqPHeCKDTaNiO2OEddd4UnalOFJxPAk4kgeHFxv5TsaroqoqTvA2rWrWbt2NRAcuerffyD9+w+gf/8cevToicOhTU0lsh08WExe3k4GJDlJidL/Z5FwF+82GJziZMuBAvbt20ufPn3tLqlDUJiSsBUI+Nm/fx+7du0kL28nu3fvoqGh/tgBrniMxAEYsVk4YnuAR+FJ7GEYRvD/nycJR/KQY+Gq7gBmQxG19UVs2LCWDRvWAhAdHU129gCyswcyYMBAevXqo26BEnFWrPgYgHM1KiUSMc7t4mbL4QArVnykMNVK9A4oYcPrbSI/fw95ecHwtGfPbnw+77EDPMkYyX1wxGZhxHQDd4LCUyux/I2YFRuhqSJ4g68Oy9+I4Yq2t7AIdVy4ShkCgOWrCTa0qC+isb6IrVs3s3XrZgBcLjd9+/ajf/9guOrbtz9RUWrHL+HLsizWrVtNjMtgULLaoYtEin6JDpI8BhvWr+XGG2/F6dTr92wpTIltGhrq2b07r3nK3v79ewkEAscOiErHkdIt2GkvNgvDpc4zbcEKePHvewO8FcduDNTj3/cGrj7XYjg99hXXgRjuBIykBEgaiBOw/A3BcNVQRKC+iF27drJr1w4WLgSHw0HPnr0ZMCCneXpgbGyc3T+CSLN9+/ZSUVHB6AwXTjWeEIkYDsNgSIqTlSV17Nq1g0GDhthdUsRTmJJ2U1NTTV7eTnbt2snu3TspKCw4tkmu4cCI7tI86mTEdtNGue3ELPvs+CB1lLcCs+wznJnntX9RnYDhisFI7AeJ/YBgqLUaiptHr/bt38++ffm8994iMAy6Z3U/EqyCH0lJyfb+ANKpbdgQ7GQ5NEVXtUUizdBUFytL/GzYsFZhqhUoTEmbqaqqPLLHU3DkqaSk+Nidhiu4Se7RUaeYTO3zZBOr/kBI90nrMpwejPjeEN8bILjPVUPJkYBVxIHigxw4UMhHH70PQEZGJgMGDGTAgBwGDhxESkqqneVLJ2JZFhvWr8XjNMhOUpjqKJI8BrU+65T3ScfRO8FBnNtg44Z1XH/9LWqIdJYUpqTVVFdXsWvXjuY25SUlB4/d6fBgxPX+XHjKwDD0SzgcWL6akO6TtmU4XMGulHHdAbCsAFZjGVZ9EVZ9EaWHiyn9ZBmffLIMgIyMLs17XQ0cOEgjV9JmDh8u51DpIYamOLW3VAfSL9HJgTrzlPdJx+Ewgmsd15ZWcfBgEVlZPewuKaIpTEnIamtrmsPTjh25HDxYdOxOhwcjvg9GbHccsd0hOg3D0JUPkVAZhhMjJhNiMiFtVLBjYFM5Zv0BrLoDlB4uonTFx80d1jIzux7Z62oQAwfmkJiYZPNPIB1Ffv5uAHon6AS7I7mgu5sdlX4ONRw/OtUlxuCC7po50tH0inewthT27NmtMHWWFKakxbxeL3l5O9m2bQu5udsoKio8dqfDjRHXCyOuO0ZsD4zodIUnkTZkGAZEp+OMTofUc7AsMxiu6g5g1R+gpLSIkpIPWbbsQwC6ds1i0KDBDB48jIEDB6lboIRsz55gmOqVoPf4jiTKaXD3kBg+OehjeZEPnwXxLrh7SAxRTo1AdjS94oMXQ/LzdzN58lSbq4lsClNySpZlcfBgEdu2bWXbts3s2rUTv98XvNPhwojriXFkg1wjWtP2ROxkGA6IzsAZnQFpI7Es88i0wEKsugMcPFTMwYNFfPjhUpxOF/37D2DIkGEMHjyM7t17aJsBabH8/N04DegWqzDV0cS6DC7u4WFHRYCiepNEj4NYl94bOqL0GINop8GePXl2lxLxFKbkOPX1deTmbmPbti1s27aFysrPdXmLSseR1Cs4AhXbVeFJJIwZhgMjpgvEdIG00cE1Vw0lWLX7MesK2LFjOzt2bOfNN18lMTGJIUOGMWTIMAYNGkp8fLzd5UuYMk2TwsL9dIt14NJ6KZGI5TAMuscZ7C45iNfbhMej2QqhUpgSysvL2LBhLevXryU/f3dwLQaAMwYjcSCO+F7BUSjt8yQSsQzDiRGbBbFZwITgPld1BZh1BVTX7WfVqhWsWrUCDIM+vfsycuQYRo4cTZcumXaXLmGkrq6WQCBASpQupolEuuQoB2BSVVVFRkYXu8uJWApTndTBg8XNAaqgYN+RWw2M2G444nrhiOsF0ema+iPSQRmuGIykgTiSBh5rZlG3H6u2gL379rJ37x7mzn2V7t17MHLkGEaNGkO3bt31ntDJVVZWApCgVtkiES/BHXwdV1VVKkydBYWpTsKyLAoLC5oDVHPnPcOJEd8bR0I2RnxfDFe0vYWKSLs7rplF2misQBNW7V7M6t0cKNrPgQNvMX/+W3TpktkcrHr16qNg1QlVV1cCx07CRCRyJXqOhSkJncJUB1dbW8unn37C8uUfHds01+HCSMjGkdAv2L7c6bG3SBEJK4YzCiMpB0dSDpbpDa6zqtnDobK9LFmygCVLFpCensHkyVOZOHEyCQmJdpcs7aS6ugpQmBLpCOKPvI6Pvq4lNApTHZBlWeTl7WT58o9Yt+4zAgF/cAQqcQCOhP4Y8T0xHNozQkROz3B4MBL740jsj2X6seoKMWt2U3Z4N3Pnvsbbb7/JOeeMYvLkaQwcOAiHQx3eOrKjW15YpzlORCKHtrI5OwpTHUhdXS2rVh0/CmV4UnCkD8WRlIPh1BQ+EQmd4XBhJPTBkdAHKzAFs3ongYotrFv3GevWfUZGRhcmTZrKxImTNFrVQR3t+OU1bS5ERM6aNxC8LBIVpRlKZ0NhqgPweptYunQJS5YspKmpMTgKlZSDI3koRkxXrWsQkVZnOD04U4bhSB6K1XgIs2IrpeW7mDv3VebPf4uLL76Uiy++gpiYGLtLlVZ0dLNnX0BjUyKR7uhFEbVFPzsKUxEsEAiwatUK3nlnLlVVlRiuWBxdJuFIHqRRKBFpF4ZhYMRk4ojJxApMwqzaib98LQsXvsOyZR8xY8aVTJ48FadTv246gqNhqkkjUyIRz6cw1Sr02y1CbdmyiTfeeCXYlc/hxpF+Lo60kRgODdWKiD0MZxTO1OFYyYMwD2+itnwdL7/8Iu+//x6zZ1/LqFFj7C5RzlJKSioA5Y1KUyKRruzI6/jo61pCozAVYUzT5O2332Tx4vmAgSN5KI6MczFccXaXJiICgOFw40wfgyN5CGbZZ5SWbeGpp/7OtGkXc+21N6pJRQRLTU0jLjaOorp6u0sRkbN0oM7E7XbTrVuW3aVENP1GiyA+n49///spFi+ej+FJxtXvRpzdpilIiUhYMlwxOLtOwdXvJoyoND788D2efPLveL1NdpcmITIMg569+lDWaNHo17opkUjlNy1K6k169OiF0+m0u5yIpjAVIerr6/nb3/7IZ599ihGbhbPPNRhRGpYVkfBneJJx9p6DEdeTTZvW8+c/P0RNTbXdZUmIevfuA0BRnab6iUSqkgaTgAW9evWxu5SIp2l+EeLtt98gL28nRuIAnN0uwnDoKoK0nt/85jcnvf2Bn/+6nSuRjspwenD2nEGg+CP27dvOG2+8wu2332V3WRKCfv36A7Cj0k+/JP0uEolEOyoCAPTrl21zJZFPI1MRoKqqkhUrPgZPIs6sixWkRCQiGYYTZ7cLMKLSWL1mFWVlh+wuSUIwePBQ4uPi2VAeIGBpqp9IpLEsi/VlfqKiohgxYpTd5UQ8jUxFgPfeW4Tf78fZbYx2qZY28cADD5z8Dqf2CJLWZRgGjvSxBA4sZvHiBdxyy1ftLknOkMvlYuy5E/jww/fIqwqQk6xTCZFIsq/W5HCTxcSJ5zZvdyChC+sz86amJn7yk58wduxYJk+ezDPPPHPKYz/88EOuuuoqRo0axaxZs1i6dGk7Vtq2du3aAYYDIynH7lJERM6akZANjqjge5tEpAkTJgGwvtRvcyUicqaOvm7Hj59kcyUdQ1iHqYceeogtW7bw7LPP8otf/IJHH32URYsWnXBcbm4u9957L9dccw1z587lxhtv5Lvf/S65ubk2VN36kpNTwTLB9NldiojI2bNMMJu0t0kE69mzF1lZ3dlWEeCw9pwSiRjVXpON5QHSUtPo33+A3eV0CGEbpurr63n11Vd54IEHGDp0KJdccgl33XUXL7744gnHvvPOO0yYMIHbbruN3r17c8sttzB+/HgWLlxoQ+WtLyOjCwCWt9LeQkREWoOvCjj23iaRxzAMpk+/ioAF7xZ67S5HRFpoaaEPn2lxxfQrtedfKwnbv8Xc3Fz8fj+jRh1bGDdmzBg2btyIaR5/Fezqq6/m/vvvP+F71NTUtHmd7eFopxXz0KdYWuwrIhHMsiwCh1YB0LdvP5urkbMxatQY+vbtx6byAIW1AbvLEZHTOFRvsrbUT1a37s1TdeXshW2YKi0tJSUlBY/H03xbeno6TU1NVFZWHndsdnY2gwYNav56165drFy5kokTJ57x8xpG+H2MGjWGc84ZhVVfiFm+PuS/UxERu5kVW7Bq8xk0aAgTJkyy/f1VH6F/OBwGc+ZcD8Ci/V5d7BMJc4sKvFjA1XOuw+l02P4eEgkfLRG2LXgaGhqOC1JA89de76mnFBw+fJj77ruP0aNHc9FFF53x86alJZzxY9rDd797H/fffz+Hyz7FiErGkaAruiISWcza/ZiHVpCQkMj3v///SElJsrskOUvp6WM499xzWbNmDRvLA4xMD9vTCpFObXuFnx2VAYYPH87UqedhtDQpyGmF7bteVFTUCaHp6NfR0dEnfUxZWRl33HEHlmXxyCOPhDQXtLy8hnC9uHbHHV/nkUf+hK9wIVbaaBwZ41GrdBEJd5ZlYZavxSz9FJfLxVe/eheBgIuyso4xFbuzu/LKa9m8eTPz9jbRK95BarR+L4mEkyqvyRt7vLjdbq6++nrKy2vtLikiGEbLBlnC9h0vMzOTiooK/P5jbVdLS0uJjo4mMTHxhONLSkq45ZZb8Hq9PPfcc6SmhtYlyrLC9yM7eyA/+tHP6NIlE7N8HYH987D89SH/HYuItDUr0EigcD5m6aekpqZz//0/YfDgYba/n+qj9T7S0jK46abbaApYvJzXRMC07P5vJyJHmJbFa7ubqPdbXHfdzXTt2t3294xI+miJsA1TgwcPxuVysWHDhubb1q5dy/Dhw08Ycaqvr+euu+7C4XDwwgsvkJmZ2c7Vtp+srO786Ec/Z+TIMVj1Bwjkv4xZsRXLUmtaEQkflmViVuYS2PMyVu0+hgwZzo9//HN69epjd2nSBs49dwITJ06msM7kvUJt4yESLj4u8rGn2mTUqLFMmnS+3eV0SGEbpmJiYpg9ezYPPvggmzZt4r333uOZZ57htttuA4KjVI2NjQA88cQT7N+/n9///vfN95WWlnaYbn5fFBMTw913f4trrrkRtyNA4OCHBPa8hFmzBy0AFhE7WZaFWbuPQP4rBIqX4qSJWbOu5lvf+i5xcfF2lydt6LrrbqZLl0yWFfvYXqHNfEXstrsqwNIDPlJSUrnlltu1TqqNGFYYn303NDTw4IMPsmTJEuLj4/na177GV7/6VQBycnL43e9+x5w5c7j88svJz88/4fFXX301//d//3dGz1lWFr5rpk6mqqqKBQvmsXzFR1imiRHTFUeX83DEdrO7NIkQvp3PQKDh5Hc6Y3APvLN9C5KIZTYcwjz0CVb9AQzDYMKEScycOVub83YiBQX7+eMff4vl93HnoCh6JTjtLklC8PfNDRTVm2TFOvj28Bi7y5EQFNUFeHp7EwHDyXe/+0Oys/vbXVLEMQxITz/9mqmwDlN2iLQwdVRJyUHmzXuD9es/AwiGqpRhGAn9MRz6ZSanpjAlZ8OyAlg1+ZgVm7HqiwAYPnwkV111DVlZ3W2uTuywdetmHn/8r0Q5LL4xJJqMmLCdBCOnoDAV2Q43mjyxrZE6P9x117cYNWqM3SVFpJaGqbDt5idnJjOzK3ff/S3y83ezcOE7bNm6iUDRQQznCozkwThShmK4T2zcISISCstXi1m5FatyW3MjnMGDh3L55TMZMCDH5urETkOHDufWW+/k2Wef5t+5jXx9aDRJHgUqkfZQ57P4944man0WN954q4JUO1CY6mD69s3mW9/6LuXlZSxb9iErVnxMXfk6zPL1GPG9g6NVcT3VUl1EzphlmVj1B4JNb2r2ABYxMbGcN/VSpky5gC5dOm7zHzkz48efR3V1FW+++SrP7mjia4OiiXNrvYZIW2r0Wzy3o5HyRpMrrpjF+edfYHdJnYLCVAeVlpbO7NnXMmPGVaxf/xkfffQ++fm7CdTuBWc0joRsjMT+GLFZCladnOFOwDrFND/DHZ6bWEv7sSwLq6EYqzoPs2Y3HBmF6tmrN9OmXsiYMePweKJsrlLC0cUXX05VVRXvv7+Ep7c38tVBURqhEmkjdT6Lf+c2UlRvMmnS+cycOdvukjoNhakOzu12M27cRMaNm0hBwT5WrlzB+vWfUVW5FSq3gisGR/zRYNVNwaoTMmK7YzUeOuV90vkEA9RBrOo8rJrdWP46AOLjExg9+gImTJhEnz79bK5Swp1hGFxzzQ04nU7efXchT21r5M5B0drUV6SVVTWZ/Cu3idJGk/PPv4Drr79FnfvakRpQfEGkNqA4E6ZpsmdPHmvXrmHd+s+oqa4K3uGKDY5YJfTFiMlS44pOwgp48e99DbwVx98RlYqrzzUYDo89hUm7sqwAVv1BrNp8zOrd4K8FIC4unlGjxjJmzLkMGJBzwj5/Ii2xaNF85s17nQS3wR2DosmM1f+jcKYGFJGjvNHkmdxGKpssLrtsBldeOUdBqpWom1+IOkOY+jzTNMnL28natWtYv/4zamuP7M3lcGPEdg+us4rvreleHZwVaMQ8vBGzfANYfnDG4sq+CcMZbXdp0oYsXy1W3X7M2n1YdYVgegGIjY1j1KgxRwLUIJxOXViRs/fRR0t5+eUXiXUZ3J4TRY94/b8KVwpTkeFgvcm/chup9VnMnn0tl1463e6SOhSFqRB1tjD1eYFAgLy8nWzZspGtWzdz8GBx832GJwUjvnfwI7YbhqFfgh2RL/8VaCyF6Azcfa+3uxxpZZYVwGoowardh1m7D5rKm+/LyMhk6NDhDBs2gpycQTidmgUure/TTz/h+eefwYnFnH4eRqTp/1k4UpgKf9sr/Lyy24vPtLjhhlvVbKINqDW6nDGn00lOzmBycgZzzTU3Ul5extatm9m2bTO5udvxHt4AhzccGbXqgRHXE0dcd/CkaEhZJAxZlgXeKsz6Qqy6Qqy6gubRJ5fLTc7QEQwdOpyhQ4eTkdHF5mqlMxg//jwSEhL559OP83JeA4caTC7s7sah3yEiLWJZFh8X+3i3wIfb4+Hur93NyJFqf24njUx9QWcemfoyPp+P3bt3snXr5hNGrXDFBrsCxnbHEdsDPEkKVxFKI1ORzbIs8FVj1R3ArD+AVX8AjjSPgGOjT0OHDmfAgBw8Hq2HE3uUlBTz+GN/5VDpIYakOLk2O4oop35vhAuNTIUnn2nx5p4mNpYHSE1N5Z57vkuPHj3tLqvD0jS/EClMtUx5eRk7d25nx45cdu7MpbLyc80LXHEYsVk4YntgxHUHd6LCVYRQmIo8lrc6uPfT0fDkq22+LzExiZycwQwcOIiBAwdp9EnCSn19HU8//Ti5udvoGuvgKwOjSIlSY4pwoDAVfqq9Ji/ubKKwziQ7ewBf//q3SUhItLusDk1hKkQKU2fOsizKykrZuTMYrHbs2E710Q6BAO744KhVTDeM2G6aFhjGFKbC29Fpe1ZDEWZ98ZHwVNN8f3x8Ajk5gxkwIIecnEF06dJVrzUJa4FAgNdff5kPP3yPGJfBtf08DErRCgS7KUyFl91VAV7Z3UStz+K886Zw44234nLpddLWtGZK2o1hGGRkdCEjowuTJp2PZVkcOnSwedRq585caqt2EKjaEXyAMxojpmuwkUVMN4zoLmrDLnISlhXAaizDqi8ObpxbXwyf22A5Li6egcPGNI88de2apfAkEcXpdHL99TfTo0dPXn7peZ7f2cTkbiaX9nDjdOj/snRupmXx/gEfHx7wHXmt3MjUqRfqfT7MKExJqzMMg8zMbmRmduP88y9oDld5ebvYvTv4UVq6F6t275EHODFiMoPBKrZbMGg5o2z9GUTsYAW8wc1yjwQnq6Ek2Kr+iLS0dLKzR9K//wCysweQmdlN+z5Jh3DeeVPo06cvTz31GMuLD7KvJsAN/TXtTzqvaq/JK3lN5NeYpKenc9dd36JXrz52lyUnoWl+X6Bpfu2jqqqKPXuCwSovbxeFhfsxTfPYAVFpOJpHr7pq3VU70TS/9hNsFlFzJDwdxKwvPtKqPPgGZBgGPXr0JDt7QPNHcnKKvUWLtLGmpiZefvkFVq1aQYzLYE5fD0NSdd23vWman712Vfp5dY+XOp/F6NFjueWWrxITE2t3WZ2O1kyFSGHKHo2Njezdu6d55GrPnt14vU3HDnDFBkesjn5EZ2A49Au2tSlMtZ3glL1SrPqDzQHq85323G43ffpk079/f7KzB9K3bzYxMTqJkc5p5crlvPzSC3h9XiZkurispwePuv21G4Upe/hNi/cKfSwvDk7ru+bamzj//At0MdkmWjMlESU6OppBg4YwaNAQILgouaiokD178tizZze7d+dx+PAerJo9wQcYzmCgiul6ZHpgVwyXrtpI+LD8Dc2hyaovxmo8BFag+f6UlFT69RtKv37Z9OvXnx49emqjXJEjJk6cTJ8+ffnnP//BqqID5FWZXJftoUe81tdKx3Sw3uSV3U2U1Jt06ZLJ1752Dz179ra7LGkBjUx9gUamwldlZQV79uw+ErDyKCjYRyBw7OQUd2LztEBHTDeIStXVnDOkkanQBLvsVWDVH8RsCDaLwHuso6XD4aBHj17069e/+SM1NdXGikUig8/n4+2332Tp0kUYwLQsN9Oy1JyirWlkqv2YlsXyYh/vFfoIWDB16kVcffW1eDxaO243TfMLkcJU5PB6vezfv/dIwNrF7t151NUd22MHh+dzXQO7BptcONz2FRwBFKZaxjL9WI2HjnXZayiBQGPz/TGxsfTr25/s7P706zeA3r37EBWlX4wiodq1awfPPvsUhw8fpnucg+uyo8iIUXOKtqIw1T4ON5q8vqeJvTUmSUlJ3Hrr1xgyZJjdZckRClMhUpiKXMGugSXNwWr37jxKSoo/d4RxbGpg7JHpge542+oNRwpTJ2f5646sdSoO/tlUCtaxhikZGZlkZ/cnO3sA/fr1JzOzq7rsibSyhoYGXnvtv6xcuRy3w+DSnm4mZLpwaAZCq1OYaluWZfFZqZ8F+314AxZjx47jhhu+QlyczknCicJUiBSmOpba2lry84PBas+ePPbuzcfv9x07wJ2IEZuFIzYLIzar03cNVJgKsrzVWPVFmPVFWA1Fx03Zczpd9O7dh379joanbO1CL9KONmxYx39e/De1dbX0TnAwp28U6RqlalUKU22nosnkzT1N7K42iYmJ4cYbb+XccyfYXZachBpQiADx8fEMHz6S4cNHAuD3+yko2MeePXnk5e0kL28XdVW5BKpygw9wxWEcCVaO2G7g0bqrji643qnyc+GpGHw1zffHxMbSP2fkkfbk/enVqw9ut6aLithl5MjRZGf359VX/8Nnn63mb1sauLi7m0nd3BqlkrBlWharS/wsLvDhNS1GjBjFTTfdSlJSst2lyVnSyNQXaGSqczFNk5KSYnbt2kle3k527dpBVVXlsQOc0Ue6BWbhiOsOUekdOlx1hpEpy7KgqTwYnI58EGhovj8hIZEBA3IYMGAg/fvn0K1blqbsiYSpDRvW8dJLz1FdXU2POAdz+kWRGavX69nSyFTrKmsMjkbtrTGJi4vjhhu+wpgx4zr0+URHoJEpkRZwOBx069adbt26c/75F2BZFmVlpeTl7WgOWGVl+Vi1+ZgQDFexPXDE9cCI64nh0fSuSGD5arDqCjDrCrHqCo8LT6mpafTvP5IBA3Lo338gXbpk6hecSIQYOXI0AwYM5PXXX2bVqhX8fUsDF3R3M6WbG5c6/onNApbFyoN+3i304jdhzJhxXH/9zZoa3sFoZOoLNDIlX1RRcZi8vJ3s3JnL9u1bOXy4/Nid7sRjwSq2B4Yr2r5CW0FHGZmyAo1YdUVY9cEAhbey+b7k5BQGDx7KwIGD6N9/IGlp6fYVKiKtZuvWTbz44rNUVlaQGePg6n4eempfqpBoZOrsFdUFeDPfS1GdSUJCAjfddBsjR46xuyw5A2pAESKFKfkyR0eucnO3BT92bKOhvv7YAdEZR8JVr2BLdiOyfpFHapiyLDO4OW7tfqy6AqzGUiD4Qo6OjiEnZ9CRTaGHauRJpANraGjgrbdeY9myD8CCCZkuLunpIcqp1/yZUJgKnTdg8f4BHysO+jCt4AbUc+Zcr059EUhhKkQKU3ImTNOksHB/c7jKy9uJ3+8P3umMwojrhSO+L0Z8Lwxn+O8zFElhygp4ser2Y9buxard17zPk9PppF+/AQwaNJhBg4bQq1cfnM7ICrUicnZ2787jxRf/xcGDxSR5DK7s42FQilY2tJTCVGjyqgK8ld/E4SaL9PQMbrnlq+TkDLa7LAmRwlSIFKbkbHi9Xnbv3sWWLRvZtGkD5eVlwTsMR7CRRUJfHPF9MDxJ9hZ6CuEepixfDWbNXqza/GDjCCsAQEpKKsOHn8Pw4efQv3+ONsgVEXw+H0uWLGDxonfwBwIMT3Uys08U8W6NUp2OwtSZqfdZLNjvZX2ZH4fDwcUXX8706Vfi8XjsLk3OgsJUiBSmpLVYlkVx8QE2bdrI5s3ryd+bz9H/XEZUKkZ8XxyJAzCi02yu9JhwDFNWUwVm9U7MmnxoOrZerVevPowYEWx736NHT03dE5GTKi4u4sUX/82ePXlEuwwu7+lmTIY2+/0yClMtY1kWG8sDzN/npd5v0bt3H2655av06NHL7tKkFShMhUhhStpKdXUVW7ZsYvPmDWzbthWfzxu8IyoNR9LAYLByn/5F25bCJUxZvjrM6l2Y1TuD9QAul5tBg4YwYsRIhg0bQXJyim31iUhkMU2TFSs+Zu6br9LQ2EDvBAez+0bRRZv9npTC1OmVN5q8lR/cfNfj8XDllXOYNu1ibaXRgShMhUhhStqDz+dj69ZNrFmzis2bNzavszJis3AkDsRIzMZwtn9nQDvDlBVowqrZg1m1E6u+EAiufxo6dPj/396dR0dV5mkcf25lh0AICQHClhC2QFgCYbODsomyyaKAIJuAGza0io6idovOeGYOap+eo+ihWxFQ2tZoRFmkXcBu7JkRDbLJrglhJ5CErLXeO39E0o2AQEFSVanv5x+q7n3r3l9ySFJPve/9XfXu3U9pad1Zvgfgmpw9W6ysrHe0des3CjGkGxPDdFNimMJoo34ewtSluU1LXx13adMxl9ym1K1bD02cOFWNGzf2dWm4zghTXiJMobZVVFRo27YcffPN/2nf/r1VSwENm4zoJNli06partfScpTaDlOWZcmqPC6zcKesstzqa6BSUtqrT5/+6tkzgw5IAK67Xbt26C9/WanCwkLFRRoakxShlBga1ZxDmLq4Q6UefZTr1MlKUzExMZo4cap69OjJMvM6ijDlJcIUfKm4uEjffvu1tnzzfzpyOF+SZETEyojtJltMRxm2sBo9f22FKct0yyo5IE/hDslR1aSjefMW6tOnnzIy+nLvJwA1zuFwaN26j7Rx46cyTVPp8aEa3jpc9WlQQZj6mUq3pU8PO7XllFuGYejGGwfrttvGKSqqnq9LQw0iTHmJMAV/kZ+fpy+//ELffPO1PB63FBIhW0yqbLFdZYTXzN3TazpMWa4ymUW7ZBZ/L3nsstls6tmztwYNGqqkpLZ8ugeg1h0+nK9Vq5YrPz9P9UINjWgdph7xoUH9++j13ZXKLTWV3MCmOZ2DN0xZlqXvCz1ae8ipUpelxMQWuuuumUpOTvF1aagFhCkvEabgb0pKzuqrr/6mv/99k0pKzkoyZDRIUUiTPjIirm8ThpoKU5azRJ7TW2Sd3S/JUnR0Aw0YMFADBgykkQQAnzNNU19++YXWrMmWw+FQSkObxiRHKC4yOJsJ/HjWo69OuJTZLExtg3T5Y7HD1Md5Tu0r9igsLEwjR47RkCHDFBLC/cqCBWHKS4Qp+Cu3263vvvtWGzd+pkOHciUZMhqlKiS+t4yw63Nd0fUOU5a7Qubpb6tmoixTLVq00pAhw9SrVx+FhdXskkUAuFqFhYV67723tWPHNoXapMGJYcpsHqYQGlQEDdOy9L8n3Pr8iEtO01JqahdNnjxN8fEJvi4NtYww5SXCFPydZVnauXO7PvrofR0/fkwyQmRr3E22uJ7X3AHweoUpy+OUWfidzMLtkulSQkJT3Xbb7UpP7xXUS2cA+D/LsrRt21a99+7bOltyVk3r2TQuOVytooNzhiaYHCv3aHWuU0fLTUVHR+uOOyard+9+/N0KUoQpLxGmEChM09SWLf+rtWtXq7DwjBQSoZCEX8mI6eT1L/7rEabMkgMyT/xdlseumJhGGjlyjPr3z1RICG9EAASOysoKrV79gTZv3iRDUv9mobq5ZbjCQ3hjXde4TEubjrq0+bhLpiX175+pceMmKjqabrLBjDDlJcIUAo3L5dLmzV9qzdoP5bDbZUQnKaTZQBlh9a/+WNcQpix3pTwn/iar9AeFh0doxIjbNHDgEIWHh191HQDgL3744YDefvtNnTx5QrERhsYkhat9I66bqStySzxanevQabul+Ph4TZkyU506dfZ1WfADhCkvEaYQqAoLC7Vq1XLt2bNLRkiEbE0HyGjY4apmqbwNU2bJDzJP/k2Wu1Lt23fUtGl3s74cQJ3hcrm0YcMa/fWv66vbqI9oHa56tFEPWHa3pb/+S7vzIUNu0ahRYxQezs3hUYUw5SXCFAKZZVn6n//ZrPff/4scDruMmFSFNL9JhnFlS+yuNkxZlinz5D9kFu1QWFiYxo6doJtuGiybLTg7YAGo244cOaxVq97UoUN5qh9maHSbcHWNY5Yq0OwpcuvjPKdKnJZatGipqVPvVps2yb4uC36GMOUlwhTqgsLCM/rTn17VoUO5Muq1VEjLW2WEXP7TtqsJU5bpkufop7LK8pTYoqXuvedBJSQ0vV5fAgD4JdM0tWnT51rzcbacLqfSGododFKEopml8nuVbktrDzm17bRboaGhGjFijG6++RbaneOiCFNeIkyhrnA6HVq+/HVt25YjIyJWIa1GyQj75Zv9XmmYslzl8hxZJ8teoM6d0zRnzgOKjAzeGzsCCD6nT5/SW2+9qQMH9qleqKHbkpil8md7itz6KLfq5rvJyW01bdpsNWvW3NdlwY8RprxEmEJdYpqmPvwwS1988VcZYQ0U0ub2X2xMcSVhynLb5Tn0gSxnsQYMGKiJE++iUx+AoGSapjZv3qQPs7OYpfJTP5+NGj16vIYMGcZydFwWYcpLhCnURRs2rNXHH2fLiIhXSJtxMkIu3mHvcmHKMt3y5H8kq/KEhg0brjFj7uD+GwCCHrNU/onZKFwLwpSXCFOoiyzL0nvvrdLf/rZRRv1WCmk18qJNKX4pTFmWKc+RDbLKctW37w2aPn02QQoAfvLzWarucVWzVFGh/J6sbQ6PpfWHnPq2gNkoeO9KwxQfmwBBwDAMTZgwRcXFxdq+favMgm8UktDvqo5hntkmqyxXqaldNHXqTIIUAPwLm82mm24aoi5dumr58te1/ceDyiut1B0pEWrbkKXQtSW/1KOsHxwqdFhq3TpJM2few2wUapRfR3SHw6Enn3xSGRkZyszM1LJlyy45dvfu3ZowYYK6d++u22+/Xbt27arFSgH/Z7PZNGPGHMXFNZF5ZqvMypNX/FrLfkbm6S2KiWmk2bPvp/MRAFxCfHyCHnnkCd1223iVug0t22PXhnyn3CbLXmqSx7T0xRGn/rTHriKnNHz4KD322JMEKdQ4vw5Tixcv1q5du7RixQo988wzeuWVV7Rhw4YLxlVUVOjee+9VRkaGsrOzlZ6ervvuu08VFRU+qBrwX5GRkZoxY7YkyTz2hSzTfdnXWJYpz/EvJMujqVNnql69SzewAABUfXh1662j9NhjTykhoak2H3fpte/tOllh+rq0Oum03dQfd9u18ahLsY3j9cgjT2j06PF88Ida4bdhqqKiQllZWXrqqafUpUsX3XzzzZozZ45WrVp1wdj169crIiJC//Zv/6aUlBQ99dRTql+//kWDFxDs2rXroEGDhshyFsksuvwMrlW8V5a9QP37Z6pLl261UCEA1A1t2iRr4ZOLdOONg3SiwtSr31fq65Mucbn69fNdgUtLdtp1pNxU//6ZevLJZ5WS0t7XZSGI+G2Y2rt3r9xut9LT06u39erVS9u3b5dpnv/Jzvbt29WrV6/qazgMw1DPnj21bdu22iwZCBgjR45RZGSUrMKtskzXJcdZlkfmmW8VFham2267vRYrBIC6ITw8QnfeOU1z5z6kyKhofZzn1LsHHbK7CVTXwumx9MGPDr3/o1MhEZG65565mjZtlqKiuOchapffzn8WFBQoNjZW4eH/bOEcHx8vh8Oh4uJiNW7c+Lyx7dq1O+/1cXFxOnDgwFWfl2vqEQzq16+voUNv0dq1q2UW7VRIXE9JkmELk/XTv5JkFu+R5SrVTUNvVaNGMT6sGAACW9eu3fTU08/qjTeWaufB/TpWYded7cKVWJ/mFFfrVIWpdw46dKrSVFJSsubMeUBxcfG+Lgt1zJVmAr8NU5WVlecFKUnVz51O5xWN/fm4KxEXd/kWiEBdMGHCOH3xxV9lL9opq3EPGYZNtvjeMgu3y9a4uyzLklW0Q6GhYbrzzjsUE8PPBgBci/j4BvqP/3hOWVlZys7O1tLddo1oHa4+CaF0SL1C3xW49FGeSy7T0qhRozRlyhSFhYX5uiwEMb8NUxEREReEoXPPIyMjr2jsz8ddiTNnuM8UgkdGRl9t3vylrPKjMqJbyVa/pWz1W0qSzMoTshxFSu/dTy6XTadPl/q0VgCoK4YOHanExDZ6880/6uO8MuWVejQuOULhIQSqS3GZltbkOZVT4Fa9evU0e/psde+errNn7ZLsvi4PdZBhXNkki9+GqaZNm6qoqEhud9UN16Sq5XyRkZFq2LDhBWNPnz593rbTp08rISHhqs9rWSJMIWj07z9Amzd/KfPsHtmiW523zyreI0m64YYB/EwAwHWWmpqmJ598VsuWLdWOg/t1qtKuqR0iFBvht5ez+0yJ09Sf9zt0uNxUmzb/XNbH3yb4A7/9iU1NTVVoaOh5TSRycnLUtWvXC+5g3b17d3333XfV3XEsy9LWrVvVvXv32iwZCDht2iSpadNmssryZFme6u2WZcksy1VMTKzat+/owwoBoO5q1ChWv/nNoxo4cEhVt79ddv1Y4rn8C4NIfqlHr+6y6/BP3foeeeQJro+CX/HbMBUVFaWxY8dq0aJF2rFjhz7//HMtW7ZM06dPl1Q1S2W3V03r3nrrrSopKdHzzz+vgwcP6vnnn1dlZaWGDx/uyy8B8HuGYSgtrZtkumRVHKvebtlPSe5KpaVd+OEFAOD6CQkJ1cSJd2nq1LvltGx6c69d/3uC9umSlFPg0ut77Cr3GNXfI66Pgr/x63dJCxcuVJcuXTRjxgw9++yzmjdvnoYNGyZJyszM1Pr16yVJ0dHRWrp0qXJycjR+/Hht375df/zjH1WvXj1flg8EhLS0qhlcq+xQ9bZzj8/tAwDUrBtuGKCHHn5c0Q0aau0hpz7MdcptBmeg8liW1uY5lP2jU5H16mvevAUaOHAITTrglwyLjz7Oc/o0DSgQXFwulxYseFCe0MYKTZ4gSXIf+khW5VG99OLLioriQwkAqC3FxUVauvQVHTqUq5SGNk1pH6nI0OAJEU6PpXcPOrS32KMWLVrqvvvmKT6+ia/LQhAyjKoOnJfj1zNTAGpeWFiYWrVqI8txWpbplmWZsuwn1bx5IkEKAGpZo0axevjhx9W9e0/9UGLq9T12lThNX5dVK8pdlpbttWtvsUepqWlasGAhQQp+jzAFQMnJKZJlyrKflpxFkulSclJbX5cFAEEpPDxc99wzVzfdNETHK0wt3W3Xqcq6HagK7VVf5+EyU/36/Upz585XZGSUr8sCLoswBUAtW/7UFt1xRpb9jCSpVavWPqwIAIKbzWbTxIlTNHbsBBU7LP1xt12HSutmp7+j5R4t3W3XGbup4cNHa9q0WQoJ8du79wDnIUwBUGJiC0mS5SiU5SiUJDVv3sKXJQFA0DMMQ8OGDdfMmff81OnPoR/O1q1AlV/q0Rt7HCp3S5MnT9fo0eNoNIGAQpgCoGbNmkuSLGehLGeRJKl580RflgQA+EmfPv31wAO/kWwhWrnfroN1JFAdKvXozX0OuSxDc+bM1YABA31dEnDVCFMAFB4eoZiYRrKcpbKcJYqIiFR09OU72AAAakfnzml6YO5vZNhC9dZ+uw4Uu31d0jXJLfFo+V6HPLLpnnseVHp6L1+XBHiFMAVAkqruKO8ulVwliouLZ5kFAPiZTp26aO6DD8sICdPbBxzaF6CB6scSj1bsc8g0qoJU9+7pvi4J8BphCoAkqXHjOMkyJdNZ9RgA4Hc6dkzVgw8+LFtIuFbtD7xrqPJLPVq5zyHLsOne++apW7cevi4JuCaEKQCSpJiYRv/yOMZ3hQAAflGHDp1+mqEK1aoDDh0rD4xAVVBpauV+hzwydO9985SW1s3XJQHXjDAFQJLUsGHMRR8DAPxPhw4dNWvWfXKa0op9DhXa/fs+VGedppbvtavSbWnatFkEKdQZhCkAkqSGDRtWP27QoOEvjAQA+IMePXpp0qSpKnNZWr7PoXKX5euSLqrSbWnFPoeKnZbGjp2gvn1v8HVJwHVDmAIgSapXr1714/r16/uwEgDAlbrxxkEaPny0zthNrdhnl8v0r0DlMS2t2m/XyQpTgwffrJtvvtXXJQHXFWEKgCSpXr36//K43i+MBAD4k1Gjxqp//0wdLTf1ca5TluU/gWrDYadyS0317Jmh8eMn0SkWdQ5hCoAkKSoqqvpxZCRhCgAChWEYuvPOaWrTJllbT7u15ZR/tEzfdtqt/znhVmJiC02bNls2G287UffwvxqAJCkiIrL6cWRk5C+MBAD4m7CwMN1774OKjo7WukNO5Zf6tsPf8XKPVuc6FRUVpfvum6eIiAif1gPUFMIUAEk67w8df/QAIPDExjbWnDlzZRk2/fmgQ2U+akhR6bb05wNOuS1Ld999n5o0SfBJHUBtIEwBkFR1zVTHjqlKTk5Ro0axvi4HAOCFDh06ady4iSp1Wsr+0eGT66fW5DlU6DA1YsQYWqCjzgv1dQEA/IPNZtNvfvOYr8sAAFyjwYNv1u7dO7Vnz/fKKXArIyGs1s6964xb28941LZtOw0fPrrWzgv4CjNTAAAAdYhhGJo69W5FRUZpfb5LRY7auaFvqdPUR3lOhYeFa/p0Gk4gOPC/HAAAoI6JjW2siZOmyuGx9MEPDpk1vNzPsiytznWqwm1p/O2TlJDQtEbPB/gLwhQAAEAd1KdPP6Wn91Juqamcgpptl76r0KO9xR6lpnbRgAEDa/RcgD8hTAEAANRBhmFo4sS7FBkRoc+OuFTprpnZKafH0if5ToWFhmrKlBncmBdBhTAFAABQR8XENNLwEWNU7rK08aizRs7x9+MunXVaGnrzcMXFxdfIOQB/RZgCAACowwYNGqqEhKb6v5Nunaq4vs0oCu2mNh93KTY2VrfcMuK6HhsIBIQpAACAOiw0NFQTJkyRaUmf5F/f2alPDzvlNqXx4ycpPJwbviP4EKYAAADquC5duqpTp87af9ajw2We63LMkxWmdhV6lJTUVj179r4uxwQCDWEKAAAgCIwYcZskadNR13U53pfHnLIkjRx5G00nELQIUwAAAEGgXbsO6tChk/YVe3TkGmenTlWa2nnGo9atk9S5c9frVCEQeAhTAAAAQWLEiDGSrn126m9HmZUCJMIUAABA0OjQoaPatm2nfcUeFdq96+xX6jS1s9CjFi1aKi2t+3WuEAgshCkAAIAgctNNg2VJ+uaU26vXf1vglseqOg6zUgh2hCkAAIAg0qNHLzWIbqBvC9xymdZVvdZjWfrmlFuRkZHKyOhXQxUCgYMwBQAAEETCwsJ0w69uVIXb0veFV9eIYl+RR2edlvr2/ZUiIyNrqEIgcBCmAAAAgkxm5k2SpK0FV9eI4rvTVUsDBwwYeL1LAgISYQoAACDIxMXFKyWlvX4sMVXmurKlfna3pf1nqxpPJCa2qOEKgcBAmAIAAAhCvXr1kSVpV+GVNaLYU+SW25R69epbs4UBAYQwBQAAEITS0zNkGIZ2nrmyMLXzp+urMjJ612RZQEAhTAEAAAShmJgYdejQSYdKL7/Uz+GxdPCsR61bt1F8fEItVQj4P8IUAABAkEpL6y5L0sGzv9zV74ezHnksqWvXHrVSFxAoCFMAAABBqnPnLpKkA8W/vNTvwE9hq3PntBqvCQgkhCkAAIAg1axZoho1itWBs6ZM6+JL/SzL0oGzHtWrV09t2iTXcoWAfyNMAQAABCnDMNS5c5rK3ZZOVJgXHVPosFTksNSpUxfZbLx1BP4VPxEAAABBrH37TpKkQ6UXD1OHSquW+HXo0KnWagICBWEKAAAgiKWkpEiSDpddvAlFfllVyEpOTqm1moBAQZgCAAAIYnFxTdQgukF1aPq5/FKPwsPDlZjYopYrA/wfYQoAACCIGYahtintVOSwVOo8P1DZ3ZZOVVpKTk5RSEiIjyoE/JffhinLsvTiiy+qX79+6tOnjxYvXizTvPgnJpK0bds23XnnnUpPT9ctt9yirKysWqwWAAAgcJ3r0nfsZ00oTlSasiS1bp1U+0UBASDU1wVcyptvvqm1a9fqlVdekdvt1mOPPaa4uDjNnj37grEFBQW65557NHnyZP3Xf/2Xvv/+ey1cuFBNmjTRwIEDa794AACAANKiRUtJ0skKUx0b/XP7uQ5/LVu28kFVgP/z25mplStXav78+crIyFC/fv306KOPatWqVRcd+/nnnys+Pl6PPPKIkpKSNHLkSI0dO1Zr1qyp5aoBAAACT4sWVWHp5+3Rzz0/F7YAnM8vZ6ZOnjyp48ePq3fv3tXbevXqpaNHj+rUqVNKSEg4b/yAAQOUmpp6wXHKyspqvFYAAIBAFxvbWFGRUTpwtlLvHbRXb/+hxFRISIiaNm3mw+oA/+WXYaqgoECSzgtN8fHxkqQTJ05cEKZatmypli3/+YnJmTNntG7dOs2bN++qz20Y3lQMAAAQuAzDUGrnLtq69VttP3N+i/TU1E4KDfXLt4xAjbnSTOCznwy73a6TJ09edF9FRYUkKTw8vHrbucdOp/Oyx503b57i4+M1adKkq64rLq7BVb8GAAAg0D3++GMqKSm5YHuDBg3o5Adcgs/C1Pbt2zV9+vSL7nvsscckVQWniIiI6seSFBUVdcljlpeXa+7cucrLy9Of//znXxx7KWfOlMqyrvplAAAAdcCFoamoqMIHdQC+ZRhXNsniszDVt29f7du376L7Tp48qRdeeEEFBQXVy/fOLf1r0qTJRV9TVlamOXPmKD8/XytWrFBSUpJXdVmWCFMAAAAALssvu/k1bdpUiYmJysnJqd6Wk5OjxMTEC66XkiTTNPXrX/9aR44c0VtvvaX27dvXZrkAAAAAgpDfXk04efJkvfjii2rWrKp7zEsvvaRZs2ZV7y8sLFRERITq16+v999/X19//bVee+01NWzYsHoWKywsTI0aNfJF+QAAAADqOMOy/HNRm8fj0eLFi5Wdna2QkBDdcccdWrBggYyfWmsMHjxY48aN07x58zR79mx99dVXFxyjT58+euutt67qvKdPc80UAAAAEMwMQ4qPv/w1U34bpnyFMAUAAAAEtysNU355zRQAAAAA+DvCFAAAAAB4gTAFAAAAAF4gTAEAAACAFwhTAAAAAOAFwhQAAAAAeIEwBQAAAABeIEwBAAAAgBcIUwAAAADgBcIUAAAAAHiBMAUAAAAAXiBMAQAAAIAXQn1dgL8xDF9XAAAAAMCXrjQTGJZlWTVbCgAAAADUPSzzAwAAAAAvEKYAAAAAwAuEKQAAAADwAmEKAAAAALxAmAIAAAAALxCmAAAAAMALhCkAAAAA8AJhCgAAAAC8QJgCAAAAAC8QpgDI4XDoySefVEZGhjIzM7Vs2TJflwQA8CGn06lRo0bp66+/9nUpgF8L9XUBAHxv8eLF2rVrl1asWKFjx47p8ccfV2Jiom699VZflwYAqGUOh0MLFizQgQMHfF0K4PcIU0CQq6ioUFZWlv70pz+pS5cu6tKliw4cOKBVq1YRpgAgyBw8eFALFiyQZVm+LgUICCzzA4Lc3r175Xa7lZ6eXr2tV69e2r59u0zT9GFlAIDatmXLFvXt21fvvvuur0sBAgIzU0CQKygoUGxsrMLDw6u3xcfHy+FwqLi4WI0bN/ZhdQCA2jRlyhRflwAEFGamgCBXWVl5XpCSVP3c6XT6oiQAAICAQJgCglxERMQFoenc88jISF+UBAAAEBAIU0CQa9q0qYqKiuR2u6u3FRQUKDIyUg0bNvRhZQAAAP6NMAUEudTUVIWGhmrbtm3V23JyctS1a1fZbPyKAAAAuBTeKQFBLioqSmPHjtWiRYu0Y8cOff7551q2bJmmT5/u69IAAAD8Gt38AGjhwoVatGiRZsyYoejoaM2bN0/Dhg3zdVkAAAB+zbC4KxsAAAAAXDWW+QEAAACAFwhTAAAAAOAFwhQAAAAAeIEwBQAAAABeIEwBAAAAgBcIUwAAAADgBcIUAAAAAHiBMAUAAAAAXgj1dQEAAJzzxBNP6MMPP7zk/pUrV6pv3741XsfZs2f12muv6dNPP9WZM2eUmJioSZMmafr06bLZqj6H7NixY63VAwDwT4QpAIDfeOqpp7RgwQJJ0vr167Vs2TK9//771ftjYmJqvIaioiJNmjRJCQkJev7559WyZUvt3LlT//7v/67Dhw/rt7/9bY3XAAAIDIQpAIDfaNCggRo0aFD9OCQkRE2aNKnVGl566SWFh4frjTfeUEREhCSpVatWioyM1Ny5czV16lQlJyfXak0AAP/ENVMAgIBx5MgRdezYUUuWLFHv3r313HPP6eWXX9a0adPOGzd48GBlZ2dLkizL0pIlS5SZmamMjAzdf//9Onbs2EWP73Q6tW7dOt11113VQeqcQYMGafny5WrRosUFrzt58qTmz5+v3r17Ky0tTePGjVNOTk71/pUrV2rQoEHq2rWrxo8fr2+//bZ63+9//3tlZmaqW7dumjZtmg4cOOD19wcAULsIUwCAgLN161Z98MEHmj59+mXHvv3221qzZo1eeuklvfvuu4qLi9OsWbPkcrkuGJufn6+Kigp17dr1gn2GYahfv34KDw+/YN+jjz4qj8ejv/zlL1q9erWaNm2qRYsWSZJ2796txYsX65lnntEnn3yijIwMPfTQQzJNU5999pneffdd/eEPf9DatWsVHx+vhQsXXv03BADgEyzzAwAEnBkzZqh169ZXNPb111/XM888U90o4rnnnlNmZqY2b96swYMHnze2pKREkqqXGl4Jy7I0dOhQ3XLLLWrWrJkk6a677tK9994rSTp69KgMw1BiYqJatmyphx56SIMGDZJpmjp69KjCwsKUmJioxMRE/fa3v9WPP/54xecGAPgWYQoAEHAuttTuYsrLy3XixAk9/PDD1V34JMlutysvL++C8Y0aNZJU1c3vShmGocmTJ2v9+vXaunWrcnNztWvXLpmmKUnKzMxUhw4dNHr0aHXu3FlDhgzRhAkTFBoaqpEjR+rtt9/WkCFD1KNHDw0dOlR33HHHFZ8bAOBbhCkAQMD51+uZDMO4YL/b7ZYkeTweSdJ///d/X9A04mKdAVu3bq0GDRro+++/V7du3S7Y/8ADD2jatGm64YYbqreZpqlZs2appKREI0aM0ODBg+VyufTrX/9akhQVFaWsrCxt2bJFmzZtUnZ2tt555x1lZ2eradOm+uSTT/SPf/xDmzZt0htvvKH33ntPq1evVlRUlBffGQBAbeKaKQBAQAsLC1N5eXn18/LychUWFkqSGjZsqLi4OBUUFKhNmzZq06aNmjdvrhdeeEG5ubkXHCs0NFQjRozQqlWr5HQ6z9u3ceNGbdy4UQkJCedtP3jwoL755hstX75c999/vwYOHKhTp05JqloC+N1332np0qXq16+fFi5cqA0bNsjhcCgnJ0dffvmlsrKyNHDgQD377LP66KOPlJeXp/3791/vbxMAoAYQpgAAAa1r167au3evPvnkE+Xm5up3v/vdeUv6Zs6cqT/84Q/auHGj8vLy9PTTT2vr1q1q27btRY83b948lZWVafbs2dqyZYvy8/OVlZWlJ554QtOnT1e7du3OG9+wYUPZbDatW7dOR48e1YYNG/Tyyy9LquoOGBkZqSVLligrK0tHjhzRunXrVFFRoY4dO8o0TS1evFifffaZjhw5ouzsbEVFRSkpKanGvl8AgOuHZX4AgIDWv39/zZw5szpE3X333dUzQ5I0e/ZslZeX63e/+53KysqUlpamN95445I3AG7SpIneeecdvfzyy3r00UdVXFys1q1ba/78+Zo8efIF45s1a6ZFixZpyZIl+v3vf6/k5GQ9/fTTevzxx7V7926lp6fr+eef16uvvqrnnntOiYmJeuGFF5SSkqKUlBTNnz9f//mf/6mCggK1bdtWr776aq3cnBgAcO0My7IsXxcBAAAAAIGGZX4AAAAA4AXCFAAAAAB4gTAFAAAAAF4gTAEAAACAFwhTAAAAAOAFwhQAAAAAeIEwBQAAAABeIEwBAAAAgBcIUwAAAADgBcIUAAAAAHiBMAUAAAAAXvh/bcb/GVIcP6oAAAAASUVORK5CYII="
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 100
  },
  {
   "cell_type": "markdown",
   "id": "8e87dfe7",
   "metadata": {},
   "source": [
    "## Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0517d43fb1c2ac6",
   "metadata": {},
   "source": "### SHAP-values"
  },
  {
   "cell_type": "code",
   "id": "021d54c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T20:37:49.324177Z",
     "start_time": "2025-01-15T20:37:48.686262Z"
    }
   },
   "source": [
    "def shap_analysis_top_features_save(model, X_train, X_test, feature_names, top_n=7, output_dir=\"./\", label_size=10):\n",
    "    import os\n",
    "    try:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        explainer = shap.TreeExplainer(model)\n",
    "        shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "        mean_abs_shap_values = pd.DataFrame({\n",
    "            'Feature': feature_names,\n",
    "            'MeanAbsSHAP': abs(shap_values).mean(axis=0)\n",
    "        }).sort_values(by='MeanAbsSHAP', ascending=False)\n",
    "\n",
    "        top_features = mean_abs_shap_values.head(top_n)['Feature'].values\n",
    "        X_test_top = pd.DataFrame(X_test, columns=feature_names)[top_features]\n",
    "\n",
    "        feature_indices = [list(feature_names).index(feature) for feature in top_features]\n",
    "        shap_values_top = shap_values[:, feature_indices]\n",
    "\n",
    "        summary_plot_path = os.path.join(output_dir, f\"{model.__class__.__name__}_SHAP_Summary_Top{top_n}.png\")\n",
    "        shap.summary_plot(shap_values_top, X_test_top, feature_names=top_features, show=False)\n",
    "        plt.gca().tick_params(labelsize=label_size)\n",
    "\n",
    "        plt.savefig(summary_plot_path, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"Saved SHAP Summary Plot to {summary_plot_path}\")\n",
    "\n",
    "        bar_plot_path = os.path.join(output_dir, f\"{model.__class__.__name__}_SHAP_Bar_Top{top_n}.png\")\n",
    "        shap.summary_plot(shap_values_top, X_test_top, feature_names=top_features, plot_type=\"bar\", show=False)\n",
    "        plt.gca().tick_params(labelsize=label_size)\n",
    "\n",
    "        plt.savefig(bar_plot_path, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"Saved SHAP Bar Plot to {bar_plot_path}\")\n",
    "\n",
    "        return shap_values_top, top_features\n",
    "    except Exception as e:\n",
    "        print(f\"Error in SHAP analysis: {e}\")\n",
    "        return None, None\n",
    "\n",
    "output_directory = \"/Users/janlinzner/Library/Mobile Documents/com~apple~CloudDocs/Documents/McGill/1_Lectures/Decision Analytics (MGSC 662)/2_Coding/Exercises/ADA_Coding/Code/Plots\"\n",
    "\n",
    "print(\"Analyzing Gradient Boosting Model...\")\n",
    "gb_shap_values_top, gb_top_features = shap_analysis_top_features_save(\n",
    "    best_gb_model, X_train_scaled, X_test_scaled, X.columns, top_n=7, output_dir=output_directory, label_size=10\n",
    ")\n",
    "\n",
    "print(\"Analyzing LightGBM Model...\")\n",
    "lgb_shap_values_top, lgb_top_features = shap_analysis_top_features_save(\n",
    "    best_lgb_model, X_train_scaled, X_test_scaled, X.columns, top_n=7, output_dir=output_directory, label_size=10\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing Gradient Boosting Model...\n",
      "Saved SHAP Summary Plot to /Users/janlinzner/Library/Mobile Documents/com~apple~CloudDocs/Documents/McGill/1_Lectures/Decision Analytics (MGSC 662)/2_Coding/Exercises/ADA_Coding/Code/Plots/GradientBoostingClassifier_SHAP_Summary_Top7.png\n",
      "Saved SHAP Bar Plot to /Users/janlinzner/Library/Mobile Documents/com~apple~CloudDocs/Documents/McGill/1_Lectures/Decision Analytics (MGSC 662)/2_Coding/Exercises/ADA_Coding/Code/Plots/GradientBoostingClassifier_SHAP_Bar_Top7.png\n",
      "Analyzing LightGBM Model...\n",
      "Saved SHAP Summary Plot to /Users/janlinzner/Library/Mobile Documents/com~apple~CloudDocs/Documents/McGill/1_Lectures/Decision Analytics (MGSC 662)/2_Coding/Exercises/ADA_Coding/Code/Plots/LGBMClassifier_SHAP_Summary_Top7.png\n",
      "Saved SHAP Bar Plot to /Users/janlinzner/Library/Mobile Documents/com~apple~CloudDocs/Documents/McGill/1_Lectures/Decision Analytics (MGSC 662)/2_Coding/Exercises/ADA_Coding/Code/Plots/LGBMClassifier_SHAP_Bar_Top7.png\n"
     ]
    }
   ],
   "execution_count": 101
  },
  {
   "cell_type": "markdown",
   "id": "ceaf28a919227f3e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T18:09:09.201737Z",
     "start_time": "2025-01-15T18:09:09.197982Z"
    }
   },
   "source": "## Dashboard Preparation (Data to JSON)"
  },
  {
   "cell_type": "code",
   "id": "cf6e9fa392e2f450",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T20:37:49.395170Z",
     "start_time": "2025-01-15T20:37:49.389425Z"
    }
   },
   "source": [
    "def prepare_dashboard_data(test_results, feature_names,\n",
    "                           gb_shap_values, lgb_shap_values,\n",
    "                           ensemble_test_predictions, ensemble_test_probabilities):\n",
    "    from sklearn.metrics import confusion_matrix, classification_report\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import json\n",
    "\n",
    "    def prepare_confusion_matrix_section(true_class, predicted_class):\n",
    "        tn, fp, fn, tp = confusion_matrix(true_class, predicted_class).ravel()\n",
    "        return {\n",
    "            \"truePositive\": int(tp),\n",
    "            \"trueNegative\": int(tn),\n",
    "            \"falsePositive\": int(fp),\n",
    "            \"falseNegative\": int(fn)\n",
    "        }\n",
    "\n",
    "    def transform_results(results, prediction_column, probability_column):\n",
    "        transformed = []\n",
    "        for _, row in results.iterrows():\n",
    "            transformed.append({\n",
    "                \"id\": row[\"ID\"],\n",
    "                \"actualLabel\": \"Positive\" if row[\"True Class (Test)\"] == 1 else \"Negative\",\n",
    "                \"predictedLabel\": \"Positive\" if row[prediction_column] == 1 else \"Negative\",\n",
    "                \"confidence\": f\"{row[probability_column] * 100:.2f}%\",\n",
    "                \"timestamp\": pd.Timestamp.now().isoformat()\n",
    "            })\n",
    "        return transformed\n",
    "\n",
    "    def transform_results_ensemble(true_class, predictions, probabilities, observation_ids):\n",
    "        transformed = []\n",
    "        for obs_id, actual, pred, prob in zip(observation_ids, true_class, predictions, probabilities):\n",
    "            transformed.append({\n",
    "                \"id\": obs_id,\n",
    "                \"actualLabel\": \"Positive\" if actual == 1 else \"Negative\",\n",
    "                \"predictedLabel\": \"Positive\" if pred == 1 else \"Negative\",\n",
    "                \"confidence\": f\"{prob * 100:.2f}%\",\n",
    "                \"timestamp\": pd.Timestamp.now().isoformat()\n",
    "            })\n",
    "        return transformed\n",
    "\n",
    "    def format_shap_values(shap_values, feature_names):\n",
    "        shap_formatted = [\n",
    "            {\"feature\": feature, \"value\": float(shap_value)}\n",
    "            for feature, shap_value in zip(feature_names, np.abs(shap_values).mean(axis=0))\n",
    "        ]\n",
    "        return shap_formatted\n",
    "\n",
    "    gb_shap_data = format_shap_values(gb_shap_values, feature_names)\n",
    "    lgb_shap_data = format_shap_values(lgb_shap_values, feature_names)\n",
    "\n",
    "    gb_data = {\n",
    "        \"name\": \"Gradient Boosting\",\n",
    "        \"stats\": {\n",
    "            \"accuracy\": (test_results['True Class (Test)'] == test_results['GB Prediction (Test)']).mean(),\n",
    "            \"precision\": classification_report(\n",
    "                test_results['True Class (Test)'],\n",
    "                test_results['GB Prediction (Test)'],\n",
    "                output_dict=True\n",
    "            )['macro avg']['precision'],\n",
    "            \"recall\": classification_report(\n",
    "                test_results['True Class (Test)'],\n",
    "                test_results['GB Prediction (Test)'],\n",
    "                output_dict=True\n",
    "            )['macro avg']['recall'],\n",
    "            \"f1Score\": classification_report(\n",
    "                test_results['True Class (Test)'],\n",
    "                test_results['GB Prediction (Test)'],\n",
    "                output_dict=True\n",
    "            )['macro avg']['f1-score']\n",
    "        },\n",
    "        \"confusionMatrix\": prepare_confusion_matrix_section(\n",
    "            test_results['True Class (Test)'],\n",
    "            test_results['GB Prediction (Test)']\n",
    "        ),\n",
    "        \"results\": transform_results(\n",
    "            test_results, \"GB Prediction (Test)\", \"GB Probability (Success, Test)\"\n",
    "        ),\n",
    "        \"shapValues\": gb_shap_data,\n",
    "        \"paperUrl\": \"https://arxiv.org/abs/1603.02754\"\n",
    "    }\n",
    "\n",
    "    lgb_data = {\n",
    "        \"name\": \"LightGBM\",\n",
    "        \"stats\": {\n",
    "            \"accuracy\": (test_results['True Class (Test)'] == test_results['LGB Prediction (Test)']).mean(),\n",
    "            \"precision\": classification_report(\n",
    "                test_results['True Class (Test)'],\n",
    "                test_results['LGB Prediction (Test)'],\n",
    "                output_dict=True\n",
    "            )['macro avg']['precision'],\n",
    "            \"recall\": classification_report(\n",
    "                test_results['True Class (Test)'],\n",
    "                test_results['LGB Prediction (Test)'],\n",
    "                output_dict=True\n",
    "            )['macro avg']['recall'],\n",
    "            \"f1Score\": classification_report(\n",
    "                test_results['True Class (Test)'],\n",
    "                test_results['LGB Prediction (Test)'],\n",
    "                output_dict=True\n",
    "            )['macro avg']['f1-score']\n",
    "        },\n",
    "        \"confusionMatrix\": prepare_confusion_matrix_section(\n",
    "            test_results['True Class (Test)'],\n",
    "            test_results['LGB Prediction (Test)']\n",
    "        ),\n",
    "        \"results\": transform_results(\n",
    "            test_results, \"LGB Prediction (Test)\", \"LGB Probability (Success, Test)\"\n",
    "        ),\n",
    "        \"shapValues\": lgb_shap_data,\n",
    "        \"paperUrl\": \"https://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree\"\n",
    "    }\n",
    "\n",
    "    logistic_data = {\n",
    "        \"name\": \"Logistic Regression\",\n",
    "        \"stats\": {\n",
    "            \"accuracy\": (test_results['True Class (Test)'] == test_results['Logistic Prediction (Test)']).mean(),\n",
    "            \"precision\": classification_report(\n",
    "                test_results['True Class (Test)'],\n",
    "                test_results['Logistic Prediction (Test)'],\n",
    "                output_dict=True\n",
    "            )['macro avg']['precision'],\n",
    "            \"recall\": classification_report(\n",
    "                test_results['True Class (Test)'],\n",
    "                test_results['Logistic Prediction (Test)'],\n",
    "                output_dict=True\n",
    "            )['macro avg']['recall'],\n",
    "            \"f1Score\": classification_report(\n",
    "                test_results['True Class (Test)'],\n",
    "                test_results['Logistic Prediction (Test)'],\n",
    "                output_dict=True\n",
    "            )['macro avg']['f1-score']\n",
    "        },\n",
    "        \"confusionMatrix\": prepare_confusion_matrix_section(\n",
    "            test_results['True Class (Test)'],\n",
    "            test_results['Logistic Prediction (Test)']\n",
    "        ),\n",
    "        \"results\": transform_results(\n",
    "            test_results, \"Logistic Prediction (Test)\", \"Logistic Probability (Success, Test)\"\n",
    "        ),\n",
    "        \"shapValues\": [],\n",
    "        \"paperUrl\": \"https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\"\n",
    "    }\n",
    "\n",
    "    nn_data = {\n",
    "        \"name\": \"Neural Network\",\n",
    "        \"stats\": {\n",
    "            \"accuracy\": (test_results['True Class (Test)'] == test_results['NN Prediction (Test)']).mean(),\n",
    "            \"precision\": classification_report(\n",
    "                test_results['True Class (Test)'],\n",
    "                test_results['NN Prediction (Test)'],\n",
    "                output_dict=True\n",
    "            )['macro avg']['precision'],\n",
    "            \"recall\": classification_report(\n",
    "                test_results['True Class (Test)'],\n",
    "                test_results['NN Prediction (Test)'],\n",
    "                output_dict=True\n",
    "            )['macro avg']['recall'],\n",
    "            \"f1Score\": classification_report(\n",
    "                test_results['True Class (Test)'],\n",
    "                test_results['NN Prediction (Test)'],\n",
    "                output_dict=True\n",
    "            )['macro avg']['f1-score']\n",
    "        },\n",
    "        \"confusionMatrix\": prepare_confusion_matrix_section(\n",
    "            test_results['True Class (Test)'],\n",
    "            test_results['NN Prediction (Test)']\n",
    "        ),\n",
    "        \"results\": transform_results(\n",
    "            test_results, \"NN Prediction (Test)\", \"NN Probability (Success, Test)\"\n",
    "        ),\n",
    "        \"shapValues\": [],\n",
    "        \"paperUrl\": \"https://arxiv.org/abs/1512.03385\"\n",
    "    }\n",
    "\n",
    "    true_classes = test_results['True Class (Test)']\n",
    "    ensemble_data = {\n",
    "        \"name\": \"Ensemble Model\",\n",
    "        \"stats\": {\n",
    "            \"accuracy\": (true_classes == ensemble_test_predictions).mean(),\n",
    "            \"precision\": classification_report(\n",
    "                true_classes,\n",
    "                ensemble_test_predictions,\n",
    "                output_dict=True\n",
    "            )['macro avg']['precision'],\n",
    "            \"recall\": classification_report(\n",
    "                true_classes,\n",
    "                ensemble_test_predictions,\n",
    "                output_dict=True\n",
    "            )['macro avg']['recall'],\n",
    "            \"f1Score\": classification_report(\n",
    "                true_classes,\n",
    "                ensemble_test_predictions,\n",
    "                output_dict=True\n",
    "            )['macro avg']['f1-score']\n",
    "        },\n",
    "        \"confusionMatrix\": prepare_confusion_matrix_section(\n",
    "            true_classes,\n",
    "            ensemble_test_predictions\n",
    "        ),\n",
    "        \"results\": transform_results_ensemble(\n",
    "            true_classes,\n",
    "            ensemble_test_predictions,\n",
    "            ensemble_test_probabilities,\n",
    "            test_results[\"Observation Index\"]\n",
    "        ),\n",
    "        \"shapValues\": [],\n",
    "        \"paperUrl\": \"\"\n",
    "    }\n",
    "\n",
    "    dashboard_data = [gb_data, lgb_data, logistic_data, nn_data, ensemble_data]\n",
    "    with open(\"dashboard_data.json\", \"w\") as f:\n",
    "        json.dump(dashboard_data, f, indent=4)\n",
    "\n",
    "    print(\"Dashboard data is saved under dashboard_data.json\")\n"
   ],
   "outputs": [],
   "execution_count": 102
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T20:37:49.571972Z",
     "start_time": "2025-01-15T20:37:49.512726Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Calling the function\n",
    "prepare_dashboard_data(\n",
    "    test_results=test_results,\n",
    "    feature_names=X.columns,\n",
    "    gb_shap_values=gb_shap_values,\n",
    "    lgb_shap_values=lgb_shap_values,\n",
    "    ensemble_test_predictions=ensemble_test_predictions,\n",
    "    ensemble_test_probabilities=ensemble_test_probabilities\n",
    ")"
   ],
   "id": "f6bac92d3a9d73c3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dashboard data is saved under dashboard_data.json\n"
     ]
    }
   ],
   "execution_count": 103
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
