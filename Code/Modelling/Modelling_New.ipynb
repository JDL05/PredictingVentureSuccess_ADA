{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7917d461468ce96a",
   "metadata": {},
   "source": [
    "# Prediction\n",
    "Explanation:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d89661898e930c7",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "id": "4bfa78658af2b407",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T19:18:43.724695Z",
     "start_time": "2025-01-16T19:18:40.316681Z"
    }
   },
   "source": [
    "from importnb import Notebook\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, roc_auc_score, roc_curve, precision_recall_curve, make_scorer\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, LeakyReLU\n",
    "from tensorflow.keras.initializers import HeNormal\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "import shap\n",
    "from sklearn.calibration import calibration_curve\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "c66d297d4dfe8590",
   "metadata": {},
   "source": [
    "## Dataframe import from 'FeatureEngineering'"
   ]
  },
  {
   "cell_type": "code",
   "id": "26daa4a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T19:18:43.732372Z",
     "start_time": "2025-01-16T19:18:43.730818Z"
    }
   },
   "source": [
    "pd.set_option('display.max_columns', None)"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "dcbd2b89652d5181",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T19:18:43.737316Z",
     "start_time": "2025-01-16T19:18:43.735664Z"
    }
   },
   "source": [
    "#with Notebook():\n",
    "#    from ModelPreparation import data\n"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "64fa4fc5",
   "metadata": {},
   "source": [
    "## Manual Upload"
   ]
  },
  {
   "cell_type": "code",
   "id": "7e2a957c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T19:18:43.751285Z",
     "start_time": "2025-01-16T19:18:43.740751Z"
    }
   },
   "source": "data = pd.read_csv('/Users/janlinzner/Library/Mobile Documents/com~apple~CloudDocs/Documents/McGill/1_Lectures/Decision Analytics (MGSC 662)/2_Coding/Exercises/ADA_Coding/Code/data.csv')",
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "870ec4eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T19:18:43.876354Z",
     "start_time": "2025-01-16T19:18:43.839763Z"
    }
   },
   "source": [
    "data"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "        ID  Number of Founders  Number of Employees  Number of Funding Rounds  \\\n",
       "0        1                 1.0                 30.5                         2   \n",
       "1        2                 2.0                 30.5                         2   \n",
       "2        3                 2.0                 30.5                         1   \n",
       "3        4                 3.0                  5.5                         1   \n",
       "4        5                 4.0                 30.5                         1   \n",
       "...    ...                 ...                  ...                       ...   \n",
       "1505  1506                 4.0                  5.5                         2   \n",
       "1506  1507                 3.0                 30.5                         3   \n",
       "1507  1508                 4.0                 30.5                         2   \n",
       "1508  1509                 2.0                  5.5                         2   \n",
       "1509  1510                 1.0                 30.5                         1   \n",
       "\n",
       "      Last Funding Amount  Total Funding Amount  Number of Lead Investors  \\\n",
       "0               7000000.0             7000000.0                       3.0   \n",
       "1               3000000.0             3000000.0                       1.0   \n",
       "2                     0.0                   0.0                       1.0   \n",
       "3               1000000.0             1000000.0                       2.0   \n",
       "4                     0.0                   0.0                       0.0   \n",
       "...                   ...                   ...                       ...   \n",
       "1505              36000.0              161000.0                       2.0   \n",
       "1506                  0.0               60000.0                       0.0   \n",
       "1507                  0.0                   0.0                       2.0   \n",
       "1508                  0.0                   0.0                       1.0   \n",
       "1509                  0.0                   0.0                       0.0   \n",
       "\n",
       "      Number of Investors  Number of Acquisitions  X: Followers  X: Following  \\\n",
       "0                     4.0                     0.0           0.0           0.0   \n",
       "1                     2.0                     0.0           0.0           0.0   \n",
       "2                     1.0                     0.0           0.0           0.0   \n",
       "3                     2.0                     0.0         269.0         346.0   \n",
       "4                     0.0                     0.0           0.0           0.0   \n",
       "...                   ...                     ...           ...           ...   \n",
       "1505                  2.0                     0.0         102.0         438.0   \n",
       "1506                  2.0                     0.0           0.0           0.0   \n",
       "1507                  4.0                     0.0         232.0         482.0   \n",
       "1508                  2.0                     0.0          69.0         298.0   \n",
       "1509                  1.0                     0.0           0.0           0.0   \n",
       "\n",
       "      X: Number of Tweets  X: Account Age Days  X: Tweet Activity  \\\n",
       "0                     0.0                  0.0               0.00   \n",
       "1                     0.0                  0.0               0.00   \n",
       "2                     0.0                  0.0               0.00   \n",
       "3                   248.0               3389.0               0.07   \n",
       "4                     0.0                  0.0               0.00   \n",
       "...                   ...                  ...                ...   \n",
       "1505                169.0               2499.0               0.07   \n",
       "1506                  0.0                  0.0               0.00   \n",
       "1507                786.0               2467.0               0.32   \n",
       "1508                 71.0               3456.0               0.02   \n",
       "1509                  0.0                  0.0               0.00   \n",
       "\n",
       "      X: Followers Max Growth  X: Followers Max Loss  X: Tweets Max Growth  \\\n",
       "0                         0.0                    0.0                   0.0   \n",
       "1                         0.0                    0.0                   0.0   \n",
       "2                         0.0                    0.0                   0.0   \n",
       "3                         2.0                   -3.0                   0.0   \n",
       "4                         0.0                    0.0                   0.0   \n",
       "...                       ...                    ...                   ...   \n",
       "1505                      4.0                   -2.0                   0.0   \n",
       "1506                      0.0                    0.0                   0.0   \n",
       "1507                     29.0                  -42.0                 104.0   \n",
       "1508                      4.0                   -4.0                   0.0   \n",
       "1509                      0.0                    0.0                   0.0   \n",
       "\n",
       "      X: Tweets Max Loss  Months until First Round  Grant Y/N  \\\n",
       "0                    0.0                      40.0          0   \n",
       "1                    0.0                      15.0          0   \n",
       "2                    0.0                      29.0          0   \n",
       "3                   -2.0                      -1.0          0   \n",
       "4                    0.0                      32.0          0   \n",
       "...                  ...                       ...        ...   \n",
       "1505                -2.0                       4.0          1   \n",
       "1506                 0.0                      59.0          0   \n",
       "1507                -8.0                       4.0          0   \n",
       "1508                -1.0                      14.0          0   \n",
       "1509                 0.0                       9.0          0   \n",
       "\n",
       "      Made Acquisitions  Project Funding  Startup Funding  Growth Funding  \\\n",
       "0                     0                1                1               0   \n",
       "1                     0                1                1               0   \n",
       "2                     0                1                1               0   \n",
       "3                     0                0                0               0   \n",
       "4                     0                1                1               0   \n",
       "...                 ...              ...              ...             ...   \n",
       "1505                  0                1                0               0   \n",
       "1506                  0                1                1               0   \n",
       "1507                  0                1                1               0   \n",
       "1508                  0                1                1               0   \n",
       "1509                  0                1                1               0   \n",
       "\n",
       "      Expansion Funding  Exit Funding  Average Time To Next Round  \\\n",
       "0                     0             0                   31.550000   \n",
       "1                     0             0                   16.433333   \n",
       "2                     0             0                   29.400000   \n",
       "3                     0             0                   -1.000000   \n",
       "4                     0             0                   32.433333   \n",
       "...                 ...           ...                         ...   \n",
       "1505                  0             0                   11.633333   \n",
       "1506                  0             0                   34.366667   \n",
       "1507                  0             0                   21.683333   \n",
       "1508                  0             0                   20.383333   \n",
       "1509                  0             0                    8.366667   \n",
       "\n",
       "      Average Funding Size  Average Number of Investments by Investors  \\\n",
       "0                7000000.0                                    0.000000   \n",
       "1                3000000.0                                  321.000000   \n",
       "2                      0.0                                   36.000000   \n",
       "3                      0.0                                    0.000000   \n",
       "4                      0.0                                    0.000000   \n",
       "...                    ...                                         ...   \n",
       "1505               80500.0                                   28.000000   \n",
       "1506               30000.0                                    4.000000   \n",
       "1507                   0.0                                   58.666667   \n",
       "1508                   0.0                                   53.000000   \n",
       "1509                   0.0                                    2.000000   \n",
       "\n",
       "      Average Number of Exits by Investors  \\\n",
       "0                                    182.0   \n",
       "1                                     32.0   \n",
       "2                                      7.0   \n",
       "3                                      0.0   \n",
       "4                                      0.0   \n",
       "...                                    ...   \n",
       "1505                                   2.0   \n",
       "1506                                   0.0   \n",
       "1507                                   6.0   \n",
       "1508                                   8.0   \n",
       "1509                                   0.0   \n",
       "\n",
       "      Average Number of Lead Investments by Investors  \\\n",
       "0                                          220.000000   \n",
       "1                                           49.500000   \n",
       "2                                           19.000000   \n",
       "3                                            0.000000   \n",
       "4                                            0.000000   \n",
       "...                                               ...   \n",
       "1505                                         8.500000   \n",
       "1506                                         0.000000   \n",
       "1507                                        23.333333   \n",
       "1508                                        27.000000   \n",
       "1509                                         0.000000   \n",
       "\n",
       "      Average Number of Portfolio Organizations by Investors  \\\n",
       "0                                                   0.0        \n",
       "1                                                 277.0        \n",
       "2                                                  29.0        \n",
       "3                                                   0.0        \n",
       "4                                                   0.0        \n",
       "...                                                 ...        \n",
       "1505                                               27.0        \n",
       "1506                                                3.0        \n",
       "1507                                               41.0        \n",
       "1508                                               53.0        \n",
       "1509                                                2.0        \n",
       "\n",
       "      Investor Country_AUS  Investor Country_GER  Investor Country_SWI  \\\n",
       "0                      0.0                   0.0                   0.0   \n",
       "1                      0.0                   1.0                   0.0   \n",
       "2                      0.0                   1.0                   0.0   \n",
       "3                      0.0                   0.0                   0.0   \n",
       "4                      0.0                   0.0                   0.0   \n",
       "...                    ...                   ...                   ...   \n",
       "1505                   0.0                   1.0                   0.0   \n",
       "1506                   0.0                   1.0                   0.0   \n",
       "1507                   0.0                   1.0                   0.0   \n",
       "1508                   0.0                   1.0                   0.0   \n",
       "1509                   0.0                   1.0                   0.0   \n",
       "\n",
       "      Investor Country_UK  Investor Country_USA  Top Investor Participation  \\\n",
       "0                     0.0                   1.0                           1   \n",
       "1                     0.0                   1.0                           1   \n",
       "2                     0.0                   0.0                           0   \n",
       "3                     0.0                   0.0                           0   \n",
       "4                     0.0                   0.0                           0   \n",
       "...                   ...                   ...                         ...   \n",
       "1505                  0.0                   0.0                           0   \n",
       "1506                  0.0                   0.0                           0   \n",
       "1507                  0.0                   0.0                           0   \n",
       "1508                  0.0                   0.0                           0   \n",
       "1509                  0.0                   0.0                           0   \n",
       "\n",
       "      Industry_administrative services  Industry_advertising  \\\n",
       "0                                  0.0                   0.0   \n",
       "1                                  0.0                   0.0   \n",
       "2                                  0.0                   0.0   \n",
       "3                                  0.0                   0.0   \n",
       "4                                  0.0                   0.0   \n",
       "...                                ...                   ...   \n",
       "1505                               1.0                   0.0   \n",
       "1506                               0.0                   0.0   \n",
       "1507                               0.0                   0.0   \n",
       "1508                               0.0                   0.0   \n",
       "1509                               0.0                   0.0   \n",
       "\n",
       "      Industry_agriculture and farming  Industry_apps  \\\n",
       "0                                  0.0            0.0   \n",
       "1                                  0.0            0.0   \n",
       "2                                  0.0            0.0   \n",
       "3                                  0.0            0.0   \n",
       "4                                  0.0            0.0   \n",
       "...                                ...            ...   \n",
       "1505                               0.0            1.0   \n",
       "1506                               0.0            1.0   \n",
       "1507                               0.0            0.0   \n",
       "1508                               1.0            0.0   \n",
       "1509                               0.0            0.0   \n",
       "\n",
       "      Industry_artificial intelligence (ai)  Industry_biotechnology  \\\n",
       "0                                       0.0                     0.0   \n",
       "1                                       1.0                     0.0   \n",
       "2                                       0.0                     0.0   \n",
       "3                                       0.0                     0.0   \n",
       "4                                       1.0                     0.0   \n",
       "...                                     ...                     ...   \n",
       "1505                                    0.0                     0.0   \n",
       "1506                                    1.0                     0.0   \n",
       "1507                                    1.0                     0.0   \n",
       "1508                                    0.0                     0.0   \n",
       "1509                                    0.0                     0.0   \n",
       "\n",
       "      Industry_blockchain and cryptocurrency  Industry_clothing and apparel  \\\n",
       "0                                        0.0                            0.0   \n",
       "1                                        0.0                            0.0   \n",
       "2                                        0.0                            0.0   \n",
       "3                                        0.0                            0.0   \n",
       "4                                        0.0                            0.0   \n",
       "...                                      ...                            ...   \n",
       "1505                                     0.0                            0.0   \n",
       "1506                                     0.0                            0.0   \n",
       "1507                                     0.0                            0.0   \n",
       "1508                                     0.0                            0.0   \n",
       "1509                                     0.0                            0.0   \n",
       "\n",
       "      Industry_commerce and shopping  Industry_community and lifestyle  \\\n",
       "0                                0.0                               0.0   \n",
       "1                                0.0                               0.0   \n",
       "2                                0.0                               0.0   \n",
       "3                                0.0                               0.0   \n",
       "4                                0.0                               0.0   \n",
       "...                              ...                               ...   \n",
       "1505                             0.0                               0.0   \n",
       "1506                             0.0                               0.0   \n",
       "1507                             0.0                               0.0   \n",
       "1508                             1.0                               1.0   \n",
       "1509                             0.0                               0.0   \n",
       "\n",
       "      Industry_consumer electronics  Industry_consumer goods  \\\n",
       "0                               0.0                      0.0   \n",
       "1                               0.0                      0.0   \n",
       "2                               0.0                      0.0   \n",
       "3                               0.0                      0.0   \n",
       "4                               0.0                      0.0   \n",
       "...                             ...                      ...   \n",
       "1505                            0.0                      0.0   \n",
       "1506                            0.0                      0.0   \n",
       "1507                            0.0                      0.0   \n",
       "1508                            0.0                      1.0   \n",
       "1509                            0.0                      0.0   \n",
       "\n",
       "      Industry_content and publishing  Industry_data and analytics  \\\n",
       "0                                 0.0                          0.0   \n",
       "1                                 0.0                          1.0   \n",
       "2                                 0.0                          0.0   \n",
       "3                                 0.0                          0.0   \n",
       "4                                 0.0                          1.0   \n",
       "...                               ...                          ...   \n",
       "1505                              0.0                          0.0   \n",
       "1506                              0.0                          1.0   \n",
       "1507                              0.0                          1.0   \n",
       "1508                              0.0                          0.0   \n",
       "1509                              0.0                          0.0   \n",
       "\n",
       "      Industry_design  Industry_education  Industry_energy  Industry_events  \\\n",
       "0                 0.0                 0.0              0.0              0.0   \n",
       "1                 0.0                 0.0              0.0              0.0   \n",
       "2                 0.0                 0.0              0.0              0.0   \n",
       "3                 0.0                 0.0              0.0              0.0   \n",
       "4                 0.0                 0.0              0.0              0.0   \n",
       "...               ...                 ...              ...              ...   \n",
       "1505              0.0                 0.0              0.0              0.0   \n",
       "1506              0.0                 0.0              0.0              0.0   \n",
       "1507              0.0                 0.0              0.0              0.0   \n",
       "1508              0.0                 0.0              0.0              0.0   \n",
       "1509              0.0                 0.0              0.0              0.0   \n",
       "\n",
       "      Industry_financial services  Industry_food and beverage  \\\n",
       "0                             0.0                         0.0   \n",
       "1                             0.0                         0.0   \n",
       "2                             0.0                         1.0   \n",
       "3                             0.0                         0.0   \n",
       "4                             0.0                         0.0   \n",
       "...                           ...                         ...   \n",
       "1505                          0.0                         0.0   \n",
       "1506                          0.0                         0.0   \n",
       "1507                          1.0                         0.0   \n",
       "1508                          0.0                         1.0   \n",
       "1509                          0.0                         0.0   \n",
       "\n",
       "      Industry_gaming  Industry_government and military  Industry_hardware  \\\n",
       "0                 0.0                               0.0                0.0   \n",
       "1                 0.0                               0.0                1.0   \n",
       "2                 0.0                               0.0                0.0   \n",
       "3                 0.0                               0.0                0.0   \n",
       "4                 0.0                               0.0                0.0   \n",
       "...               ...                               ...                ...   \n",
       "1505              0.0                               0.0                0.0   \n",
       "1506              0.0                               0.0                1.0   \n",
       "1507              0.0                               0.0                0.0   \n",
       "1508              0.0                               0.0                0.0   \n",
       "1509              0.0                               0.0                0.0   \n",
       "\n",
       "      Industry_health care  Industry_information technology  \\\n",
       "0                      0.0                              0.0   \n",
       "1                      0.0                              0.0   \n",
       "2                      0.0                              0.0   \n",
       "3                      0.0                              0.0   \n",
       "4                      0.0                              1.0   \n",
       "...                    ...                              ...   \n",
       "1505                   0.0                              0.0   \n",
       "1506                   0.0                              1.0   \n",
       "1507                   0.0                              0.0   \n",
       "1508                   1.0                              0.0   \n",
       "1509                   0.0                              0.0   \n",
       "\n",
       "      Industry_internet services  Industry_lending and investments  \\\n",
       "0                            0.0                               0.0   \n",
       "1                            0.0                               0.0   \n",
       "2                            0.0                               0.0   \n",
       "3                            0.0                               0.0   \n",
       "4                            1.0                               0.0   \n",
       "...                          ...                               ...   \n",
       "1505                         0.0                               0.0   \n",
       "1506                         1.0                               0.0   \n",
       "1507                         0.0                               0.0   \n",
       "1508                         0.0                               0.0   \n",
       "1509                         0.0                               0.0   \n",
       "\n",
       "      Industry_manufacturing  Industry_media and entertainment  \\\n",
       "0                        0.0                               0.0   \n",
       "1                        1.0                               0.0   \n",
       "2                        0.0                               0.0   \n",
       "3                        1.0                               0.0   \n",
       "4                        0.0                               0.0   \n",
       "...                      ...                               ...   \n",
       "1505                     0.0                               0.0   \n",
       "1506                     0.0                               0.0   \n",
       "1507                     0.0                               0.0   \n",
       "1508                     0.0                               0.0   \n",
       "1509                     0.0                               0.0   \n",
       "\n",
       "      Industry_messaging and telecommunications  Industry_mobile  \\\n",
       "0                                           0.0              0.0   \n",
       "1                                           0.0              0.0   \n",
       "2                                           0.0              0.0   \n",
       "3                                           0.0              0.0   \n",
       "4                                           0.0              0.0   \n",
       "...                                         ...              ...   \n",
       "1505                                        0.0              1.0   \n",
       "1506                                        0.0              0.0   \n",
       "1507                                        0.0              0.0   \n",
       "1508                                        0.0              0.0   \n",
       "1509                                        0.0              0.0   \n",
       "\n",
       "      Industry_music and audio  Industry_natural resources  \\\n",
       "0                          0.0                         0.0   \n",
       "1                          0.0                         0.0   \n",
       "2                          0.0                         0.0   \n",
       "3                          0.0                         0.0   \n",
       "4                          0.0                         1.0   \n",
       "...                        ...                         ...   \n",
       "1505                       0.0                         0.0   \n",
       "1506                       0.0                         0.0   \n",
       "1507                       0.0                         0.0   \n",
       "1508                       0.0                         0.0   \n",
       "1509                       0.0                         0.0   \n",
       "\n",
       "      Industry_navigation and mapping  Industry_other  Industry_payments  \\\n",
       "0                                 0.0             0.0                0.0   \n",
       "1                                 0.0             0.0                0.0   \n",
       "2                                 0.0             0.0                0.0   \n",
       "3                                 0.0             0.0                0.0   \n",
       "4                                 0.0             1.0                0.0   \n",
       "...                               ...             ...                ...   \n",
       "1505                              0.0             1.0                0.0   \n",
       "1506                              0.0             0.0                0.0   \n",
       "1507                              0.0             0.0                0.0   \n",
       "1508                              0.0             0.0                0.0   \n",
       "1509                              0.0             1.0                0.0   \n",
       "\n",
       "      Industry_platforms  Industry_privacy and security  \\\n",
       "0                    0.0                            0.0   \n",
       "1                    0.0                            0.0   \n",
       "2                    0.0                            0.0   \n",
       "3                    0.0                            0.0   \n",
       "4                    0.0                            0.0   \n",
       "...                  ...                            ...   \n",
       "1505                 0.0                            0.0   \n",
       "1506                 0.0                            0.0   \n",
       "1507                 0.0                            0.0   \n",
       "1508                 0.0                            0.0   \n",
       "1509                 0.0                            0.0   \n",
       "\n",
       "      Industry_professional services  Industry_real estate  \\\n",
       "0                                0.0                   0.0   \n",
       "1                                0.0                   0.0   \n",
       "2                                0.0                   0.0   \n",
       "3                                0.0                   0.0   \n",
       "4                                0.0                   1.0   \n",
       "...                              ...                   ...   \n",
       "1505                             0.0                   0.0   \n",
       "1506                             0.0                   1.0   \n",
       "1507                             1.0                   0.0   \n",
       "1508                             0.0                   0.0   \n",
       "1509                             0.0                   1.0   \n",
       "\n",
       "      Industry_sales and marketing  Industry_science and engineering  \\\n",
       "0                              0.0                               0.0   \n",
       "1                              0.0                               1.0   \n",
       "2                              0.0                               0.0   \n",
       "3                              0.0                               0.0   \n",
       "4                              0.0                               1.0   \n",
       "...                            ...                               ...   \n",
       "1505                           0.0                               0.0   \n",
       "1506                           0.0                               1.0   \n",
       "1507                           0.0                               0.0   \n",
       "1508                           0.0                               0.0   \n",
       "1509                           0.0                               0.0   \n",
       "\n",
       "      Industry_social impact  Industry_software  Industry_sports  \\\n",
       "0                        0.0                1.0              0.0   \n",
       "1                        0.0                1.0              0.0   \n",
       "2                        0.0                0.0              0.0   \n",
       "3                        0.0                1.0              0.0   \n",
       "4                        0.0                1.0              0.0   \n",
       "...                      ...                ...              ...   \n",
       "1505                     0.0                1.0              0.0   \n",
       "1506                     0.0                1.0              0.0   \n",
       "1507                     0.0                1.0              0.0   \n",
       "1508                     0.0                0.0              1.0   \n",
       "1509                     0.0                0.0              0.0   \n",
       "\n",
       "      Industry_sustainability  Industry_transportation  \\\n",
       "0                         0.0                      1.0   \n",
       "1                         0.0                      0.0   \n",
       "2                         0.0                      0.0   \n",
       "3                         0.0                      0.0   \n",
       "4                         0.0                      0.0   \n",
       "...                       ...                      ...   \n",
       "1505                      0.0                      0.0   \n",
       "1506                      0.0                      1.0   \n",
       "1507                      0.0                      0.0   \n",
       "1508                      0.0                      0.0   \n",
       "1509                      0.0                      0.0   \n",
       "\n",
       "      Industry_travel and tourism  Industry_video  \\\n",
       "0                             0.0             0.0   \n",
       "1                             0.0             0.0   \n",
       "2                             0.0             0.0   \n",
       "3                             0.0             0.0   \n",
       "4                             0.0             0.0   \n",
       "...                           ...             ...   \n",
       "1505                          0.0             0.0   \n",
       "1506                          0.0             0.0   \n",
       "1507                          0.0             0.0   \n",
       "1508                          0.0             0.0   \n",
       "1509                          0.0             0.0   \n",
       "\n",
       "      LinkedIn: Average Followers Founders  \\\n",
       "0                              4815.000000   \n",
       "1                              6482.000000   \n",
       "2                                 0.000000   \n",
       "3                              5664.666667   \n",
       "4                                 0.000000   \n",
       "...                                    ...   \n",
       "1505                            202.000000   \n",
       "1506                              0.000000   \n",
       "1507                           2646.666667   \n",
       "1508                              0.000000   \n",
       "1509                           2572.000000   \n",
       "\n",
       "      LinkedIn: Average Connections Founders  \\\n",
       "0                                3914.000000   \n",
       "1                                6479.000000   \n",
       "2                                   0.000000   \n",
       "3                                5337.666667   \n",
       "4                                   0.000000   \n",
       "...                                      ...   \n",
       "1505                              204.000000   \n",
       "1506                                0.000000   \n",
       "1507                             2589.000000   \n",
       "1508                                0.000000   \n",
       "1509                             2091.000000   \n",
       "\n",
       "      LinkedIn: Min Followers Founders  LinkedIn: Max Followers Founders  \\\n",
       "0                               4815.0                            4815.0   \n",
       "1                               6482.0                            6482.0   \n",
       "2                                  0.0                               0.0   \n",
       "3                               2089.0                            9782.0   \n",
       "4                                  0.0                               0.0   \n",
       "...                                ...                               ...   \n",
       "1505                             202.0                             202.0   \n",
       "1506                               0.0                               0.0   \n",
       "1507                            1176.0                            5489.0   \n",
       "1508                               0.0                               0.0   \n",
       "1509                            2572.0                            2572.0   \n",
       "\n",
       "      LinkedIn: Min Connections Founders  LinkedIn: Max Connections Founders  \\\n",
       "0                                 3914.0                              3914.0   \n",
       "1                                 6479.0                              6479.0   \n",
       "2                                    0.0                                 0.0   \n",
       "3                                 1893.0                              9238.0   \n",
       "4                                    0.0                                 0.0   \n",
       "...                                  ...                                 ...   \n",
       "1505                               204.0                               204.0   \n",
       "1506                                 0.0                                 0.0   \n",
       "1507                              1116.0                              5477.0   \n",
       "1508                                 0.0                                 0.0   \n",
       "1509                              2091.0                              2091.0   \n",
       "\n",
       "      Highest Education Bachelor  Highest Education Doctor/PhD  \\\n",
       "0                            0.0                           0.0   \n",
       "1                            0.0                           0.0   \n",
       "2                            0.0                           0.0   \n",
       "3                            0.0                           0.0   \n",
       "4                            0.0                           0.0   \n",
       "...                          ...                           ...   \n",
       "1505                         0.0                           0.0   \n",
       "1506                         0.0                           0.0   \n",
       "1507                         0.0                           0.0   \n",
       "1508                         0.0                           0.0   \n",
       "1509                         0.0                           0.0   \n",
       "\n",
       "      Highest Education Master  International Team  Top University  \\\n",
       "0                          1.0                   0        0.000000   \n",
       "1                          1.0                   1        0.500000   \n",
       "2                          0.0                   1        0.000000   \n",
       "3                          2.0                   1        0.333333   \n",
       "4                          0.0                   1        0.000000   \n",
       "...                        ...                 ...             ...   \n",
       "1505                       1.0                   1        0.000000   \n",
       "1506                       0.0                   1        0.000000   \n",
       "1507                       2.0                   1        0.250000   \n",
       "1508                       0.0                   1        0.000000   \n",
       "1509                       0.0                   0        0.000000   \n",
       "\n",
       "      Studies Abroad Founder  Firsttime Founder Ratio  All Firsttime Founders  \\\n",
       "0                       0.00                 0.000000                     0.0   \n",
       "1                       0.50                 0.000000                     0.0   \n",
       "2                       0.00                 0.000000                     0.0   \n",
       "3                       0.00                 0.333333                     0.0   \n",
       "4                       0.00                 0.000000                     0.0   \n",
       "...                      ...                      ...                     ...   \n",
       "1505                    0.00                 1.000000                     1.0   \n",
       "1506                    0.00                 0.000000                     0.0   \n",
       "1507                    0.25                 1.000000                     1.0   \n",
       "1508                    0.00                 0.000000                     0.0   \n",
       "1509                    0.00                 1.000000                     1.0   \n",
       "\n",
       "      Any Firsttime Founder  Researcher Ratio  All Researchers  \\\n",
       "0                       0.0          0.000000              0.0   \n",
       "1                       0.0          1.000000              1.0   \n",
       "2                       0.0          0.000000              0.0   \n",
       "3                       1.0          0.666667              0.0   \n",
       "4                       0.0          0.000000              0.0   \n",
       "...                     ...               ...              ...   \n",
       "1505                    1.0          0.000000              0.0   \n",
       "1506                    0.0          0.000000              0.0   \n",
       "1507                    1.0          0.333333              0.0   \n",
       "1508                    0.0          0.000000              0.0   \n",
       "1509                    1.0          0.000000              0.0   \n",
       "\n",
       "      Any Researcher  Executive Ratio  All Executives  Any Executive  \\\n",
       "0                0.0         0.000000             0.0            0.0   \n",
       "1                1.0         0.000000             0.0            0.0   \n",
       "2                0.0         0.000000             0.0            0.0   \n",
       "3                1.0         0.666667             0.0            1.0   \n",
       "4                0.0         0.000000             0.0            0.0   \n",
       "...              ...              ...             ...            ...   \n",
       "1505             0.0         0.000000             0.0            0.0   \n",
       "1506             0.0         0.000000             0.0            0.0   \n",
       "1507             1.0         0.333333             0.0            1.0   \n",
       "1508             0.0         0.000000             0.0            0.0   \n",
       "1509             0.0         0.000000             0.0            0.0   \n",
       "\n",
       "      Few Years Experience Ratio  Decade Experience Ratio  \\\n",
       "0                            0.0                 1.000000   \n",
       "1                            0.0                 1.000000   \n",
       "2                            0.0                 0.000000   \n",
       "3                            0.0                 0.666667   \n",
       "4                            0.0                 0.000000   \n",
       "...                          ...                      ...   \n",
       "1505                         0.0                 0.000000   \n",
       "1506                         0.0                 0.000000   \n",
       "1507                         0.0                 1.000000   \n",
       "1508                         0.0                 0.000000   \n",
       "1509                         0.0                 0.000000   \n",
       "\n",
       "      Mid Career Experience Ratio  Avg Gaps in Experience  \\\n",
       "0                        0.000000                0.000000   \n",
       "1                        0.000000                7.000000   \n",
       "2                        0.000000                0.000000   \n",
       "3                        0.333333               15.666667   \n",
       "4                        0.000000                0.000000   \n",
       "...                           ...                     ...   \n",
       "1505                     1.000000               13.000000   \n",
       "1506                     0.000000                0.000000   \n",
       "1507                     0.000000                2.666667   \n",
       "1508                     0.000000                0.000000   \n",
       "1509                     1.000000                0.000000   \n",
       "\n",
       "      Avg Longest Position Duration  Success  \n",
       "0                        150.166667        0  \n",
       "1                         82.233333        0  \n",
       "2                          0.000000        0  \n",
       "3                         99.055556        0  \n",
       "4                          0.000000        0  \n",
       "...                             ...      ...  \n",
       "1505                      56.833333        0  \n",
       "1506                       0.000000        0  \n",
       "1507                     151.888889        0  \n",
       "1508                       0.000000        0  \n",
       "1509                      97.400000        0  \n",
       "\n",
       "[1510 rows x 114 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Number of Founders</th>\n",
       "      <th>Number of Employees</th>\n",
       "      <th>Number of Funding Rounds</th>\n",
       "      <th>Last Funding Amount</th>\n",
       "      <th>Total Funding Amount</th>\n",
       "      <th>Number of Lead Investors</th>\n",
       "      <th>Number of Investors</th>\n",
       "      <th>Number of Acquisitions</th>\n",
       "      <th>X: Followers</th>\n",
       "      <th>X: Following</th>\n",
       "      <th>X: Number of Tweets</th>\n",
       "      <th>X: Account Age Days</th>\n",
       "      <th>X: Tweet Activity</th>\n",
       "      <th>X: Followers Max Growth</th>\n",
       "      <th>X: Followers Max Loss</th>\n",
       "      <th>X: Tweets Max Growth</th>\n",
       "      <th>X: Tweets Max Loss</th>\n",
       "      <th>Months until First Round</th>\n",
       "      <th>Grant Y/N</th>\n",
       "      <th>Made Acquisitions</th>\n",
       "      <th>Project Funding</th>\n",
       "      <th>Startup Funding</th>\n",
       "      <th>Growth Funding</th>\n",
       "      <th>Expansion Funding</th>\n",
       "      <th>Exit Funding</th>\n",
       "      <th>Average Time To Next Round</th>\n",
       "      <th>Average Funding Size</th>\n",
       "      <th>Average Number of Investments by Investors</th>\n",
       "      <th>Average Number of Exits by Investors</th>\n",
       "      <th>Average Number of Lead Investments by Investors</th>\n",
       "      <th>Average Number of Portfolio Organizations by Investors</th>\n",
       "      <th>Investor Country_AUS</th>\n",
       "      <th>Investor Country_GER</th>\n",
       "      <th>Investor Country_SWI</th>\n",
       "      <th>Investor Country_UK</th>\n",
       "      <th>Investor Country_USA</th>\n",
       "      <th>Top Investor Participation</th>\n",
       "      <th>Industry_administrative services</th>\n",
       "      <th>Industry_advertising</th>\n",
       "      <th>Industry_agriculture and farming</th>\n",
       "      <th>Industry_apps</th>\n",
       "      <th>Industry_artificial intelligence (ai)</th>\n",
       "      <th>Industry_biotechnology</th>\n",
       "      <th>Industry_blockchain and cryptocurrency</th>\n",
       "      <th>Industry_clothing and apparel</th>\n",
       "      <th>Industry_commerce and shopping</th>\n",
       "      <th>Industry_community and lifestyle</th>\n",
       "      <th>Industry_consumer electronics</th>\n",
       "      <th>Industry_consumer goods</th>\n",
       "      <th>Industry_content and publishing</th>\n",
       "      <th>Industry_data and analytics</th>\n",
       "      <th>Industry_design</th>\n",
       "      <th>Industry_education</th>\n",
       "      <th>Industry_energy</th>\n",
       "      <th>Industry_events</th>\n",
       "      <th>Industry_financial services</th>\n",
       "      <th>Industry_food and beverage</th>\n",
       "      <th>Industry_gaming</th>\n",
       "      <th>Industry_government and military</th>\n",
       "      <th>Industry_hardware</th>\n",
       "      <th>Industry_health care</th>\n",
       "      <th>Industry_information technology</th>\n",
       "      <th>Industry_internet services</th>\n",
       "      <th>Industry_lending and investments</th>\n",
       "      <th>Industry_manufacturing</th>\n",
       "      <th>Industry_media and entertainment</th>\n",
       "      <th>Industry_messaging and telecommunications</th>\n",
       "      <th>Industry_mobile</th>\n",
       "      <th>Industry_music and audio</th>\n",
       "      <th>Industry_natural resources</th>\n",
       "      <th>Industry_navigation and mapping</th>\n",
       "      <th>Industry_other</th>\n",
       "      <th>Industry_payments</th>\n",
       "      <th>Industry_platforms</th>\n",
       "      <th>Industry_privacy and security</th>\n",
       "      <th>Industry_professional services</th>\n",
       "      <th>Industry_real estate</th>\n",
       "      <th>Industry_sales and marketing</th>\n",
       "      <th>Industry_science and engineering</th>\n",
       "      <th>Industry_social impact</th>\n",
       "      <th>Industry_software</th>\n",
       "      <th>Industry_sports</th>\n",
       "      <th>Industry_sustainability</th>\n",
       "      <th>Industry_transportation</th>\n",
       "      <th>Industry_travel and tourism</th>\n",
       "      <th>Industry_video</th>\n",
       "      <th>LinkedIn: Average Followers Founders</th>\n",
       "      <th>LinkedIn: Average Connections Founders</th>\n",
       "      <th>LinkedIn: Min Followers Founders</th>\n",
       "      <th>LinkedIn: Max Followers Founders</th>\n",
       "      <th>LinkedIn: Min Connections Founders</th>\n",
       "      <th>LinkedIn: Max Connections Founders</th>\n",
       "      <th>Highest Education Bachelor</th>\n",
       "      <th>Highest Education Doctor/PhD</th>\n",
       "      <th>Highest Education Master</th>\n",
       "      <th>International Team</th>\n",
       "      <th>Top University</th>\n",
       "      <th>Studies Abroad Founder</th>\n",
       "      <th>Firsttime Founder Ratio</th>\n",
       "      <th>All Firsttime Founders</th>\n",
       "      <th>Any Firsttime Founder</th>\n",
       "      <th>Researcher Ratio</th>\n",
       "      <th>All Researchers</th>\n",
       "      <th>Any Researcher</th>\n",
       "      <th>Executive Ratio</th>\n",
       "      <th>All Executives</th>\n",
       "      <th>Any Executive</th>\n",
       "      <th>Few Years Experience Ratio</th>\n",
       "      <th>Decade Experience Ratio</th>\n",
       "      <th>Mid Career Experience Ratio</th>\n",
       "      <th>Avg Gaps in Experience</th>\n",
       "      <th>Avg Longest Position Duration</th>\n",
       "      <th>Success</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>30.5</td>\n",
       "      <td>2</td>\n",
       "      <td>7000000.0</td>\n",
       "      <td>7000000.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>31.550000</td>\n",
       "      <td>7000000.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>182.0</td>\n",
       "      <td>220.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4815.000000</td>\n",
       "      <td>3914.000000</td>\n",
       "      <td>4815.0</td>\n",
       "      <td>4815.0</td>\n",
       "      <td>3914.0</td>\n",
       "      <td>3914.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>150.166667</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>30.5</td>\n",
       "      <td>2</td>\n",
       "      <td>3000000.0</td>\n",
       "      <td>3000000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16.433333</td>\n",
       "      <td>3000000.0</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>32.0</td>\n",
       "      <td>49.500000</td>\n",
       "      <td>277.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6482.000000</td>\n",
       "      <td>6479.000000</td>\n",
       "      <td>6482.0</td>\n",
       "      <td>6482.0</td>\n",
       "      <td>6479.0</td>\n",
       "      <td>6479.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>82.233333</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>30.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29.400000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>7.0</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>1</td>\n",
       "      <td>1000000.0</td>\n",
       "      <td>1000000.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>269.0</td>\n",
       "      <td>346.0</td>\n",
       "      <td>248.0</td>\n",
       "      <td>3389.0</td>\n",
       "      <td>0.07</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5664.666667</td>\n",
       "      <td>5337.666667</td>\n",
       "      <td>2089.0</td>\n",
       "      <td>9782.0</td>\n",
       "      <td>1893.0</td>\n",
       "      <td>9238.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>15.666667</td>\n",
       "      <td>99.055556</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>30.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32.433333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1505</th>\n",
       "      <td>1506</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>2</td>\n",
       "      <td>36000.0</td>\n",
       "      <td>161000.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>438.0</td>\n",
       "      <td>169.0</td>\n",
       "      <td>2499.0</td>\n",
       "      <td>0.07</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11.633333</td>\n",
       "      <td>80500.0</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.500000</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>202.000000</td>\n",
       "      <td>204.000000</td>\n",
       "      <td>202.0</td>\n",
       "      <td>202.0</td>\n",
       "      <td>204.0</td>\n",
       "      <td>204.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>56.833333</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1506</th>\n",
       "      <td>1507</td>\n",
       "      <td>3.0</td>\n",
       "      <td>30.5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>34.366667</td>\n",
       "      <td>30000.0</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1507</th>\n",
       "      <td>1508</td>\n",
       "      <td>4.0</td>\n",
       "      <td>30.5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>232.0</td>\n",
       "      <td>482.0</td>\n",
       "      <td>786.0</td>\n",
       "      <td>2467.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>29.0</td>\n",
       "      <td>-42.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21.683333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>58.666667</td>\n",
       "      <td>6.0</td>\n",
       "      <td>23.333333</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2646.666667</td>\n",
       "      <td>2589.000000</td>\n",
       "      <td>1176.0</td>\n",
       "      <td>5489.0</td>\n",
       "      <td>1116.0</td>\n",
       "      <td>5477.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>151.888889</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1508</th>\n",
       "      <td>1509</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>298.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>3456.0</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20.383333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>8.0</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>53.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1509</th>\n",
       "      <td>1510</td>\n",
       "      <td>1.0</td>\n",
       "      <td>30.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.366667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2572.000000</td>\n",
       "      <td>2091.000000</td>\n",
       "      <td>2572.0</td>\n",
       "      <td>2572.0</td>\n",
       "      <td>2091.0</td>\n",
       "      <td>2091.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>97.400000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1510 rows  114 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "6c0f1dc5615ec0d6",
   "metadata": {},
   "source": [
    "## Train-Test-Split"
   ]
  },
  {
   "cell_type": "code",
   "id": "209dc639c5059b2f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T19:18:44.002192Z",
     "start_time": "2025-01-16T19:18:43.933261Z"
    }
   },
   "source": [
    "ID = data['ID']\n",
    "X = data.drop(columns=['Success', 'ID'])\n",
    "y = data['Success']\n",
    "\n",
    "# Train-Test Split\n",
    "X_train, X_test, y_train, y_test, ID_train, ID_test = train_test_split(\n",
    "    X, y, ID, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Apply SMOTE on the training data\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "additional_ids = len(y_train_resampled) - len(ID_train)\n",
    "\n",
    "new_ids = np.arange(len(ID_train), len(ID_train) + additional_ids)\n",
    "\n",
    "ID_train_resampled = np.concatenate([ID_train, new_ids])\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_resampled)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "85d6efb7",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ee23bf",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "id": "fab62c93",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T19:18:44.070470Z",
     "start_time": "2025-01-16T19:18:44.053633Z"
    }
   },
   "source": [
    "# Logistic Regression\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning) # No influence on the result and just for a cleaner output\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning) # No influence on the result and just for a cleaner output\n",
    "logistic_model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "logistic_model.fit(X_train_scaled, y_train_resampled)\n",
    "logistic_predictions = logistic_model.predict(X_test_scaled)\n",
    "logistic_proba = logistic_model.predict_proba(X_test_scaled)[:, 1]\n",
    "logistic_auc = roc_auc_score(y_test, logistic_proba)\n",
    "logistic_report = classification_report(y_test, logistic_predictions, output_dict=True)\n",
    "\n",
    "print('Test AUC:', logistic_auc)\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test, logistic_predictions))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test AUC: 0.7910131090399333\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.76      0.84       249\n",
      "           1       0.39      0.70      0.50        53\n",
      "\n",
      "    accuracy                           0.75       302\n",
      "   macro avg       0.65      0.73      0.67       302\n",
      "weighted avg       0.83      0.75      0.78       302\n",
      "\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "153478c3",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "id": "217fb1c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T19:21:40.457677Z",
     "start_time": "2025-01-16T19:18:44.137313Z"
    }
   },
   "source": [
    "# Gradient Boosting\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'learning_rate': [0.03, 0.035],  \n",
    "    'n_estimators': [400, 450],  \n",
    "    'max_depth': [5, 6],  \n",
    "    'min_samples_split': [8, 10],  \n",
    "    'min_samples_leaf': [6, 7],  \n",
    "    'subsample': [0.6, 0.7],  \n",
    "    'max_features': ['sqrt', 'log2'],  \n",
    "}\n",
    "\n",
    "# Initialize the Gradient Boosting Classifier\n",
    "gb_model = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Define the scoring metric\n",
    "scorer = make_scorer(roc_auc_score, needs_proba=True)\n",
    "\n",
    "# Set up GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=gb_model,\n",
    "    param_grid=param_grid,\n",
    "    scoring=scorer,\n",
    "    cv=10,  # Stratified K-Fold Cross-Validation\n",
    "    verbose=2,\n",
    "    n_jobs=-1  # Use all available cores\n",
    ")\n",
    "\n",
    "# Fit GridSearchCV to the data\n",
    "grid_search.fit(X_train_scaled, y_train_resampled)\n",
    "\n",
    "# Get the best parameters and score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print(\"Best Parameters:\", best_params)\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "best_gb_model = grid_search.best_estimator_\n",
    "test_predictions = best_gb_model.predict(X_test_scaled)\n",
    "test_auc = roc_auc_score(y_test, best_gb_model.predict_proba(X_test_scaled)[:, 1])\n",
    "\n",
    "print(\"Test AUC Score:\", test_auc)\n",
    "print(\"Classification Report on Test Set:\")\n",
    "print(classification_report(y_test, test_predictions))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 128 candidates, totalling 1280 fits\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.7s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   0.7s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   0.7s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.6s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.6s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   0.7s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   0.7s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.7s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   0.7s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.6s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.6s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   0.7s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   0.7s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.4s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.4s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.2s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.4s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.3s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.4s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.5s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.2s[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.7s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.6s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.6s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.7s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   0.7s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.6s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.6s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   0.7s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.6s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   0.7s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   0.7s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   0.7s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.5s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.5s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.7s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.2s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.4s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.5s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.4s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.2s[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.2s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.7s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.7s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.7s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   0.7s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   0.7s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   0.7s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.7s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   0.7s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.6s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   0.7s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   0.7s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.4s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.3s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.5s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.2s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.5s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.5s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.4s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.3s[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.7s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   0.7s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   0.7s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.7s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   0.7s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   0.7s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   0.7s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.7s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   0.7s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   0.7s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   0.7s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.6s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   0.7s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   0.7s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   0.7s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.4s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.2s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.6s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.4s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.2s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.4s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.2s[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.2s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.6s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   0.7s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.6s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   0.6s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.7s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   0.7s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   0.7s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   0.7s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.6s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   0.7s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   0.7s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.3s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.6s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.4s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.4s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.5s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.3s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.3s[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.7s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   0.7s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   0.6s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   0.7s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.7s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   0.7s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   0.7s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.7s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   0.7s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.7s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   0.7s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   0.7s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.4s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.3s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.7s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.2s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.5s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.4s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.4s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.3s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.2s[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.7s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.7s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   0.7s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.7s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.7s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   0.7s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.6s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   0.7s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   0.7s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.4s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.3s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.5s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.5s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.5s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.4s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.4s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.3s[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.2s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.7s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.7s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.6s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.6s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   0.7s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.7s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.7s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.7s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   0.7s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   0.7s\n",
      "[CV] END learning_rate=0.03, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.2s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.5s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.3s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.6s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.2s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.5s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.5s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.3s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.4s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.2s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.8s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.6s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.5s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.3s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.4s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.6s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.7s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.5s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.2s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.7s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.7s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   2.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.4s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.4s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.4s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.4s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.7s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.4s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.3s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.4s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.7s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.8s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   2.2s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.3s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.4s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.4s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.7s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.6s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   2.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   2.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.2s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.5s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.7s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   0.7s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.4s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.8s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.9s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.4s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.3s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.2s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.2s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.4s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.5s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   2.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   2.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.3s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.2s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.7s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   0.8sBest Parameters: {'learning_rate': 0.035, 'max_depth': 6, 'max_features': 'sqrt', 'min_samples_leaf': 7, 'min_samples_split': 8, 'n_estimators': 450, 'subsample': 0.6}\n",
      "Test AUC Score: 0.8320830491778435\n",
      "Classification Report on Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.95      0.93       249\n",
      "           1       0.70      0.53      0.60        53\n",
      "\n",
      "    accuracy                           0.88       302\n",
      "   macro avg       0.80      0.74      0.76       302\n",
      "weighted avg       0.87      0.88      0.87       302\n",
      "\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "2522117d",
   "metadata": {},
   "source": [
    "### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "id": "116d54aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T19:42:35.591271Z",
     "start_time": "2025-01-16T19:21:40.689962Z"
    }
   },
   "source": [
    "# LightGBM\n",
    "# Define the hyperparameter grid for LightGBM\n",
    "param_grid_lgb = {\n",
    "    'learning_rate': [0.02, 0.025],          \n",
    "    'n_estimators': [300, 350],             \n",
    "    'min_child_samples': [4, 6, 8],        \n",
    "    'num_leaves': [15, 20],                 \n",
    "    'subsample': [0.5, 0.55],               \n",
    "    'colsample_bytree': [0.6],             \n",
    "    'reg_alpha': [0.5, 1.0],                \n",
    "    'reg_lambda': [3.0],                   \n",
    "    'scale_pos_weight': [1.0, 1.5, 2.0],    \n",
    "}\n",
    "# Initialize the LightGBM Classifier\n",
    "lgb_model = LGBMClassifier(random_state=42)\n",
    "\n",
    "# Set up GridSearchCV for LightGBM\n",
    "grid_search_lgb = GridSearchCV(\n",
    "    estimator=lgb_model,\n",
    "    param_grid=param_grid_lgb,\n",
    "    scoring=scorer,  # Use AUC as the scoring metric\n",
    "    cv=5,  \n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit GridSearchCV to the data\n",
    "grid_search_lgb.fit(X_train_scaled, y_train_resampled)\n",
    "\n",
    "# Get the best parameters and score for LightGBM\n",
    "best_params_lgb = grid_search_lgb.best_params_\n",
    "best_score_lgb = grid_search_lgb.best_score_\n",
    "\n",
    "print(\"Best Parameters (LightGBM):\", best_params_lgb)\n",
    "\n",
    "# Evaluate the best LightGBM model on the test set\n",
    "best_lgb_model = grid_search_lgb.best_estimator_\n",
    "lgb_predictions = best_lgb_model.predict(X_test_scaled)\n",
    "lgb_proba = best_lgb_model.predict_proba(X_test_scaled)[:, 1]\n",
    "lgb_test_auc = roc_auc_score(y_test, lgb_proba)\n",
    "\n",
    "print(\"Test AUC Score (LightGBM):\", lgb_test_auc)\n",
    "print(\"Classification Report (LightGBM):\\n\", classification_report(y_test, lgb_predictions))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 288 candidates, totalling 1440 fits\n",
      "\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.5s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.7s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   2.3s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.5s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.4s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.8s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.6s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   2.5s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.2s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.7s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.7s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.4s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.4s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.5s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.4s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.4s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.5s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.6s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.6s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   2.8s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.7s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.6s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.7s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.4s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.5s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.4s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.4s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.4s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.6s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.5s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.5s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   2.6s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.8s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.8s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.5s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   0.8s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011593 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.7s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   0.7s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.7s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.4s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.4s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.5s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.5s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.5s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.4s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.4s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.4s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.5s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.5s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   2.4s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.8s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.8s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.6s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.3s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006222 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.7s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.4s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.5s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.5s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.5s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.5s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.6s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.5s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.6s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.5s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   2.7s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.8s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.5s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.7s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   0.7s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.7s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.4s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.4s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.5s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.5s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.6s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.5s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.4s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.7s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.8s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   2.3s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.6s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.6s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   0.7s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.7s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.4s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.5s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.5s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.4s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.5s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.5s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.5s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   2.0s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   2.1s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.7s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.5s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.7s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.4s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.4s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.4s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.4s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.4s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.6s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.6s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.5s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   2.7s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.7s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.8s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.5s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.7s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.7s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=5, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.4s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.5s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.4s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.5s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.4s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.5s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.5s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.5s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=sqrt, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   2.7s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.6; total time=   0.8s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=400, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.6; total time=   1.6s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.6s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=8, n_estimators=450, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=400, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.1s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.6; total time=   1.2s\n",
      "[CV] END learning_rate=0.035, max_depth=6, max_features=log2, min_samples_leaf=7, min_samples_split=10, n_estimators=450, subsample=0.7; total time=   0.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005417 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008109 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006200 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   3.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005344 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010723 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.023057 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005779 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010084 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008028 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005602 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008498 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006585 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.038261 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   3.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005086 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   3.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034906 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   3.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007152 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   3.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.082968 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   3.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004509 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   3.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006461 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   3.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005194 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007163 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005316 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010665 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   3.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008828 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   3.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.087016 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005682 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   3.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004564 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   3.3s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006460 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   3.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009375 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   3.5s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008783 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   3.5s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005282 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012314 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.6s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003687 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005631 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005712 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010619 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   3.5s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008434 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007793 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   3.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005050 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   3.6s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005346 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   3.5s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007084 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005206 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   3.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009139 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   3.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004559 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   5.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009543 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005064 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006481 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010818 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005956 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   3.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004850 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005420 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   3.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.023623 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   3.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022911 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   3.6s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019359 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   3.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005226 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032033 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013295 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.045453 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004891 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009946 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004684 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004105 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   3.6s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004738 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   3.0s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008034 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   3.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003207 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   3.9s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006819 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   3.6s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005082 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   3.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005474 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.6s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004896 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005042 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006224 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.119011 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006305 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   3.8s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004801 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   3.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005090 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   3.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006063 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   3.6s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006434 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   3.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005290 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   3.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005571 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.9s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005583 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005104 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004772 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005738 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.066710 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008156 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   3.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004175 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   3.8s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009568 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   3.6s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005695 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   3.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004554 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006484 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.045808 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005675 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004812 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005317 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006513 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.9s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004645 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017893 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007903 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004953 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.048296 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   3.8s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010844 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004690 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006621 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006060 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005492 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006135 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005585 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   5.8s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014861 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.079345 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007339 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005103 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004914 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014100 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006455 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015963 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004751 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   3.8s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005489 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005615 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005292 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   5.1s\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   5.0s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005760 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.073120 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004873 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004754 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003991 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.066904 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015600 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004391 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.055956 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006384 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011187 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005473 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011798 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010331 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013469 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004949 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.0s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004875 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005404 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003911 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012723 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002931 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004613 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007088 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   5.0s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027196 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006317 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005868 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005243 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.9s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005788 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005513 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006830 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.0s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005858 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005543 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005578 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005955 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007824 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   5.5s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005662 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   5.6s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.107374 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.4s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013675 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004527 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.040298 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018294 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005858 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006528 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010670 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004485 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005528 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   3.9s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004536 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   5.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006786 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   5.8s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005811 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004977 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005547 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024524 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.128248 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007476 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   3.9s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024363 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   3.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005234 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006991 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   3.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005509 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009970 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009042 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   5.9s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.057506 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005463 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005557 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015967 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006328 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   3.9s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005379 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010475 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005093 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009157 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004529 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   3.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.026435 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   6.0s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006032 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   5.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007927 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   5.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009263 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   5.5s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021600 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   5.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014139 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   5.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011037 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005468 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   5.9s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006612 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   5.0s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006793 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   3.6s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013447 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   3.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006714 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004507 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008550 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   3.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.083683 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   5.1s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023084 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   5.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006298 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   5.5s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004961 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   5.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009971 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   5.2s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006741 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   5.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006717 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   3.3s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005629 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   3.6s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007422 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005169 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005286 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005164 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.028050 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   5.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005511 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   5.6s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007469 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   5.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005780 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   5.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004529 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   5.0s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006016 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005719 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   3.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005945 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005538 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   3.8s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007786 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   3.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005750 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   3.6s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005265 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020462 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   5.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.129144 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   6.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004411 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   5.9s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005166 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   5.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005562 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   5.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004601 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   3.3s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008432 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003367 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   3.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004569 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   3.4s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017053 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.049164 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004992 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.2s\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   5.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004776 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   5.1s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002449 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   5.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004680 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   5.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005492 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   5.5s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005376 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007962 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   5.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.053066 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   3.3s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.103042 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   3.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006026 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   3.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004823 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   3.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007263 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006599 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   5.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005755 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   6.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004975 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   5.8s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009814 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   5.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006012 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   5.4s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005600 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018164 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   6.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006692 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   3.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005703 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   3.6s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005572 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   3.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.129169 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   3.9s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018466 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007005 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017556 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   5.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003836 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006512 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   6.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.024426 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   5.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005257 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   5.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019137 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005444 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   3.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.079561 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   3.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005304 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   3.2s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005388 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   3.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020009 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.0s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004823 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   3.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005063 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   6.0s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007464 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   5.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006274 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   6.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.034473 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   5.6s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006524 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   5.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012890 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   5.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006802 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   3.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005325 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   3.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005882 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   3.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019648 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004951 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006677 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007253 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   3.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006470 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004821 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007759 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   6.3s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.045653 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   5.0s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008173 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   5.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.026153 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   5.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007060 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005813 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020963 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004063 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015101 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005779 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006987 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   3.8s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008566 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005512 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   5.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007861 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   5.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004469 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.037933 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.4s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005722 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   5.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011559 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.5s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005235 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   5.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006015 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.016392 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007551 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007867 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006738 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   5.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006207 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   6.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012660 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   5.4s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.101733 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027455 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005914 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011666 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010946 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009258 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   3.9s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017716 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007942 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011812 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005867 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   6.1s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.077148 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   6.0s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005349 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   5.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004388 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.6s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006492 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.9s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004629 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005912 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   5.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005655 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.9s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006909 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   3.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008568 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.056389 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006461 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.4s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009564 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   6.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015910 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   6.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.035918 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.5s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015584 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005155 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   5.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005916 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011988 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   5.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007174 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004710 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.5s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011410 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007788 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005991 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008310 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   6.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009381 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   5.8s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.026245 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   5.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005878 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.5s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.042251 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004225 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   5.0s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006324 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004937 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.4s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.027873 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.5s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005536 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004638 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011076 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009944 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.4s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005174 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   6.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007198 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   5.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017468 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   5.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005932 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   5.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004694 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004247 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005318 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.9s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008091 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.039253 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013894 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   5.0s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006040 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   5.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015697 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   6.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007459 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   5.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.067800 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005492 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006421 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   5.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007826 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   5.0s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005584 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004872 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.091494 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009988 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.5s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014858 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006595 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008341 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007015 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.0s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004145 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005175 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   5.1s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006060 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   5.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005120 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   5.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005090 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010716 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   5.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006421 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   5.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005767 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   6.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011133 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005216 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009335 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005648 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.4s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004928 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007080 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.4s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.102629 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005322 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   5.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005194 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006032 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   5.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028688 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   5.1s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006320 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   5.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005008 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005185 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005285 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007434 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006210 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.4s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007375 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006603 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   5.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005769 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   5.9s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032853 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006706 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   5.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007280 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   5.5s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.127374 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   5.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009746 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005546 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.0s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.087467 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005004 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   3.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007079 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029722 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   5.1s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009999 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   5.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020873 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006586 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   5.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004672 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   5.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005102 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   5.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006597 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   5.6s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047251 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.6s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007141 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005297 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004517 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.046939 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006078 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   5.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006727 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013168 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.9s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004550 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   5.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007228 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   5.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006994 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   5.6s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005158 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   5.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006679 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   3.9s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003858 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005785 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.5s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006241 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005715 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007815 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   5.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010778 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   5.6s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005073 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   5.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004375 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   5.6s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004440 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   5.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007522 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   5.9s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011249 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   5.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018799 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005202 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005499 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011064 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.098497 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   3.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008532 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006383 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   5.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004380 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   6.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005755 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   5.1s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007013 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   5.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007020 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   5.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029112 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   5.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005941 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   5.7s\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007290 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008365 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014429 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   3.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005358 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   5.6s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006602 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   5.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015869 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   5.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.044386 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   5.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006966 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   5.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009697 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   5.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005322 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   5.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005711 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005551 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005443 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   3.2s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.058284 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   3.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004507 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   3.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005171 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   3.1s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006025 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   3.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005177 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   3.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005192 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   3.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005336 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.0s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.088965 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011400 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.090551 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   5.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027387 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020233 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   3.3s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004807 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   3.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031730 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   3.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007560 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   3.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020611 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   3.6s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.016568 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   3.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004819 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   3.2s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013865 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007876 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005523 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004796 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   5.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006227 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   5.5s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.086650 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017802 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   3.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006678 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   3.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007729 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   3.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005335 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   3.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007972 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   3.5s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.034242 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   3.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.023619 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017291 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004886 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004582 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   5.1s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004562 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005633 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   3.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006099 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   3.3s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004524 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   3.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.128682 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   3.9s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005617 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014697 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   3.8s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005219 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005509 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006289 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004353 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.4s\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   5.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008819 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030430 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   3.2s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004625 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   3.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004386 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   3.1s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005574 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   3.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014893 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   3.3s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007242 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   3.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007295 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   3.5s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035081 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004769 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   5.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.072864 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   5.0s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005835 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   3.2s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004779 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   3.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005196 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   3.1s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013774 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   3.8s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011924 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   3.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006002 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006523 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   3.8s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007410 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   3.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.056238 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006038 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005505 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.053652 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.9s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.117641 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008495 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   3.5s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004484 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   3.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005648 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   3.5s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005674 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   3.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006543 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   3.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005482 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   3.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005825 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   3.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005557 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   5.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012968 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.9s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010688 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006456 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004839 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   3.1s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006766 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   3.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005572 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   3.6s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004849 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   3.6s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006590 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   3.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005525 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   3.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008337 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005187 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   5.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006790 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.9s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006243 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   5.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004787 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005225 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004599 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010829 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   5.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006385 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.9s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005460 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004903 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007626 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004971 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   3.8s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005119 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022671 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004840 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005548 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time= 2.9min\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006959 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.1s\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.065513 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007748 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.6s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047236 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024296 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006885 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   3.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005080 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.4s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005130 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   3.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005664 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.5s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005994 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   5.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005675 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013356 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time= 2.9min\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010616 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005635 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.9s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.027430 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005006 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008562 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006088 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.027408 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.4s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006653 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004457 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006080 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.4s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008345 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time= 2.9min\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003874 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005954 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019904 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004434 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005421 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005607 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005861 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005227 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006683 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022409 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006022 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005195 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time= 2.9min\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005680 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005024 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   5.5s\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004899 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   5.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009974 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004664 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011003 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005136 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005384 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010035 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.4s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008653 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005634 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time= 2.9min\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010723 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005073 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   5.5s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007467 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006259 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005993 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015532 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008276 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005745 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006642 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005583 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   5.2s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011078 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time= 2.9min\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002706 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006399 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004882 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   5.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005083 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009523 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047157 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   5.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004865 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.4s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006607 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   3.6s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005462 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006583 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.039637 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   5.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.024708 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.074087 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time= 2.9min\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005212 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.0s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004665 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005244 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   5.5s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006025 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.5s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005071 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.0s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005349 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005851 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011529 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022315 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012916 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   5.0s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017388 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004974 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time= 2.9min\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017199 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033121 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   5.1s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004559 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005971 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004199 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   5.1s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011486 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   5.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007840 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   5.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005908 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   5.3s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005588 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   5.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007171 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   5.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004793 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   5.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004854 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061737 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   3.2s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.128014 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   3.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016993 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   3.3s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007562 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.081587 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   5.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006147 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   5.6s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.131031 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   5.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006538 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   5.8s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007812 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   5.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005064 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   5.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005540 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   5.4s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004545 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   3.1s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021784 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   3.6s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005203 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009985 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005058 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   5.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005574 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   5.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.034204 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   5.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.039545 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   5.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005259 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   5.5s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007792 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   5.6s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005437 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   5.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.036666 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   3.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005131 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   3.2s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.016677 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012009 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   3.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006314 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.9s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007672 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   5.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004072 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   5.5s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012217 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   5.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010257 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   5.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004693 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004107 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   5.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006582 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   5.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013996 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   3.2s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004778 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   3.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005177 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.5s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015770 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   3.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005442 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   3.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005582 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004871 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.9s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006004 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   5.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006427 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   6.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005993 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   5.5s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005751 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   5.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005360 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   5.9s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007989 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   5.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004837 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   3.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008351 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   3.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.045056 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   3.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017408 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   3.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008270 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   3.3s\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   5.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031376 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   5.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007336 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   5.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019570 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   5.4s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015206 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   5.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027901 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   5.1s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005915 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   3.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008943 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   3.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005942 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   3.6s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004844 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004759 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   3.4s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007810 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005584 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   5.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006852 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   5.6s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.027452 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   5.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005084 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   5.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015506 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004724 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   5.6s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005279 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   3.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004999 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   3.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005856 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   3.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008716 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004794 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   3.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022970 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   3.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032614 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   5.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005217 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004343 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   6.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007353 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   5.4s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004678 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.02, min_child_samples=8, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   5.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006567 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   3.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005446 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   3.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005119 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   3.8s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008159 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009100 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005423 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   3.4s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.016561 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   3.5s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014271 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006531 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   3.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007477 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   3.5s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006137 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   3.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004717 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.4s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007532 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006293 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006328 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013976 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005907 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.129263 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   5.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029458 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011907 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   3.8s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016244 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   3.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.079992 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   3.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009649 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   3.3s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012268 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004982 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005539 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008686 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005663 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005029 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004881 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   5.0s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005379 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009193 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   3.8s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005413 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   3.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006574 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   3.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005812 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.6s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005545 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.057482 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.4s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008779 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005449 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006129 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007046 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005672 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005914 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005028 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005256 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   3.2s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005392 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   3.8s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004444 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031828 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.9s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004588 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014304 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   5.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005576 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008836 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014885 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008522 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   5.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.038843 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   3.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017974 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   3.8s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005118 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   3.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022973 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   5.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005418 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006010 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005197 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.045415 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004064 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.4s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.074145 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005279 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.9s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006368 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005169 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011121 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004857 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   5.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005998 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.9s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005091 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005726 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.026301 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   5.2s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019026 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013774 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007079 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005369 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   3.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.034124 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.5s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006360 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   3.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013509 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006228 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009481 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014719 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005642 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.054048 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   5.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006607 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004841 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   3.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008860 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006088 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004842 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007597 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   3.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005771 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.9s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006432 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   5.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005854 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006643 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037090 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005050 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005882 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006898 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012718 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007346 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020229 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   3.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005206 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   3.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005290 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006369 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007837 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   3.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005032 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006981 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007157 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time= 1.1min\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008352 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.119647 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   5.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005112 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   5.5s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004758 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   5.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006381 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   5.8s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006998 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005833 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.4s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005016 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   3.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004916 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.048509 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006328 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time= 1.1min\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006032 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.4s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004602 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   5.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.025540 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   5.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005714 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   5.6s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005409 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018541 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   5.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019348 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   3.8s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005765 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005853 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.024799 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006048 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time= 1.1min\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004576 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   3.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008616 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   5.5s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.046119 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008054 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   5.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005964 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   5.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010543 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   5.6s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006193 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   6.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018801 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013090 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   3.8s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006021 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005621 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.5s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004955 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time= 1.1min\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005002 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.050663 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   5.6s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004380 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003941 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005197 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   5.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006906 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   5.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005337 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   6.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.049370 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   5.0s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004682 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.024438 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008874 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004605 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time= 1.1min\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004066 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.4s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007342 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   5.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.044320 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   5.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007108 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   5.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005626 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   5.6s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004296 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021476 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   6.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005006 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   5.4s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.026480 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005606 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011544 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   3.8s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006456 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time= 1.1min\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006874 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   5.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005730 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   5.4s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031798 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.074555 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   5.3s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005954 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022977 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   6.2s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.054669 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   5.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005632 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   5.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012408 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018929 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006038 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time= 1.1min\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020411 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005908 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   5.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009480 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   5.1s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006416 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   5.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022274 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   5.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004933 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031267 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   6.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006959 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   5.9s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011655 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   3.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007438 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.024652 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007546 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time= 1.1min\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008051 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   5.8s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005199 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   5.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005583 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   5.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006165 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   5.9s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014091 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   5.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005502 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   6.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007234 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060807 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   5.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005852 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   5.5s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004794 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   5.5s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007189 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   5.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024068 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   3.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007308 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   3.2s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010006 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   3.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007583 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005862 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   3.5s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007378 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   3.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012223 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   3.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005293 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008313 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   6.1s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006106 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11322\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   5.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035013 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008132 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   3.6s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013712 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005494 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   3.2s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005363 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   3.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007188 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   3.1s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004442 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   3.6s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014984 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   3.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008100 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012184 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   5.0s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016275 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11570\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   5.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.131123 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   3.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005209 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   3.8s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007105 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012360 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   3.6s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004862 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   3.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006029 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   3.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008592 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   3.5s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032830 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.9s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004848 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006513 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   5.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005135 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.051474 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   3.8s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017203 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   3.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012822 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   3.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005227 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   3.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005111 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   3.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013641 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   3.1s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005550 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   3.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011596 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   5.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017845 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008383 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   5.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006006 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010979 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005937 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=4, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   5.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.045153 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   3.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005699 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   3.5s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004746 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   3.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019423 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   3.2s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011112 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   3.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005154 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006377 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   3.5s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005679 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010486 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007517 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.056371 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.1s\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   3.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004246 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006225 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   3.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005749 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   3.0s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006585 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005857 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   3.6s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019218 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   3.6s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005842 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   3.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008958 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005702 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   5.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007301 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004711 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014376 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   3.1s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.043132 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006228 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   3.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008596 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   3.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006361 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   3.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006418 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   3.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004181 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   3.2s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004913 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005522 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006074 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011437 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.5s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013600 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005106 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010079 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   3.6s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005894 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   3.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004662 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   3.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004924 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   3.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006269 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   3.5s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014054 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   3.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006060 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   5.1s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005713 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006894 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006262 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005505 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   5.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007309 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013808 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005945 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004417 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009450 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   5.0s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022147 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006920 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006407 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   3.6s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005746 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017901 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005590 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004587 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005422 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.4s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006694 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005195 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009995 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.6s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006308 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006018 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.036531 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005251 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003851 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008016 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006018 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005627 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005263 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006296 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011634 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.4s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007003 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.045507 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.026131 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006049 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005994 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005261 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007244 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012813 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006596 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time= 3.0min\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011522 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.025930 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004761 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.058388 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.4s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006785 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.023635 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021656 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006216 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005550 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003682 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005314 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time= 3.0min\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008882 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=  39.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.144035 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   5.0s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004941 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006233 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005715 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.070397 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   3.6s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004627 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005411 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005409 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.4s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005198 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005656 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003475 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time= 3.0min\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009080 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=  40.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.316488 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006828 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005860 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.034260 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004849 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004476 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006281 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007034 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006311 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024514 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time= 3.0min\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005365 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   5.8s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009638 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=  40.5s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005036 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005739 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004897 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005149 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009178 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005966 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.4s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005278 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017850 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005704 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011803 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005259 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time= 3.0min\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004838 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   5.8s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020502 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004901 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006372 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005035 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013635 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012338 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   3.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012325 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005365 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   3.8s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006049 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012228 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time= 3.0min\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011277 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   5.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006006 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=  40.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007427 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032502 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008065 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time= 3.0min\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004185 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   5.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009531 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=  40.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004395 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005835 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   5.0s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004387 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   5.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016073 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.9s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005624 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   5.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010625 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   5.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009936 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   3.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004672 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   3.4s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005357 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   3.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005792 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time= 3.0min\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015808 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=  39.9s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010647 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   5.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008148 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   5.3s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008103 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   5.6s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.040708 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   5.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008330 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   5.1s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010930 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   5.6s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004430 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   3.0s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.051111 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   3.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006606 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030323 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.9s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006173 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=  40.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007039 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   5.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.073485 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   5.1s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008733 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   5.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011046 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   5.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007518 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   5.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005396 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020007 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   3.6s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004376 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   3.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004890 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   3.9s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004840 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   3.6s\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   5.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006038 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   5.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033989 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   5.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.044835 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   5.8s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005556 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   5.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.129502 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   5.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004710 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   3.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007179 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   3.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.027673 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   3.9s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.051549 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   3.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012431 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   3.1s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005184 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   3.6s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   6.0s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.053541 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   5.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005957 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   5.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006424 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   5.5s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006598 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   5.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005745 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   5.8s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007523 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   3.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.028696 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   3.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007180 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008058 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   3.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005045 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   3.3s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006126 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   3.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004873 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   5.8s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.093498 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   5.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005534 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   5.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041413 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   5.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006050 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005438 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   5.4s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012495 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   3.2s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007995 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   3.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006071 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   3.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.048561 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004541 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   3.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007419 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   3.8s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.033291 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   5.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004329 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   5.8s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007153 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   5.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004841 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006518 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   5.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005262 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006074 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   3.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005281 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   3.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005940 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007243 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   3.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.042739 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   3.4s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006160 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   3.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004644 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=  39.6s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006214 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   6.0s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004660 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   6.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004874 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11506\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   5.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010885 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   5.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007897 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=6, n_estimators=350, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   5.5s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005552 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004976 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   3.5s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005553 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   3.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005189 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   3.9s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005689 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   3.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007329 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   3.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.068776 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   3.6s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005201 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   3.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017729 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023769 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   3.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004895 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   3.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005197 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   3.3s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036912 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006989 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   5.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005537 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.9s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.069767 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003884 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.4s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013457 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.9s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005354 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   3.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004852 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   3.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005813 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   3.5s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004944 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   3.9s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009460 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   3.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005884 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.3s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005655 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005360 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004739 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   5.1s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.040448 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   5.0s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006764 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004946 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007842 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.043489 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   3.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005050 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   3.4s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005111 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   3.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010200 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008215 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004363 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006332 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   4.6s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012193 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004796 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.9s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021945 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006994 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008412 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12848\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   3.9s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004638 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   3.1s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019013 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=15, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.5; total time=   3.6s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036891 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006340 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.067268 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006540 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.7s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005482 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.5; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005842 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   4.8s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005484 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.55; total time=   4.7s\n",
      "[LightGBM] [Info] Number of positive: 797, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008178 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11564\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499687 -> initscore=-0.001254\n",
      "[LightGBM] [Info] Start training from score -0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=300, num_leaves=20, reg_alpha=1.0, reg_lambda=3.0, scale_pos_weight=2.0, subsample=0.55; total time=   4.2s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005903 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1595, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001254\n",
      "[LightGBM] [Info] Start training from score 0.001254\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.0, subsample=0.55; total time=   4.0s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005139 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11675\n",
      "[LightGBM] [Info] Number of data points in the train set: 1596, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.025, min_child_samples=8, n_estimators=350, num_leaves=15, reg_alpha=0.5, reg_lambda=3.0, scale_pos_weight=1.5, subsample=0.5; total time=   3.6s\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 797[LightGBM] [Info] Number of positive: 997, number of negative: 997\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001877 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12978\n",
      "[LightGBM] [Info] Number of data points in the train set: 1994, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Best Parameters (LightGBM): {'colsample_bytree': 0.6, 'learning_rate': 0.025, 'min_child_samples': 6, 'n_estimators': 350, 'num_leaves': 20, 'reg_alpha': 0.5, 'reg_lambda': 3.0, 'scale_pos_weight': 1.5, 'subsample': 0.5}\n",
      "Test AUC Score (LightGBM): 0.8432977191786013\n",
      "Classification Report (LightGBM):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.95      0.93       249\n",
      "           1       0.70      0.57      0.62        53\n",
      "\n",
      "    accuracy                           0.88       302\n",
      "   macro avg       0.80      0.76      0.78       302\n",
      "weighted avg       0.87      0.88      0.88       302\n",
      "\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "02c3567d",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "id": "afb9fdf4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T19:42:43.393243Z",
     "start_time": "2025-01-16T19:42:35.682724Z"
    }
   },
   "source": [
    "# Neural Network\n",
    "nn_model = Sequential([\n",
    "    Dense(256, kernel_regularizer=regularizers.l2(0.01), input_dim=X_train_scaled.shape[1], kernel_initializer=HeNormal(seed=42)),\n",
    "    LeakyReLU(alpha=0.1),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.1),\n",
    "\n",
    "    Dense(128, kernel_regularizer=regularizers.l2(0.01), kernel_initializer=HeNormal(seed=42)),\n",
    "    LeakyReLU(alpha=0.1),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.1),\n",
    "\n",
    "    Dense(64, kernel_regularizer=regularizers.l2(0.01), kernel_initializer=HeNormal(seed=42)),\n",
    "    LeakyReLU(alpha=0.1),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.1),\n",
    "\n",
    "    Dense(32, kernel_regularizer=regularizers.l2(0.01), kernel_initializer=HeNormal(seed=42)),\n",
    "    LeakyReLU(alpha=0.1),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.1),\n",
    "\n",
    "    Dense(1, activation='sigmoid', kernel_initializer=HeNormal(seed=42))\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "nn_model.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy', 'AUC'])\n",
    "\n",
    "# Train the model with class weights\n",
    "class_weights = {0: 1.0, 1: 3.0}\n",
    "nn_model.fit(X_train_scaled, y_train_resampled, epochs=100, batch_size=32, verbose=0, shuffle=False, class_weight=class_weights)\n",
    "\n",
    "# Predictions with optimized threshold\n",
    "y_probs = nn_model.predict(X_test_scaled).flatten()\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_probs)\n",
    "optimal_idx = np.argmax(tpr - fpr)\n",
    "optimal_threshold = thresholds[optimal_idx]\n",
    "\n",
    "nn_predictions = (y_probs > optimal_threshold).astype(int)\n",
    "nn_report = classification_report(y_test, nn_predictions, output_dict=True)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Optimal Threshold:\", optimal_threshold)\n",
    "print(\"Test AUC Score (Neural Network):\", roc_auc_score(y_test, y_probs))\n",
    "print(\"Classification Report (Neural Network):\\n\", classification_report(y_test, nn_predictions))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m10/10\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step \n",
      "Optimal Threshold: 0.642708\n",
      "Test AUC Score (Neural Network): 0.7288777752519512\n",
      "Classification Report (Neural Network):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.86      0.88       249\n",
      "           1       0.46      0.55      0.50        53\n",
      "\n",
      "    accuracy                           0.81       302\n",
      "   macro avg       0.68      0.71      0.69       302\n",
      "weighted avg       0.82      0.81      0.81       302\n",
      "\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "id": "7a7ea580",
   "metadata": {},
   "source": [
    "### Weighted Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "id": "56c8dc27",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T19:42:43.433457Z",
     "start_time": "2025-01-16T19:42:43.397182Z"
    }
   },
   "source": [
    "# Weighted Ensemble \n",
    "ensemble_proba_weighted = (\n",
    "    0.35 * best_gb_model.predict_proba(X_test_scaled)[:, 1] +\n",
    "    0.35 * best_lgb_model.predict_proba(X_test_scaled)[:, 1] +\n",
    "    0.3 * nn_model.predict(X_test_scaled).flatten() +\n",
    "    0.0 * logistic_model.predict_proba(X_test_scaled)[:, 1]\n",
    ")\n",
    "\n",
    "# Optimize Threshold for Weighted Ensemble\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, ensemble_proba_weighted)\n",
    "f1_scores = 2 * (precision * recall) / (precision + recall)\n",
    "optimal_idx = f1_scores.argmax()\n",
    "optimal_threshold = thresholds[optimal_idx]\n",
    "\n",
    "ensemble_predictions_weighted = (ensemble_proba_weighted > optimal_threshold).astype(int)\n",
    "\n",
    "# Evaluation des Weighted Ensembles\n",
    "ensemble_auc_weighted = roc_auc_score(y_test, ensemble_proba_weighted)\n",
    "ensemble_report_weighted = classification_report(y_test, ensemble_predictions_weighted)\n",
    "\n",
    "print(\"Optimized Weighted Ensemble Test AUC Score:\", ensemble_auc_weighted)\n",
    "print(\"Optimal Threshold for Weighted Ensemble:\", optimal_threshold)\n",
    "print(\"Classification Report (Optimized Weighted Ensemble):\\n\", ensemble_report_weighted)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m10/10\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 943us/step\n",
      "Optimized Weighted Ensemble Test AUC Score: 0.8081382132302796\n",
      "Optimal Threshold for Weighted Ensemble: 0.6230595951515092\n",
      "Classification Report (Optimized Weighted Ensemble):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.98      0.94       249\n",
      "           1       0.87      0.51      0.64        53\n",
      "\n",
      "    accuracy                           0.90       302\n",
      "   macro avg       0.89      0.75      0.79       302\n",
      "weighted avg       0.90      0.90      0.89       302\n",
      "\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Add TabPFN classifier results from Modelling_tabpfn.py"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "26cf75b056696018"
  },
  {
   "cell_type": "code",
   "source": [
    "# To load the predictions\n",
    "\n",
    "loaded_predictions_tabpfn = pd.read_csv('/Users/janlinzner/Library/Mobile Documents/com~apple~CloudDocs/Documents/McGill/1_Lectures/Decision Analytics (MGSC 662)/2_Coding/Exercises/ADA_Coding/Code/tabpfn_predictions.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-16T19:42:43.489556Z",
     "start_time": "2025-01-16T19:42:43.484431Z"
    }
   },
   "id": "bf3a166b33e6db5b",
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "id": "edc7d307",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "id": "a30a19ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T19:42:44.105516Z",
     "start_time": "2025-01-16T19:42:43.542482Z"
    }
   },
   "source": [
    "### Results\n",
    "# Results Comparison\n",
    "results = {\n",
    "    \"Model\": [\"Gradient Boosting\", \"LightGBM\", \"Neural Network\", \"Logistic Regression\", \"Weighted Ensemble\", \"TabPFN\"],\n",
    "    \"Accuracy\": [\n",
    "        classification_report(y_test, best_gb_model.predict(X_test_scaled), output_dict=True)['accuracy'],\n",
    "        classification_report(y_test, best_lgb_model.predict(X_test_scaled), output_dict=True)['accuracy'],\n",
    "        nn_report['accuracy'],\n",
    "        logistic_report['accuracy'],\n",
    "        classification_report(y_test, ensemble_predictions_weighted, output_dict=True)['accuracy'],\n",
    "        classification_report(y_test, loaded_predictions_tabpfn[\"Predicted\"], output_dict=True)['accuracy'],\n",
    "    ],\n",
    "    \"Precision\": [\n",
    "        classification_report(y_test, best_gb_model.predict(X_test_scaled), output_dict=True)['macro avg']['precision'],\n",
    "        classification_report(y_test, best_lgb_model.predict(X_test_scaled), output_dict=True)['macro avg']['precision'],\n",
    "        nn_report['macro avg']['precision'],\n",
    "        logistic_report['macro avg']['precision'],\n",
    "        classification_report(y_test, ensemble_predictions_weighted, output_dict=True)['macro avg']['precision'],\n",
    "        classification_report(y_test, loaded_predictions_tabpfn[\"Predicted\"], output_dict=True)['macro avg']['precision']\n",
    "    ],\n",
    "    \"Recall\": [\n",
    "        classification_report(y_test, best_gb_model.predict(X_test_scaled), output_dict=True)['macro avg']['recall'],\n",
    "        classification_report(y_test, best_lgb_model.predict(X_test_scaled), output_dict=True)['macro avg']['recall'],\n",
    "        nn_report['macro avg']['recall'],\n",
    "        logistic_report['macro avg']['recall'],\n",
    "        classification_report(y_test, ensemble_predictions_weighted, output_dict=True)['macro avg']['recall'],\n",
    "        classification_report(y_test, loaded_predictions_tabpfn[\"Predicted\"], output_dict=True)['macro avg']['recall']\n",
    "    ],\n",
    "    \"F1-Score\": [\n",
    "        classification_report(y_test, best_gb_model.predict(X_test_scaled), output_dict=True)['macro avg']['f1-score'],\n",
    "        classification_report(y_test, best_lgb_model.predict(X_test_scaled), output_dict=True)['macro avg']['f1-score'],\n",
    "        nn_report['macro avg']['f1-score'],\n",
    "        logistic_report['macro avg']['f1-score'],\n",
    "        classification_report(y_test, ensemble_predictions_weighted, output_dict=True)['macro avg']['f1-score'],\n",
    "        classification_report(y_test, loaded_predictions_tabpfn[\"Predicted\"], output_dict=True)['macro avg']['f1-score']\n",
    "    ],\n",
    "    \"AUC\": [\n",
    "        roc_auc_score(y_test, best_gb_model.predict_proba(X_test_scaled)[:, 1]),\n",
    "        roc_auc_score(y_test, best_lgb_model.predict_proba(X_test_scaled)[:, 1]),\n",
    "        roc_auc_score(y_test, nn_model.predict(X_test_scaled).ravel()),\n",
    "        logistic_auc,\n",
    "        roc_auc_score(y_test, ensemble_proba_weighted),\n",
    "        roc_auc_score(y_test, loaded_predictions_tabpfn[\"Probability\"])\n",
    "    ]\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n",
    "\n",
    "# Plotting ROC Curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "for model, name, color in zip([best_gb_model, best_lgb_model, logistic_model],\n",
    "                              [\"Gradient Boosting\", \"LightGBM\", \"Logistic Regression\"],\n",
    "                              [\"green\", \"orange\", \"red\"]):\n",
    "    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    plt.plot(fpr, tpr, label=f'{name} (AUC = {auc:.2f})', linewidth=2, color=color)\n",
    "\n",
    "# Add Neural Network to ROC Curve\n",
    "nn_pred_proba = nn_model.predict(X_test_scaled).ravel()\n",
    "fpr, tpr, _ = roc_curve(y_test, nn_pred_proba)\n",
    "auc = roc_auc_score(y_test, nn_pred_proba)\n",
    "plt.plot(fpr, tpr, label=f'Neural Network (AUC = {auc:.2f})', linewidth=2, color='blue')\n",
    "\n",
    "# Add Weighted Ensemble to ROC Curve\n",
    "fpr, tpr, _ = roc_curve(y_test, ensemble_proba_weighted)\n",
    "auc = roc_auc_score(y_test, ensemble_proba_weighted)\n",
    "plt.plot(fpr, tpr, label=f'Weighted Ensemble (AUC = {auc:.2f})', linewidth=2, color='black')\n",
    "\n",
    "# Add TabPFN to ROC Curve\n",
    "fpr, tpr, _ = roc_curve(y_test, loaded_predictions_tabpfn[\"Probability\"])\n",
    "auc = roc_auc_score(y_test, loaded_predictions_tabpfn[\"Probability\"])\n",
    "plt.plot(fpr, tpr, label=f'TabPFN (AUC = {auc:.2f})', linewidth=2, color='purple')\n",
    "\n",
    "# Add Baseline\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random Chance', linewidth=1)\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve Comparison', fontsize=14)\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Predictions\n",
    "models = [\n",
    "    (\"Gradient Boosting\", best_gb_model, best_gb_model.predict(X_test_scaled)),\n",
    "    (\"LightGBM\", best_lgb_model, best_lgb_model.predict(X_test_scaled)),\n",
    "    (\"Neural Network\", nn_model, (nn_model.predict(X_test_scaled) > optimal_threshold).astype(int)),\n",
    "    (\"Logistic Regression\", logistic_model, logistic_predictions),\n",
    "    (\"Weighted Ensemble\", None, ensemble_predictions_weighted),  # None for model since it's a manual ensemble\n",
    "    (\"TabPFN\", None, loaded_predictions_tabpfn[\"Predicted\"])  # None for model since it's externally computed due to computation constraints\n",
    "]\n",
    "\n",
    "for name, model, predictions in models:\n",
    "    # Calculate confusion matrix and normalize to percentage\n",
    "    cm = confusion_matrix(y_test, predictions)\n",
    "    cm_percentage = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
    "\n",
    "    # Display confusion matrix\n",
    "    cmd = ConfusionMatrixDisplay(cm_percentage, display_labels=[\"No-Success\", \"Success\"])\n",
    "    cmd.plot(cmap=plt.cm.Blues, values_format=\".2f\")\n",
    "    plt.title(f\"Confusion Matrix (Percentage): {name}\")\n",
    "    plt.show()\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m10/10\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 918us/step\n",
      "                 Model  Accuracy  Precision    Recall  F1-Score       AUC\n",
      "0    Gradient Boosting  0.877483   0.802290  0.740055  0.764872  0.832083\n",
      "1             LightGBM  0.880795   0.804436  0.756914  0.777067  0.843298\n",
      "2       Neural Network  0.807947   0.679950  0.705312  0.690574  0.728878\n",
      "3  Logistic Regression  0.751656   0.653873  0.730583  0.665905  0.791013\n",
      "4    Weighted Ensemble  0.900662   0.887513  0.746685  0.792582  0.808138\n",
      "5               TabPFN  0.907285   0.932391  0.743275  0.798282  0.847920\n",
      "\u001B[1m10/10\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 779us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0sAAAIjCAYAAADSlID1AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAD3aklEQVR4nOzdeZxN9R/H8de9s8/csTOLVHZ+0lSULKXsW0hkT5HSQiXZdwYjbagsEZWIrMmeSlERmVJhFCmz2Jl9u+f3x5hhzHrHnbkz4/18POaROffccz73zon7me/3fN8mwzAMREREREREJB2zowsQEREREREpjNQsiYiIiIiIZELNkoiIiIiISCbULImIiIiIiGRCzZKIiIiIiEgm1CyJiIiIiIhkQs2SiIiIiIhIJtQsiYiIiIiIZELNkoiIiIiISCacHV2AiEhxMmfOHObOnZvpY15eXvj6+tK0aVNeeOEFLBZLpvv9+OOPfPbZZxw4cIDz589TsmRJKleuTMeOHenYsSNubm5Znv/PP//ks88+48cffyQ8PByTyUS1atXo0KEDPXv2xNXVNdevJS4ujvXr17Nx40aOHz/OxYsXKVu2LPfeey9PP/00tWrVyvWxCjN7vmdF2X///Ufz5s155JFHmDVrlqPLEREpFEyGYRiOLkJEpLhIbZa6d+9OvXr10j0WERHB1q1bOXToEPXq1ePjjz/Gyckp7fGEhAQmTJjAmjVruOWWW2jfvj2VKlXiwoULfP/99/z000/UqFGDuXPnctttt2U497vvvsu7775LmTJl6NixI7feeiuRkZF8/fXX7N+/n3r16rFw4UK8vLxyfB1///03L730EiEhITz00EPUq1ePEiVKEBISwtq1a4mLi2PWrFm0bdv2xt80B7Lne1bUxcTEsH37dipVqsQ999zj6HJERAoHQ0RE7Gb27NlGjRo1jNWrV2f6eHJystG3b1+jRo0axvbt29M9NnbsWKNGjRrG5MmTjYSEhAzP3b59uxEQEGA8/PDDxuXLl9M99sknnxg1atQwnnvuOSMuLi7Dc9966y2jRo0axpAhQ3J8DZGRkUaLFi2MgIAAY/fu3RkeP3XqlPHAAw8Y//vf/4w///wzx+MVVvZ8z0REpHjSPUsiIgXIbDbz+OOPA/Dzzz+nbQ8ODmblypU88MADjBs3DhcXlwzPbdGiBa+99hqnTp3i7bffTtt++fJlZs6cSYUKFXjrrbcynab30ksvUblyZb766itOnTqVbY0LFizg5MmTvPrqqzRq1CjD4/7+/rz22mskJSWxbNmy3L70QsXe75mIiBRPapZERAqYp6dnhm0rV64EYMCAAdk+t0ePHpQuXZovvviChIQEALZu3UpcXBzdu3fP8n4mk8nEwoUL+fHHH6lYsWKWxzcMgw0bNuDu7p7W1GWmbdu2bNq0iSlTpqRta9asGQ8++GCGfVetWkXNmjVZs2ZN2raaNWsybtw4pkyZwl133cV9993H2rVrqVmzZqb3fIWEhFCzZs1054uIiGDcuHE8+OCD3HHHHTz88MNMnTqVCxcuZFl3qht5z+Li4pg7dy5t2rThjjvu4L777mPQoEEcPHgw3fPnzJlDzZo1CQkJYeTIkTRo0IC77rqLvn37EhISwoULFxg9ejQNGjSgfv36PP300xw/fjzt+T/99FPa+/bOO+/QpEkT7rrrLrp27crWrVsz1HvixAnGjBlDs2bNuOOOO7jrrrvo3LlzhoZ25MiR1K1bl2+//ZaHH36YunXrMnToUP777z9q1qzJsGHD0vaNjY1l+vTptGnThjvvvJMGDRrw7LPPsn///gzn//bbb+nXrx/33HMPd955J506deKjjz7CarWm7ZN6jgULFvDZZ5/RoUMH6tatS5MmTZg8eTJRUVHZ/+BERAqYFngQESlgX331FQB33HFH2raff/4ZJyenHO8VcXJyomHDhmzatInDhw9z5513EhwcDJDhHqnrVapUKcfaTp8+TVhYGPXr1892IQlnZ2eqVq2a4/Gys3HjRvz8/Bg+fDj//vsvDzzwAOXKleOLL77gxRdfTLfv2rVrAXjssccA+Pfff+nZsycJCQl0796dihUrcvjwYVasWMGuXbtYsWIFZcqUyfLceX3PYmNj6devH8HBwbRo0YK+ffty9uxZVqxYQe/evTO9j2vgwIFUrVqVV155hePHj/PRRx8xaNAgPD098ff356WXXuLEiRN8/PHHDB48mA0bNmA2X/1d5ty5c4mOjqZPnz54eXmxcuVKhgwZwvjx4+ndu3fa+9G1a1fc3d3p0aMHPj4+nD59ms8//5zJkyfj5OREjx490o6ZlJTEsGHD6NOnD6VKlcLX1zfT1z906FB2795N7969qVKlCmfPnmXZsmX069ePzz//PG2Rj8WLFxMUFMRtt93GwIED8fDwYMeOHQQGBvLTTz8xZ86cdK9pxYoVREdH06tXL/z8/Ni6dSvLli3j8uXLWlxCRAoVNUsiIvkgJiaG8+fPp31vtVo5c+YMX3zxBatXr6ZOnTrpPlSfPn2akiVLZtugpEr9YBsREZH2XIAKFSrccN2px7THsXISExPD/Pnz0zUknTp1YtGiRQQHBxMQEABAcnIyX3zxBbVr1+Z///sfAJMnTyY2Npa1a9dy6623pj2/VatWPPXUU8yePZuJEydmee68vmeLFy8mODiY559/npdeeilte48ePejYsSPjxo2jSZMmeHt7pz1WtWpVPvjgA0wmEwAnT55k586dtGjRgnfffTdtv/DwcLZu3cp///2X7jWdO3eODRs2pC3q8fjjj6etWNexY0e8vb355JNPiIyMZMmSJema8NatW9O+fXu+/vrrdM2S1WqlT58+6V7Df//9l+61nj9/np07d9KzZ09GjBiRtv3+++9n5MiR/Pbbb9SqVYt///2XN954g6pVq/L555+njZz269ePYcOGsXHjRtavX8+jjz6adowzZ86wadOmtJ99165dad26NVu2bGHKlCl4eHjY8mMREck3moYnIpIPpkyZQsOGDdO+GjduTOfOnVm+fDndunVj0aJF6VbCMwwDZ+fc/f4q9XnGlcVMU7+/drpTXqXWkJycfMPHysktt9ySYeQmdeRo/fr1adv27NnD6dOn0x67dOkS33//PfXr18disXD+/Pm0r1q1alGpUiW2b9+e7bnz+p5t2bIFd3d3nn322XTbfXx86NOnD5GRkXz33XfpHmvXrl1aowSkjchdPwKV2gylNqypOnXqlG71Q4vFQq9evYiJiUk718iRI9m9e3e6RslqtZKUlASQ6fS2zO5Hu5bFYsHb25utW7eyatUqzpw5A8Ddd9/N1q1b6datGwDbt28nKSmJgQMHpptiajKZeOWVVwDYtGlTumPffffd6X72ZrOZ2rVrk5iYyMWLF7OtS0SkIGlkSUQkHwwYMIAmTZpgGAZnzpzh448/5siRIwwePJj+/ftn2N/X15dTp06RmJiY6eIO1woPD097DqR8UIeU39ZXr179huq+9lj5rVy5chm2Va1albvvvptNmzYxatQoXFxcWLduHa6urjzyyCMA/PPPP1itVr755hsaNmyY5fHj4+OzHKnL63t28uRJKlWqhLu7e4bHUo9z/QjN9a8ztVG7fnvqNLXrG7iaNWtmOFeVKlWAlOl3kNKYJCUlMXfuXA4dOsSpU6c4efIk8fHxmR4ToGzZslm8yhSurq7MmDGDUaNGMXbsWABq1KhBkyZNeOSRR9JG+U6ePAlAtWrVMhzjlltuwdPTM8N7Ur58+UzPBwXTqIuI5JZGlkRE8kG1atVo1KhRuhGl+vXrExQURFBQUIb9GzRoQEJCAgcOHMj2uMnJyezbt4+SJUum3S9y3333AelX18vMypUrGThwID/88EOW+5QtW5YqVapw6NAh4uListwvKSmJXr16MWXKlLQRruz2zcy197Bc67HHHuPChQvs2rWLqKgoduzYQfPmzSlVqhRw9YN/ixYt+PDDD7P8unbk7np5fc+ye62pdV0fYptV83vtaFN2MgvFTW0oUkcCv/rqK1q2bMnSpUtxdXWlRYsWTJ8+nW+//TbL42b3/qRq0aIFu3btYs6cOXTv3p2EhAQWL15Mly5d+Oijj4Ds3xNIeV+ufw25fe0iIo6mZklEpAC4urry9ttv4+vry+LFi9m4cWO6x7t164bJZGLevHnZfvhcu3Yt4eHhdOzYMe0D6IMPPoi3tzerV68mNjY20+clJyezbNkydu3aleP9IO3btychISFthb7MfPXVV+zfv58TJ06kffB1cnJKW6HvWmfPns32fNdr164dnp6ebNq0iW3bthEXF5c2BQ9SRisgZVW6Ro0aZfiKiYnB1dU122mNeX3Pbr31Vv79999MG8mQkBAgZWl1e7p2hbxUf//9NwCVK1cGYMaMGbi6uvLll18ye/ZsXnrpJdq1a3dD542KimL//v1cvnyZVq1aMXnyZLZu3cq6devw9vZOu98q9f6qY8eOZThG6nvl5+d3Q7WIiDiKmiURkQJSqlQpgoKCMJlMTJo0KW06HaSsjPfUU0+xZ88eJk2aRGJiYobnf/vttwQGBlKxYsV0N+ZbLBaGDBlCeHg4w4cPT5t6lcpqtTJ9+nQOHz5Ms2bNuOuuu7Kts3///vj6+vL2229nOgr1119/MWHCBJydndPVUaFCBS5evJhuylV8fDxbtmzJ8b25lpeXF23atOHbb79l/fr1+Pn50bhx47THy5UrR7169di9ezf79u1L99xvv/2WF154gQULFmR7jry+Z61btyYuLo758+en2//MmTN8+umneHl50aRJE5teb05Wr16dbrGQy5cv88knn1C6dOm09+XChQuUKVMmw/S2hQsXAnmb2nbkyBF69erFe++9l2579erV8fb2TmtGW7ZsiZOTEwsXLiQmJiZtP8Mw0vLA2rRpY/P5RUQKA92zJCJSgO6//3769u3LRx99xOjRo1m0aFHayMywYcOwWq0sWbKE3bt30759e2655RaioqL4/vvv+e6776hRowbvvPNOutXWAPr06cN///3H0qVLadmyJR07duTWW2/l9OnTbNu2jSNHjhAQEMD06dNzrNHT05MFCxYwcOBA+vfvT9OmTbn//vtxcXHhjz/+SFt8YerUqdx5551pz+vcuTM///wzAwYMoHfv3litVlavXp3jNK3MPPbYY6xZs4Yff/yR5557LsOUvQkTJtCnTx+eeuopunfvTo0aNfj7779ZsWIFpUqVSrd6W1by8p4NGDCAr7/+mvfee4+QkBAaNmzIuXPnWLFiBZGRkcycOTPTHK0bERsbS9euXenRowcmk4nPPvuMCxcupAvTbd68OevWreO5557j4YcfJjY2lm3btnHgwAFcXV25fPmyzee95557aNSoEStWrODy5cvcd999JCcns3nzZk6dOpX2Ht966628/PLLvPHGG3Tu3JkuXbqkLR2+d+9eHn74YTp27GjX90REpKCoWRIRKWDDhg3j+++/Z/fu3Sxbtow+ffoAKdPYRo0aRbt27fj000/ZtGkT4eHheHt7U6VKFaZMmUKnTp0yXbTAbDYzevRoHn74YVasWMHWrVs5ffo0ZrOZGjVqMH78eLp3757rFfdq1qzJ+vXrWbVqFVu3bmX+/PlERkZSrlw5OnTowIABAzIsjNCtWzdiYmJYvnw5M2fOpFy5cnTq1IkHH3yQXr162fQe1a9fn8qVK3PixIl0U/CurW/NmjW89957bN26lc8++4zy5cvTpk0bnn/++XSrx2UlL++Zp6cnn3zyCQsWLGDz5s188803eHt7U69ePZ5++ukcR+3yol+/fphMJhYvXkxCQgIBAQHMmDGD+vXrp+0zfvx4SpUqxbZt29i9ezdlypShRo0afPTRR3z22Wd8+eWX/Pvvv7nK2kplMpmYM2cOixcvTnutALVq1WLWrFlpC24APPPMM1SpUoUlS5akjbpVrlyZ8ePH07NnzyzvTxMRKexMRl5+5SciIiL56qeffuKJJ55g0KBBaUtwi4hIwdKvekRERERERDKhZklERERERCQTapZEREREREQyoXuWREREREREMqGRJRERERERkUyoWRIREREREcmEmiUREREREZFMqFkSERERERHJRO6i3IuRc+ciKQxLWri4OJGYmOzoMqQI0TUjttI1I7bSNSO20jUjtihM14vJBGXLeue4303XLBkGhaJZgsJThxQdumbEVrpmxFa6ZsRWumbEFkXtetE0PBERERERkUyoWRIREREREcmEmiUREREREZFMqFkSERERERHJhJolERERERGRTKhZEhERERERyYSaJRERERERkUyoWRIREREREcmEmiUREREREZFMqFkSERERERHJhJolERERERGRTKhZEhERERERyYSaJRERERERkUwUimYpISGBDh068NNPP2W5zx9//EG3bt0ICAjgscce49ChQwVYoYiIiIiI3Gwc3izFx8czdOhQQkJCstwnJiaGZ555hvr167NmzRruvvtunn32WWJiYgqwUhERERERuZk4tFk6duwYjz/+OCdPnsx2v02bNuHm5sbw4cOpWrUqY8aMwcvLiy1bthRQpSIiIiIicrNxaLO0d+9eGjRowGeffZbtfsHBwdSrVw+TyQSAyWTinnvu4eDBgwVQpYiIiIiI3IycHXnyXr165Wq/M2fOUK1atXTbypYtm+3UPREREZGixjViLV5/BWJKinJ0KbliMoFh5P954pJiiUyIxIo1/0+WS3/vrc3+1Q+RGOfm6FIKncsxrkTHJwMpF0cCCfzID/jizy3VKjB5zxuOLdAGDm2Wcis2NhZXV9d021xdXUlISLD5WC4uTvYq64Y4OxeOOqTo0DUjttI1I7bSNeN4lr+m4RR91NFlFDpegFchuzzXrnmIi2HlHV1GoeQElLjy57/4i3WsI5ZY2uCD9Zg7rq6F7IeZjSLRLLm5uWVojBISEnB3d7f5WImJyQXyG5DcSEhIdnQJUsTomhFb6ZoRW+macbCkSAAMzFjdfB1cTM4KamQpIiYcq9UKJjCbHL4+GQCxsSkjSiaTFc9SRWMksKCEXTCTTCJOOGHFwAdfmvIQHnjh5vJvofh75srdPTkqEs2Sj48PZ8+eTbft7NmzVKhQwUEViYiIiOQfq5sv5x887OgycuTq6lQgH3zrL61FWHQofl7+BPcrJO+L+wIgCk/fEvQLHuboagqNy5cvUa1aLSAak8mfiIiUn1eZgFo4hYWSXM6f844t0SaFozXPQUBAAL/88gvGlV9dGIbBgQMHCAgIcHBlIiIiIiJiGAafffYpDRvWA4pPvE+hbZbOnDlDXFwcAG3atOHy5csEBgZy7NgxAgMDiY2NpW3btg6uUkRERERE+vfvy+DBg2jcuAkmkw9A2krWRVmhbZaaNGnCpk2bALBYLMyfP5/9+/fTpUsXgoODWbBgAZ6eng6uUkRERETk5nTx4oW0W2Uef7wnq1d/wYIFSzCZis4CDjkpNPcsHTlyJNvv77zzTtauXVuQJYmIiIiIyHWsVivLl3/C1KkTaN26HW+//S5t27Z3dFn5otA0SyIiIiI3o2uzlczx4Y4up8CsWPAJx2eH4BSf8yhET6MHGGA2m1n65oICqC5nMRHRji4hX2zYsJagoECiojJf4S8hIYHLly+RmJiIh4cHO3ZsIyCgVrp9rNar17HrhrV4BQVijiia17aaJREREREH8vorEOfrspUMZ4uDqik4x2eHUPJ0SZufF32pcC3T7WJxzXmnIiQoKJCQkNxlfcXGxhIbG5vl4yaTN15BgThfczzDUrSubTVLIiIiIg5kSkr58J+arWQ4W4iuOtbBVeW/1BElq8lKTImcV08zY8bb1Rt3Z4/8Li3XXCyuNBjZyNFl2FXqiJLZbMbHxxfDMIiNjcXDwwOTyURCQjwuLq7ZLt4QEWHCavXG23sSpqhXADDMZqzVqhM9YkyBvA57UbMkIiIiUggUlWwle4spEcPwkPGOLkOu4+Pjy6JFHzFy5DB+/fUg77+/kBYtWufquQEBXoSFmfHwsAIpzZLVx5eovQcKRSCtLQrtangiIiIiIuIYFy9eoF27FgB8+eX2XDdKxY1GlkREREREJJ24uDhmznyLvn2fxMmp+CwFbis1SyIiIiIiN7kff9zDjh3b0r6vUMGHJ58c4MCKCgdNwxMRERERuUlFRITz/PMD6dixDd9//y1WqxVIWeBBNLIkIiIiNji24Sh7g/aQGJXg6FKyFZcUS2RCJFasDq3Dw2RgMRnZ/nbaTA8ArECEdXKB1FUYeF72dNi5c8oSKuxMcbGYIiPBatv1HWt4cNmwYL1yRRpEYxAJmDBRkl8OnMLgNAARYQZ3++bt/YmwpvxszRERmCma+Uqp1CyJiIhIru0N2sPFkPOOLiNXPHHch/FrZZ1Ck1HRSqCxj2S3gl8dzZYsoeIl+srX9QwMLgGX0rZYKckpq98Nnc3behHTlV9YFLV8pVRqlkRERCTXUkeUTGYTnj5eDq4maxEx4SnTiUxgNjluOpGP2Zo2qpTdGIAViDRMxBlZZ9cUR8luyVR5qXqBn/f6LKGixhwRfnVUyYbpcqHWcldGkpwxY8HAwETGa86MFyVMQ/EwheW5Rm9TFJO83yLZwx/DYiF6ZNHMDlOzJCIiIjbz9PGiX/Azji4jSwFLaxEWHYqflz/B/RyXXVRmVy2c4kNJdvMvlhlKrq5ORS4351o+Pr4EBxe9n0uZgFo4hYWS7OfP+VzUn5CQwPz57zF1ahAY3pQq9QZHj3bK5yotwBuc5420La75fMb8oGZJRERERKSYOn/+HB06tOL48b/x8HiBmJjJeHh4k/l0PLmelrkQERERESlmTp8+jWEYlC5dho4dO/PVV99TsuRbQElHl1akqFkSERERESkm4uLiePPNmdx7b122b9+CyWRi5Mhx/O9/dRxdWpGkaXgiIiKSwYZjawnaG0hUYhRVgitTf1t9XOJd8LzsiRkzETHhBCytBUAH91he847Ey+zYZbqvtbeCFQwwm8Mps6uWw+owxxftZZPzk+uGtXgFBWLKYvnuz2NjmRgZSaRh/+sqIjVLKCKcnTWGMTFyGJFGEVqtzfpTyn8jnLAGXF1oJS5uE5cvv0xy8j94eb3Mq6+2xmy++nhExM21gIg9qFkSERGRDIL2BhJyMWVp5S5bHqX02dLpHo91iSUsOhSAV8pBdZcCLzGXrBAf6ugiMJyL0AfxAuIVFIhzNst3TwTye+kFb6uViRdf5jAFvyKfXViBtAXrlgO9gObAF0RH1yY6i9uSLBajIKorFtQsiYiISAZRiVeWVjaZ8Uj0AMBqshJTIoZEt0R+aX0QPy9/AEo6hQNWkg2IsBaeGf5mzHi7euPu7OHQOgxnC9FVi+ayyfkpdUTJMJuxZrJ8d+SV5bHNgJ8Ny2PnlrfJzCRvb4ZeLgVWMJOMn/m03c+Tb8wmrBZn4s2/4ObWHMPoRHz8GtzcOmIymchqsXqLxWDkyMIdKl2YqFkSERGRLPl4+uLj6Uv0pSi8fUvwYvCwDPuU2VUrZfTG3R+XQrY8dsyVLym8rD6+mS5/bQ2oBWGh+Pj580s+Lu9tDfCCMPDxM/FLcNEYATQMgy1bNjFu3EgunzvHTz/9QcmSpYAW6Iq3r8Lz6x8REREREcnW338fo2fPx+jXryfVqlVnx45vrzRKkh80siQiIiIiUkRMnz6VY8dCWLp0OW3atLsy5U7yi5olEREREZFCyjAMNm5cj5ubG61atWX69Fl4eXnh4eHYe/FuFpqGJyIiIiJSCB09eoRu3TozYMATbNu2FYBy5cqpUSpAGlkSERERANYdXUPgnilEJUYREZNzPpBrxFq8/gosFllCOWX+SOZMJlgVk8c8pCtZR0SEpyzmcJ2ICPteVxs2OBMU5EpUVPppa4Uxeyg2NpaZM6cxf/673HJLJZYtW0nLlm0cXdZNSc2SiIiIADDth6lp2UqpLC5Zrw7m9VcgztFX9y/KWUI5Zf5I1iZyg3lIViuEZZ2FZbHY57oKCnIlJMQpm/MUnuwhJycnvvvuW4YNG8nzzw/B3d3d0SXdtNQsiYiICABRiZFASraSj6cvFhcLIxuM5TyZZ8+Ykq7k5GAm2atakc4SyinzRzJnMkFk+A3kIZnNGN7eGO6ZTyuzWCyMHGmf6yp1RMlsNvDxSd8YFYbsoT///INx40YxdeoMatWqzdatX+PklHVzJwVDzZKIiIik4+PpS3C/q2MFS1mQ7f5WN18uNPo5v8sqEFll/kjmXF2dsNauXiB5SPbi42MQHBzt6DLSXL58iddfn84HH8yncuUqREenNO5qlAoHNUsiIiIiIg7w4497GDDgCaKjoxk1ajyDBr2Aq6uro8uSa6hZEhEREREpQFFRUVgsFm6/vTLNmrVg1Khx+PtXdHRZkgktHS4iIiIiUgAuXrzAyJGv0qhRPS5duoivrx9z5sxTo1SIqVkSEREREclHVquVZcs+omHDe1i5cgXPPTcYT08vR5cluaBpeCIiUuSl5v2krs5WmP2xpzLfflafhFiXAjmfgYFh5G5J5JfpAYAJ+Hj65LTtMRc9ATPm+HDK7Lqah1NU8pVyk6FktnOmT2GzYcNagoICibJzjpTJZP88pLzKKkcplSPzlF555UWWL/+Erl27M2HCFHy04mKRoWZJRESKvOvzfgqzXSse5VxoaUeXkSdu7rE4xWfMwyns+Uq2ZCgZdsr0KWyCggIJyeccKXvlIeVVTjlKqQoqT+n8+XNcvHiRKlWq8uSTA+jZsw/339+oQM4t9qNmSUREirxr836sboX7N7bxcSl5MiazFUupmHw/n9WwkvrRMLe/VzeZTJiu29vVI5GmPQ6S7OafbrvhbCn0+Uq5zVAyLBai7ZTpU9ikjiiZzWa7jmqYTGAY9s1DyqvscpRSFUSeUnJyMh9/vITp0ydz11338Nlna7n77nr5ek7JP2qWRESk2LC6+XL+wcKd82J1WwBE4elTgr7Bw/L9fAFLaxEWHYqfl3+67KTMuLo6kZCQnO0+5+1ZXAFThhL4+PgSbMf3IDfXTEFzZI7Svn0/MWrUa/z660F69uzD2LGTHFKH2I+aJRERERGRGxQdHU2fPo9TqdJtfPnldu69t4GjSxI7ULMkIiIiIpIHSUlJfPTRh3Tu3IUyZcqyfv0WqlevgZNTzvdOSdGgpcNFRERERGz04497aNHiQUaNGsbOnTsAqFWrthqlYkbNkoiIiIhILkVEhPP88wPp2LEN7u5ubN36NV27dnd0WZJPNA1PREQkl45tOMreoD0kRuV9Na2YiNzfeL7h2FqC9gYSlZj3bJyImILNwMlNplFBK+4ZStfKKk+pqGQh3YiCylEKDT3FN9/s5K235tKzZx/MZo09FGdqlkRERHJpb9AeLobYZz04F4trjvsE7Q0k5KJ9snEsLgWTgWNLplFBK64ZStfKKU+pqGQh3Yj8yFHavfs7Pv54Ce++u4C7767HgQO/4+7ubvfzSOGjZklERCSXUkeUTGYTnj5eeT6Oi8WVBiNzDqdMHVEym8z4eOY9G8fiYmFkg4LJwMltplFBK84ZStfKLk+pqGQh3Qh75yiFhp5i0qSxrF27mnvvbcC5c+eoUKGCGqWbiJolERERG3n6eNEv+JkCO5+Pp2+OGUmFjTKNHMveeUr25sgspNxasmQREyeOxcvLizlz5vH44z0xmQpmqp8UHmqWRERERESuSExMxMXFBXd3d/r0eYLhw0dTokRJR5clDqJmSURERERuev/99y/jxo3Cy8uLuXPn06NHb6C3o8sSB9PyHSIiIiJy04qLi+Ott16nceP67N+/j2bNWji6JClENLIkIiIiIjeluLg4mjVrzIkTx3n22Rd49dXhWCzeji5LChE1SyIiUui5RqzF669ATEmZZ/eY4zPPkLElFykuKZbIhEisWLPcx/OyJ2bMRMSEE7C0Vu6KvwEFnZGUV9dmK+VnplFWGUJylb3zlHLKRTKZwLBhUbuCykLKycmT/+Dn54+7uzvPPvsCDRs2pkaNmo4uSwohNUsiIlLoef0ViHN0ztk9hnP6DBlbc5E88czVfrEusYRFh+b6uDeqoDKS8iqzbKX8yDTKKUNIrrJXnlJ+5SLlRxZSbsTGxjJ79pvMnfs2kyZNo3//gfTr198htUjRoGZJREQKvdQRJQMzVrfMs3sMZwvRVdNnyNiSixQRE47VagVTSq5RVhLdEvml9UH8vPxteQl5VpAZSXl1fbZSfmUaZZchJFfZM08pp1wkW0eWwP5ZSLlhGAZbtmxi3LiRhIWF8vzzQ+jevVeB1iBFk5olEREpMqxuvpx/0PbsmNzkIgUsrUVYdCh+Xv5FLtOosCiobKXCniFUHGWVi+Tq6kRCQrIDKrLNd999S79+PWnWrAWffbaGqlWrO7okKSK0Gp6IiIiIFDvR0dGsXr0SgAceaMratV+yfPlqNUpiEzVLIiIiIlJsGIbBF1+so0mTe3n55Rf499+TmEwmGjd+AJOpcCwwIUWHmiURERERKRaOHj1Ct26dGTDgCe64oy7ffbeXSpVudXRZUoTpniURERERKRY+++xTTp48wbJlK2nZso2jy5FiwGQYtq5hUrSdPRtp86ot+aGo3BAphYeuGbFVfl0zG46tJWhvIFGJWWfddHCP5TXvSLzMWWcW2cLHbMXJBKHJZupHZL8KWtcNFam8pwHJVldiDAsGZrxMl+lV4u20fb5INJgVZxCVxb8HTtmshieZsF75OV9ZDS+/RESkrFjo5+evBR7yQWaZShERJqxWE35+1kK5wINhGKxd+zmXLl3iqaeeJiYmBrPZjLu7u8Nqkqw5+nq5lskE5crlHECskSUREbFJ0N5AQi5mn3XzSjmo7mL/c19KtuaYb3T7nke5nFwu3TYPIx7/S1cbt7eBY9kdxLBPk3fTsVohLP/zp+yVISTpZZep5KhcpOz8+ecfjBo1jD17vqdr1+489dTTeHrmLitNJLfULImIiE1SR5TMJjM+npmPIpR0CgesJBsQYbXPKE201cybUd74eXlku5/V6gaACSuepihcTPEEuH9DqMvVOi5etoKRcuNuhWvu9zYBJpMJE7oJ3GZmM4a3N4Z79j+fG2XPDCFJL6tMJUfkImUnMTGRyZPH8cEH86lcuQqffbaWhx9u7uiypJhSsyQiInni4+mbZR5RmV21ID4U3P1xyUMuUmZKAdOufGXn4xGTwQCLOYa+4RMz3ccUUAvCQvHRdK50CtMUGXGcrDKVHM0wDEwmE87Ozpw6dYrRoyfw7LPP4+rq6ujSpBjTpGwRERERKdR++y2YDh1asX37FkwmE4sWfcTgwS+rUZJ8p2ZJRERERAqlixcvMGLEUFq2bMrly5coUaIUgPKSpMBoGp6IiIiIFDrBwb/Qo0cX4uMTmDhxKgMGPIuLSz6sHCOSDTVLIiIiIlJonD59mgoVKlCjRi26du3Oiy++jE8+Lkkvkh01SyIixdQv+0dQ6b8FeJiS8nwM970G3qsNzHFXt5288l8ToZinl8r8iVeW3jabwinjVivDw3/EVubbyPokGPb/LXGkNWXp4DCrlYCAjOeGlLwekeIks4wkW0VEOHZq27lz55g2bRKff/4Z3377I7ffXpkpU2Y4tCYRNUsiIsVUpf8WUNX5Bpf7XQOEZbdD9nlEJqw4kTF3ZxePco7SN1RaTmKJJyyHzB/l9UhxkV1Gkq0KOlMpOTmZjz9ewvTpk0lOtjJu3CRuuaVSgdYgkhU1SyIixZTnlRGlG8k68om14gQYJrCWSv9YjnlEJjOGkzeGU8bcnfgID7CmNFMWc0yeastKmNVKLPF8w9f4+flnuZ/yeqQ4ySojyVaOyFSaMmUC7703m549+zB27CTKly9foOcXyY6aJRGRYi7CasalzcW8Pdm9FhCK1def83bMI7IGLICwKDz9StA3eJjdjgsQEFCLsLBQ/JShJDehwpqRdL0zZ87w77//cM899Rkw4Bnat3+Ee+9t4OiyRDLQ0uEiIiIiUiCSkpL44IN5NGx4D8OGvYxhGFSqdKsaJSm01CyJiIiISL778cc9tGjxIGPGjKBTpy6sWrVeeUlS6GkanoiIiIjkq+TkZIYOHUyJEiXYuvVr7rrrHkeXJJIrapZERERExO4SExP54IP5NG/ekho1avL55xvw9fXDbNbEJik61CyJSLHnumEtXkGBmKKi7HpcU3IspuRIMKz8kVCbb2IeIt5ws+s5bBULXDZSF/R++ZpHJubxiD1S/hMG+E7Oc13X87R6YsZMRER4lllIeaUMpcLJHjlAkjVHZyRd7/vvdzFq1DBCQo7i4eFBjRo18fev6OiyRGzm0GYpPj6eSZMmsW3bNtzd3enfvz/9+/fPdN/t27fz5ptvEh4eTq1atRg7dix16tQp4IpFpCjyCgrEOeRovp7jGx7iLIVjuVvP/Dpw9pFKeRJrjc0xCymvlKFUuNgzB0iyVtAZSdeLiIhg3LgRrFu3hvvuu5/t23dRt+6dDq1J5EY4tFmaOXMmhw4dYunSpYSGhjJixAj8/f1p06ZNuv1CQkJ49dVXmTx5Mvfccw9Llizh2WefZfv27Xh4ZMzvEBG5VuqIkmE2Y/XxtdtxzfHhmAwrBhB/0Q2MK7lBJvuOYNkizLja09h1oovJlPJlR4mmRH7xPoifR9ZZSHmlDKXCx145QJI1R2QkXS85OYmDB39h7tz5dOvWQws4SJHnsGYpJiaGVatWsXDhQurUqUOdOnUICQlh2bJlGZql3bt3U61aNTp37gzA0KFDWbZsGceOHaNu3boOqF5EiiKrj69ds4LK7KqFU3woVjd/rIP98y03yBbXZgyZXoXQqFD8vPwJ7qe8ISkcikoOkOTeN9/s5J133uCjj5bj71+RH344gJOTRhGleHDYHXaHDx8mKSmJu+++O21bvXr1CA4OxmpNP9ejVKlSHDt2jP3792O1WlmzZg0Wi4Vbb721oMsWEREREeC///6lb99ePP54ZwzD4NKlSwBqlKRYcdjI0pkzZyhdujSurq5p28qVK0d8fDwXL16kTJkyadvbtWvHzp076dWrF05OTpjNZubPn0/JkiUdUbqIiIjITW358k8YOfJVSpUqxbx5i3j00a6acifFksOapdjY2HSNEpD2fUJC+vm2Fy5c4MyZM4wfP56AgACWL1/OqFGjWLt2LWXLlrXpvC4uheO3Hc7OhaMOKTp0zeRd6r/fJhO4utrvfbz2uPl1jhupCUyFoiYpOvLz75nC8v+I3JioqCgsFgvVqlVl4MBnGTVqDB4e+basjBQzRfGzjMOaJTc3twxNUer37u7u6bbPmjWLGjVq0Lt3bwCmTJlC27ZtWb16Nc8884xN501MTMYoJPeVJiQkO7oEKWJ0zeRN6v/zhmHf9/Da4+bXOW6kJhNGoahJipb8ulYKy/8jkjcnThxn3LiRXLhwgS++2Mq99zbk3nsb4urqpJ+n2KSwXC+5HQh1WLPk4+PDhQsXSEpKwtk5pYwzZ87g7u5OiRIl0u37+++/07dv37TvzWYztWrVIjQ0f5abFRHHsncukjmT3B3XiLV4/RWIKSnrc/yxpzLfflafhFiXzHcwrmQQmczEXCyYG9Y3bFhLUFAgUVm8N8oYcqyiniVkMpFvv1AsbDlAkjuxsbHMnv0mc+e+Tbly5Zk8ebqjSxIpUA5rlmrXro2zszMHDx6kfv36AOzfv5+6detmSHauUKECf/31V7ptx48f10p4IsVUfuUiGdfk7nj9FYhzdPbn2LXiUc6Fls7t0QFwsbjmsN+NCQoKJCQX743FYiEaxy1hfrNSllDOHJ0DJLlntVpp374lR48e5vnnh/DSS6/i5eXl6LJECpTDmiUPDw86d+7MxIkTmTZtGqdPn2bx4sVMn57yG4szZ87g7e2Nu7s7jz/+OCNHjuSOO+7g7rvvZtWqVYSGhvLoo486qnwRyUf5kYtkWCxEX5O7kzqiZGDG6pb5OeLjUnLcTGYrllIxWRRrxnDyxnDywMXiSoORjexSb1ZSR5TMZjM+Wbw3qRlD4y6MzNdaJKOiniWUnyNLUDhygCRnf/99jDJlylKqVGlGjBhD9erVqVKlmqPLEnEIh4bSjho1iokTJ9KvXz8sFguDBw+mVatWADRp0oTp06fTpUsX2rVrR3R0NPPnzyc8PJzatWuzdOlSmxd3EJGixd65SJmew82X8w9mfg6r2wIgCk8fx2YnZcbHx5fgHN6bcR+pWXKUopolpPtPbm7R0dG8/fYs3n9/Ds8/P4TRo8fTunVbR5cl4lAObZY8PDwICgoiKCgow2NHjhxJ9323bt3o1q1bQZUmIiIiclMwDIMvvljH+PGjOXfuLEOGDGXw4FccXZZIoeDQZklEREREHOvw4T95+ul+tGnTjsmTp3P77ZUdXZJIoWHOeRcRERERKU6ioiKZN28uycnJ1K79P77+eg8ffbRCjZLIddQsiYiIiNwkDMNgzZpVNGpUn+nTp/DHH4cAqFPnDgdXJlI4aRqeSBF2bMNR9gbtITGq6K8uZYq5hCkqGqxczS8KB2pNzpfzxcb34HIsWA3AlPk5PK2emDETERFOQECtPJ0nLimWyIRIrFhvoNqrrJdTjhMRE07A0uxriohR5lJ+yixTSVlCUpj9+ecfjBo1jD17vqdDh05MmhRIpUq3OroskUJNzZJIEbY3aA8XQ847ugw7cQLSB1JjAPn48jyvPU82Yq2xhIUVrhBsq4uVsOjc1WRxseS8k9gsu0wlZQlJYfTzz3s5fTqCzz5by8MPN3d0OSJFgpolkSIsdUTJZDbh6VO0gwLN4aFXmxbTlS9PE7jmz2/qwy5YU0aVIEMQ9rUSTYn84n0QPw//PJ0nIiYcq9UKJjCb7DPz2eRmxru1Nx5eHtnvZwIvZwsjG4zNdj/Jm6wylZQlJIWF1Wpl1aoVHDlymPHjJ9O79xN0794LV9f8Dc8WKU7ULIkUA54+XvQLfsbRZdyQcrVKYTpvxShj5uzhi/l+voCAWoSFheLn559jXtENnWdpLcKiQ/Hz8ie4X/5mRl1PmTkFo6hmKknx9ttvvzJy5Kvs2/cTXbp0JTk5GScnJzVKIjbSAg8iIiIixYRhGIwe/RotWz7I5cuXWLNmI/PmLcbJKfMpoyKSPY0siYiIiBRxVqsVwzBwcnLCw8OTCROm8vTTz+Li4uLo0kSKNI0siYiIiBRhBw8eoF275ixevACAceMm8dxzL6pRErEDNUsiIiIiRdC5c+d49dUhtG79MHFx8dx5592OLkmk2NE0PJFC7NocJZMJjOtWI46JKBo3lbtGrMXrr0BMSVGwJxbzZ5EQe13u0IWr3284tpagvYFEJUbleOzY4Fgit0VixNuWY2RLXtGNKO5ZR5llDd0slKkkjvT338do06YZVqvBtGkz6ddvAM7O+lgnYm/6v0qkEMttjpKLpXCvbuT1VyDO0UdTvlkBZBcP5OlM0N5AQi4ezd3BtwBn816bLXlFN6K4Zh1llzV0s1CmkhSkv/8+RpUq1ahcuSrPPz+E3r37Ub58eUeXJVJsqVkSKcSuzVHy8vXKMLIEKY1Sg5GNCrgy25iSUkaIDMwQByasGGag1HUzgT2diXnlGaIS1wApuUQ+nr7ZHjsiMRwrV3KMStg2szi3eUU3yuJSfLOOssoaulkoU0kKypkzZ5g6dQIrVixj48Zt3HtvA15+eZijyxIp9tQsiRQBnj5ePP3Hc0U+M8fq5gtu4EQoVh9/zmeVb7Q0pVny8fTNMZso4M1ahF0Kxc83f/OSJHvKGhLJH0lJSSxZ8gEzZgRiNpsICnqTe+6p7+iyRG4aWuBBREREpJCaN+9dxowZQadOXfjhh1948skBykwSKUAaWRIREREpRCIiwgkO/oVWrdry5JP9adLkAe666x5HlyVyU9LIkoiIiEghkJiYyHvvzeH+++9h9OjhJCYmYrF4q1EScSA1SyIiIiIO9v33u2jWrDGTJ4+je/ee7NixS6GyIoWApuGJFALX5ildK685Shs2rCUoKJCoqCjikmKJTIhMWTEun3gkGljiDExZLIZ2dXZ9KFzZJzk8lNPVS2W6v9WwggER5nAC3sw+AykionjnGNlLfuUhKWtIxD7efvsNSpYsxfbtu6hb905HlyMiV6hZEikEcspTsjVHKSgokJCQXOYU2UH0lS+bGMCl7Bs4K1bCLuUuA8liKZ45RvaS33lIyhoSsU18fDzz57/LnXfexUMPNWPRoqWUKFESk0m/gBApTNQsiRQC1+Ypefp4pXssLzlKUVEpuUZmsxm8wWq9kkNkyp+ZtxUuW3FKHTHK5N/5az+iJwOGCSLdTcS5ZP2hwIwZb1dv3J1zzkCyWCyMHFk8c4zsJT/zkJQ1JGKbnTt3MGbMcE6cOM7YsZN46KFmlCxZytFliUgm1CyJFCKePl70C37Gbsfz8fGFoRAWHYqfl3+OmUV5VSagFk5hoST7ZZ6dVGZXLZziQ0l28+f8g8pCciTlIYk4ztmzZxk27CU2bfqCRo2asHjxJ9Su/T9HlyUi2VCzJCIiIpKPDMPAZDLh6enJ6dMRzJ+/mM6dH9OUO5EiQKvhiYiIiOST7du30LTp/Zw4cRxPT0++/HI7jz7aVY2SSBGhZklERETEzk6cOE6fPo/Tu/fjVKjgi2Gk3CuoJkmkaNE0PBERERE7Wrv2c4YMeY5y5cqzaNHHdOjQUU2SSBGlZknkimuzjuLiYomMjExZRS7fGClfBngaFsyYiQgP5e5apTLZ08AwDHK7htnpi1eeFxfKvgoppzGbwymzK/vMokzticX8WSTEZvNeXEx5zByf+TnM8cpCcoR165wIDHQjKsqkPCSRfGYYBmfOnKFChQrcfXc9nn9+MEOGvIqXl1fOTxaRQkvNksgV12cdeeJZ4DXEGvGcOm+/Bq2UB9yS9n+5FeJzl1mUzgogl08zuVtxyuYchrOykArStGkuhISkn22tPCQR+/vrrxBGjx7OsWMh7N79M7ffXplRo8Y7uiwRsQM1SyJXXJt1FMnVUSWzOZ9u7TPSN0WJpniCPb+homvG8yUbtjdQFnd4pYuJ0GSTTZlF1zPHhWPCimEGSmXzXniYsfbwBrfMz2E4W4iuqiykgnQlbistW0l5SCL2FR0dzVtvvc7778/Bz8+fwMCZuLm5ObosEbEjNUsi1/H08WIe8wkLC8XPz5/gTHKD7MGW7KGApbVuOCsp5sqXzXW61cKJUKw+mWcoSeGnbCWR/NG3b3f27fuJl156lcGDX8HDw/ZfSIlI4aZmSURERCSXjh49gouLC5UrV2Hs2ImUKVOW22+v7OiyRCSfaOlwERERkRxERUUyceJYHnqoIbNnvwnAPffUV6MkUsxpZElEREQkC4ZhsGbNKiZOHMvly5cYPnw0gwa96OiyRKSAqFkSERERycLp0xG8+uoQmjVryeTJ07jllkqOLklECpCaJSn0NmxYS1BQIFGpS3tdUSW2MvUj6+NiuGT6vNRsotyyXMk6CgsPJezKWtkRMeEELM0+m6iDeyyveUfiZbZxxTqzFUwp56ifwzkiYq7mFLluWItXUCCm696P/GKOUEYSwIYNzgQFuRIVVXTyipStJJI3ly9f4t1332Hw4Ffw8fHl++/3qUkSuUmpWZJCLygokJCQoxm2d+FRSlPa7ueLN+LT/mx1sRIWnX3I0CvloHrm/VquXErO+RypLC4WvIICcc7k/chvhuXmzkgKCnIlJMTJ0WXkibKVRHLHarWycuVyJk8eT0xMDA8++DCNGz+gRknkJqZmSQq91BEls9mMj49v2naPCA+wghUrMeaMi2Jb85BNFG+K5xv3bzC7mDG5mfFu7Y2HV/ZLwZZ0CgesJBsQYbVtzZRoq5k3o7zxy+EcAN6u3oy4bwymoJEAGGYz1mvej/xkWCxEj7y5M5JSR5RSM4uKApMJvLyUrSSSG3/88TuvvfYy+/b9xKOPPsaECVPx96/o6LJExMHULEmR4ePjmy7zaGnAAqLDovD2K8GLwcMy7J/XbKIJNtZVZlctiA8Fd39ccshLul4pYNqVr5y4ujqRkJAMpDRLVh9f5R45QFHKLLp6zYhITs6dO0tk5GXWrv2Sxo0fcHQ5IlJIaOlwERERuelYrVY++WQp/fv3xTAMHnigKV9/vUeNkoiko2ZJREREbioHDx6gXbvmDB06GA8PD2JjYwFwciqa9yWKSP5RsyQiIiI3jQkTxtC69cPExyewYcNW3n13AZ6eno4uS0QKKd2zJDfk2Iaj7A3aQ2JU3m8gj4uLJTIyEqs18wUZelp7AGCOMLM0YEHa9uiIlIUfslre+9rltu3BNWItXn8FYkpKv2S3OT5v57F1CXCTCQzj5lvKu7As2a1luEWKruTkZOLj4/H09KRq1WpMmzaTfv0G4Oysj0Eikj39LSE3ZG/QHi6GnL/h43iSi9/qWSE6LGNjEesSm+3S2xYX+yx57fVXIM7RWS/ZbTjbdp4bXQL8ZlnKu7At2a1luEWKln37fmLkyGE0aHA/06a9zhNPPOXokkSkCFGzJDckdUTJZDbh6eOVp2NERISnjSqZzZnPDDWbzXh7e+PufnWJ7YiYcGJdYvmm2Tf4efln+jyLi4WRDeyz5HXqiJKBGatb+iW7DWcL0VVtO0/qiFJulwBPHVmCm2sp78K0ZLfFomW4RYqKM2fOMGXKeFasWEZAwN089tjjji5JRIogNUtiF54+XvQLfiZPzw0IqEVYWCh+fv7plgbP8Xl5XBr8RlndfDlv4xLh2R4vl0uA3+zLQBelJbtFxLHOnTtHo0b1MJtNvP762/Tp00+LN4hInqhZEhERkWLhl1/2ExBwN2XLlmXKlOm0atWGMmXKOrosESnCtBqeiIiIFGkREeE899zTtG79MF9++QUAPXr0VqMkIjdMzZKIiIgUSYmJibz//lwaNqzHN998xdtvv0v79o84uiwRKUY0DU9ERESKpHXrVjNp0lieeuppRowYQ6lSpR1dkogUM2qWJF9t2LCWoKBAoq7LEjIlx2JKjgTDSsSFlJXwjLhQEreUyvWx91awggFmczhldmXMWbI3W/OUcspRutnykkRE7CE09BQ7d+6gT59+dOnSjbp1A6hVq7ajyxKRYkrNkuSroKBAQnKZJVTKA/ydMg+mzZ4V4rPOWbK33OYp5TZH6WbJSxIRuRHx8fHMn/8ub745E2/vEnTq9Cje3iXUKIlIvlKzJPkqdUTJbDbjc02WkDk+HIyUxigZsLjDy10gNNm22+jMmPF29cbd2SPnne3Aljyl3OQo3Ux5SSIiebVz5w7GjBnOiRPHefrpQQwfPgpv7xKOLktEbgJqlqRA+Pj4pstQKrOrFk7xoSS7+VPpOByLDmWSlz9d2tieXxRz5auwym2OkoiIZG7Tpo34+PiyePEn1K79P0eXIyI3ETVLIiIiUqjExcXx7rvv4OPjS58+/Zg6dQZubm6YTCZHlyYiNxktHS4iIiKFxrZtm3nggft4440gTp+OAMDd3V2Nkog4hEaWRERExOEuX77E888PZNu2LTz44MN8+unnVK9ew9FlichNTs2SiIiIOExiYiIuLi5YLN54eHiyaNFHdOjQSSNJIlIoqFmSdI5tOMreoD0kRiXkav+YiOi0P284tpagvYE85HSG17wj8TJbMeKyyFAyW8EEETHhRBTm1RmKkA0bnAkKciUqqnh9wIiIKF6vR0RSGIbB5s1fMn78KObOnc/99zdi4cIlji5LRCQdNUuSzt6gPVwMOW/z81wsrgTtDSTk4lHW3wbVXVK2O3H1v5llKF1KtmI1Uv5scVHe0I0ICnIlJMQp5x2LKIvFcHQJImInf/0VwpgxI9i5cwfNm7dMFy0hIlKYqFmSdFJHlExmE54+Xrl6jovFlQYjGxF0fgYA3leWDUk2UjKUIOW/12coRVvNvBnljZ+XBxYXCyMbKG/oRqSOKJnNBj4+xauxsFgMRo7M3WiniBRu27dv4ckne+Pn589HH62gdeu2mnInIoWWmiXJlKePF/2Cn7HtSUtT/mM2mQEruPtjcgcIxeTuj8t1GUqlgGlXvsR+fHwMgoOjc95RRKSAGIbB8eN/U6VKVRo0aMjw4aN55pnn8fAomEBxEZG80tLhIiIikm+OHj1C166daN78Ac6fP0eJEiV56aVX1SiJSJGQp2YpMjKSZcuWMXXqVM6fP8/XX3/NyZMn7V2biIiIFFFRUZFMnDiWhx5qyH//nWThwg8pU6aso8sSEbGJzc3S0aNHadWqFatXr2bFihVER0ezbds2OnXqxN69e/OjRhERESlihg4dzIcfLmT48NF8++2PtGjR2tEliYjYzOZmaerUqfTs2ZM1a9bg4pKy5Nn06dPp1asXM2fOtHuBIiIiUjT88cfv7N+/D4BRo8aze/fPvPzyMNzd3R1cmYhI3ti8wMNvv/3G1KlTM2zv0aMHy5Yts0tRUnT8GjyK20MX4mFKYm8FKxjw3T4rE1dDZFw4ERcLpg7XDWvxCgrEFBVVMCfMBXNEuKNLEBEpEJcuXWTmzGksXryQli1b89FHK6hcuYqjyxIRuWE2N0tlypTh+PHj3Hrrrem2HzhwgLJlNRf5ZnN76EKqOqdf0nniajgcCnA1V8liyd8MJa+gQJxDjubrOfLKyOfXLiLiKFarlZUrlzN58nhiYmIYPXoCzz77vKPLEhGxG5ubpYEDBzJ27FgGDRqEYRj8+OOPrF27liVLljB06ND8qFEKMQ9TEpCSqRRhNWPGzOW4ZMDAbDbh4+OHxWJh5Mj8zVBKHVEyzGashSjc0LBYiM7n1y4i4iixsbHMmDGVBx54kAkTpuLvX9HRJYmI2JXNzVKPHj2oUKECixYtwt3dnZkzZ1K5cmWmTp1Ku3bt8qNGKQIirGZc2lwEwHCrBYTi4+NHcPDhbJ9nb1YfX84X8DlFRG4mFy6c5/XXpzNo0IvceuttfP31bkqXLuPoskRE8oXNzdK+fft48MEHadasWbrtCQkJ7NixgxYtWtitOBERESkcrFYry5Z9RGDgRBISEnnooWbceuttapREpFizeTW8J554gsuXL2fYHhISoml4IiIixdDhw3/Stm0zXn11CM2bt+KHHw7QqlVbR5clIpLvcjWy9OmnnzJ58mRMJhOGYdC4ceNM92vUqJFNJ4+Pj2fSpEls27YNd3d3+vfvT//+/TPd98iRI0ycOJHff/+d2267jTFjxnD//ffbdD4RERHJPcMwMJlMuLqmRIVs2LCV++9v6OCqREQKTq6apV69elG9enWsViv9+vVj9uzZlCxZMu1xk8mEh4cHNWrUsOnkM2fO5NChQyxdupTQ0FBGjBiBv78/bdq0SbdfZGQk/fv3p1mzZsyYMYP169fz4osvsnXrVq3AJyIiYmfJycksXbqY5cs/4YsvtlKlSjW2bPkak8nk6NJERApUru9ZuvfeewH46quv8Pf3v+G/MGNiYli1ahULFy6kTp061KlTh5CQEJYtW5ahWVq7di2enp5MnDgRJycnhgwZwrfffsuhQ4do2rTpDdUhtnONWIvXX4GYkqLAfHV58A0b1hIUFEiEnfOFcpOhVNQzjTZscCYoyJWoqKz/vzKZwDCyPkZEhD7EiMiN27fvJ0aOHMZvvwXTu/cTJCTE4+7urkZJRG5KecpZWrp0KceOHSM5OTlte0JCAn/88QebN2/O1XEOHz5MUlISd999d9q2evXqMW/ePKxWK2bz1dup9u7dS/PmzXFyckrbtnr1altLFzvx+isQ5+grmUZX/u2MtpoJCgok5JqsI3tlK9mSoVRUM42CglwJCXHKecdcsFiy6ahERLIxdeokZs2aSUDA3Wze/BX16t3r6JJERBzK5mZp7Nix/PDDDzRs2JAtW7bQtm1b/vnnH3777TdefPHFXB/nzJkzlC5dGldX17Rt5cqVIz4+nosXL1KmzNXVdf7991/uvPNOxo0bx86dO6lYsSIjRoygXr16tpYvdmBKupJphJmwZLiUbOXNKG+iroz8mM1mqlatZrdspdxmKBXlTKPUESWz2cDHJ/NmJ6eRJUhplEaOTMh+JxGRayQlJXH58iXKlCnL/fc3ZNasd+jd+4l0v6AUEblZ2dws7dq1i3feeYdGjRoREhLCk08+yR133MGMGTMICQnJ9XFiY2PTNUpA2vcJCek/7MXExLBgwQKeeOIJFi5cyJdffsmAAQPYvHkzfn5+NtXv4lI4/vJ3di4cdVwvdZaFyQSurpnXmLqP4e7LvcchNCoUf4tH2nZfX1/27fvF7jUZvr5E/ZHzNeaa4x6Fz9X3zuCPP+Iy3cfZ2YmkpORMH8uocF5fUrAK698zUnjs3v09r702lFtuuYWVK9fQpk1bG/6eEdHfM2Kboni92NwsxcfHc/vttwNQvXp1Dh06xB133EH37t3p06dPro/j5uaWoSlK/d7d3T3ddicnJ2rXrs2QIUMA+N///sfu3btZv349gwYNsqn+xMTkHH87X1ASEgrfP0ip741hZF3ftftc+2dy8dz8qqmoy+1rLK6vX/KPrhnJTHh4GJMmjWP16pXUq1efYcNGpV0rumbEVrpmxBaF5XrJ7W2YNucsVa1alT179gApzdL+/fuBlBXr4uPjc30cHx8fLly4QFJSUtq2M2fO4O7uTokSJdLtW758eapUqZJu2+23305YWJit5YuIiNzU4uLiaN78Ab755ivefvtdvvxyB3fddY+jyxIRKZRsHll68cUXeemll7BarXTq1In27dszaNAgjhw5QpMmTXJ9nNq1a+Ps7MzBgwepX78+APv376du3brpFncAuOuuu9i3b1+6bX///TcdOnSwtXwREZGb0u7d3xEQcDcWi4U5c+Zxzz31KFWqtKPLEhEp1GweWWrevDmbN2+mQYMG+Pn58emnn1K5cmV69uzJjBkzcn0cDw8POnfuzMSJE/n111/ZsWMHixcv5oknngBSRpni4lLu3ejRowdHjhxhzpw5/PPPP7zzzjv8+++/dOrUydbyRUREbiqnTv3HwIFP8uij7Vm5cjkAzZq1UKMkIpILNo8sAVSqVCntz7Vq1aJWrVoYhsHq1avp2rVrro8zatQoJk6cSL9+/bBYLAwePJhWrVoB0KRJE6ZPn06XLl2oWLEiH3zwAYGBgSxYsICqVauyYMECfHx88lK+5FJqblLUdflGpvgwTAYkE0p4MmBAhDkcIh1TpyPkJhfJFspIEhF7i4+PZ/78d3nzzZl4eVmYO3c+3br1cHRZIiJFiskwcl7uICkpiQULFrBjxw6cnJxo06YN/fv3Twuo+/XXX5kyZQqHDh3izz//zPeib8TZs5GFYoEHV1enQnOD27WWBiwgOiwKLz8LCywL0uUm5Vb16jXYvftnu9VUJqAWTmGhJPv5cz74sN2OeyMaN/a0Wy7StapXT2b37phMHyus14wUXrpmbm67d39H164defrpQQwfPgpv7xI5PkfXjNhK14zYojBdLyYTlCvnneN+uRpZmjFjBitXrqRTp064uroyf/584uLiGDRoEDNmzOCTTz6hatWqLF68+IYLl8Lj2twkn2vyjYy4UJyAZOC01YwZM96u3rg7e2CxWOyWr1SY5SYXyVbKSBKRG3Xy5D+sWLGM114bRePGD7Bv36/cckulnJ8oIiKZylWztHXrViZPnkznzp0BaNWqFcOHD+fvv/9m586dDB8+nCeeUIBdceXj40vwNSM6iVtK4e9kJTTZjEubi44rrBDw8TEIDo52dBkicpOLi4vj3Xff4Z133qBUqdL06dMPf/+KapRERG5QrpqlCxcucN9996V936BBA86dO8fhw4fZsGFDunuYREREpODs3LmdESNeJTT0FM8++wJDhw7HYrE4uiwRkWIhV81SUlISbm5u6ba5uLgwfvx4NUoiIiIO9Oeff3L77ZX59NPPqV69hqPLEREpVvK0Gl4qf39/e9UhIiIiuRATE8Ps2W+SmJjIuHGTGDToBZ5/fnDaoksiImI/uW6WwsPDiY+PT7ctIiIiw31KaqBERETszzAMNm/+knHjRhIREc7gwa8A6H5hEZF8lOtm6fr8JMMw6NOnT9pvsgzDwGQyFfqlw292WWUn3Rrlw/3RjShpLYsZMxHhoUQQCqSsfpe4pVTavj5ma77V57phLV5BgZiuqc8cEW7Xc9gjI0m5SCJSkBISEujXrydffbWd5s1bsmrVOqpUqeboskREir1cNUtfffVVftchBSQoKDDT7KQuPEppyqd9H2vEk9oSlfIAf6eMDVKs4YyLnevzCgrEOYtsJ8NONywHBbnaLSPJYikEoV0iUmxFR0fj4eGBq6srderUpV+/AbRu3VZT7kRECkiumqWKFSvmdx1SQLLKTvIIdwMDrFi5ZD7HLx5f4+sKFnd4pYuJ0OT0/zDHGs784/8Mde1cX+qIkmE2Y70228liIdpO+U32ykhSLpKI5BfDMNiwYS0TJoxh5Mix9OjRm7FjJzq6LBGRm84NLfAgRdf12UlLa04k+gJ4l47ixSPTc3y+C9i9UbqW1ceX89fUlx+UkSQihdHRo0cYNeo1vvvuG9q0aU/Dho0dXZKIyE1LzZKIiEghsXfvT3Tu3JZKlW5l+fLPad68laNLEhG5qZkdXYCIiMjNzDAMDhz4GYB77qnHtGmvs2vXT2qUREQKgTw3S1FRUfzxxx8kJCRkWFlNREREcvbHH7/TuXM72rZtzvHjf+Ps7MyTTw7IEAQvIiKOYXOzFB8fz9ixY7nvvvvo2rUrERERjBw5kgEDBnDp0qX8qFFERKRYuXTpImPGDKd58yacPXuGlSvXUblyFUeXJSIi17H5nqXXX3+dY8eOsXbtWnr06AHA4MGDGTVqFFOnTuX111+3e5GS0bENR9kbtIfEqATi4mKJjIzEas05/6inNeVnZo4w88H/Xsc5ORoTEHPRPstyZyezDKXr5TVTyZbsJGUkiYijzZo1g2XLPmbMmIk888xzuLq6OrokERHJhM3N0rZt23j33XepWbNm2raaNWsyZcoU+vfvb9fiJGt7g/ZwMeR82veeeNp2ACsknIUESqTb7OSef0thZ5ehdD1bM5Xykp2kjCQRKUi//RbMf//9R9u27Xnlldd4/vkh+Pn5O7osERHJhs3NUmpA3vWsVivJycl2KUpylhiV0tSYzCYiuTqqZDbnPLPSbDbj7e2NOxfS5mFaSWmU/J/Ov99uZpWhdL28ZCrZmp2kjCQRKSgXLpxnxoypLF26mAYNGtKmTTvKlCnr6LJERCQXbG6WmjVrxltvvUVQUFDatn///ZepU6fStGlTuxYnOfP08WIe8wkLC8XPzz9ddlJOEreUwt/JSmiyGZc2F/OvyOvkZ4aSspNEpLCwWq18+unHBAZOJCEhkUmTAunf/xlMJk0FFhEpKmxe4GH8+PGYzWbuu+8+YmNjeeyxx2jVqhUlSpRg3Lhx+VGjiIhIkbRs2VKaNWvJnj37efbZF3BxcXF0SSIiYgObR5a8vb2ZM2cOJ0+e5O+//yYpKYnKlStTtWrV/KhPRESkyDh37hzTpk2ie/fe3HdfA9au3YS7u7ujyxIRkTyyuVnq378/7du3p2XLljz00EP5UJKIiEjRkpyczEcffcj06ZMxDHjooWYAapRERIo4m6fh3XHHHSxcuJDGjRszaNAgNmzYQHS07hEREZGb019/hdCq1UOMGDGU9u078sMPB3jkkc6OLktEROzA5pGloUOHMnToUI4cOcK2bdtYuHAh48aNo2nTprRr1442bdrkR503hWuzk1JllaHkafXEjJmIiHAiyD6baMOxtQTtDSQqMYoO7rG85h2Jl9mKjznnXKa8yCpPKa8ZStfKKk9J2UkiUtASExNxcXGhXLnylC9fns2bv6JevXsdXZaIiNiRyTCMGwqbiYyMZPny5cybN4/Y2Fj+/PNPe9WWL86ejeTGXrF9uLo6kZCQfqn1TxsvSZedlBtnOMO7vAtA9eo12L375wz7NP60PiEXU/KN/rgNal+3OvhfSa6UaHvWpvNmp3Tj+tnmKSVVr8GFTOrMjcaNPbPNU6pePZndu2PydOzCLrNrRiQ7umbyR1JSEosXL+D99+eyZcvX+Pj4OLoku9E1I7bSNSO2KEzXi8kE5cp557ifzSNLAOfPn+err75i27Zt/Pjjj1SrVo1BgwbRvn37vBxOrrg2O8nTxwuAiIjwLDOUEk2J/OJ9ED8PfywWCyOzyCaKSkwZ4TGbzJR0ArCSbECE1Uys4cw//s9Q146vI7s8pbxkKF0ruzwlZSeJSH774YfdjBw5jMOH/+CJJ/rj6qrV7UREijObm6W+ffty4MABbrvtNtq1a8eoUaOoUqVKftR20/L08aJf8DMABATUylOGUmZ8PH3x8QTiQ8HdH5cHD+MCdm2UrqU8JREpTmbPfpOpUydSr159tm37hoCAux1dkoiI5DObm6W77rqLMWPGUKtWrfyoR0REpNBITEwkLCyUW2+9jRYtWlO+fAW6d++VYaRfRESKp1w1S6Ghofj5+WEymejZs2fatsz4+/vbrzoREREH+e67bxk1ahhOTs58880e/ve/Ovzvf3UcXZaIiBSgXDVLzZo1Y/fu3ZQtW5ZmzZphMpkwDAOT6eoKZKnfF/YFHkRERLJz6tR/TJw4lvXr19CgQUOmT5+V7t87ERG5eeSqWfrqq68oXbp02p9FRESKI6vVSteuHbl8+TJz586nW7ceapRERG5iuWqWKlasmPbnUaNGMXfuXEqUKJFun/Pnz/P000+zZs0a+1Z4E4qLi6Vx4/pERUURkYtsomtzlDLzgCmMibdBSadwzPH2rta+sspRSqU8JRHJDzt37qBmzVpUrHgL8+YtokqVqnh7l8j5iSIiUqzlqlnatWsXv/76KwD79u1j3rx5eHp6ptvnn3/+4dSpU/av8CYUGRlJyIX0OUUWiyXL/YP2BqblKGVmYlq20tUQWsM56+M5UlCQa7Y5SqkslkIQliUiRd7Jk/8wbtwoNm/eyPDhoxk2bKRWuRMRkTS5apYqV67MBx98gGEYGIbBgQMHcHG5mi1hMpnw9PQkMDAw3wq9mVybq+Tj45tthhKkz1Hy8fTN8HhJp3DAihUThpsfhrOF6Kp5zzrKT9nlKKVSnpKI3Ki4uDjmzn2b2bPfpFSp0syfv5jOnR9zdFkiIlLI5KpZqlSpEh999BGQMg1vzJgx2Y50iH34+PjalK3k4+lLcL+M+5fZVQviQzHc/Dj/YP7kHtmbcpREJD+dOvUvc+e+w9NPD2Lo0OH6N01ERDJl89LhgwcP5vLly1y+fDnTfbV0uIiIFEbHj//Nu+/OZurUGVStWp2DB/+gVKnSji5LREQKsRtaOvx6WjpcREQKm5iYGGbPfpN3332H8uUrcPLkc9SoUVONkoiI5CjXS4eXKVMm7c8iIiJFwc6dO3jttZeJiAjnhReG8NJLwzIsUCQiIpIVm5cOr1ixIpcvX8bNzQ03NzcOHz7M999/T506dWjYsGG+FSoiIpJbqUHpsbGx1KhRk1Wr1lGlSjVHlyUiIkWMychsPl02duzYwbBhw3jvvfeoWLEiXbp0wdfXl9DQUF599VX69OmTX7Xaxdmzkdj2iu3v2Iaj7Ju5h4TIBEzJsZiSI8GwEnXRE8NqJsp0mVnGm1QsY+afBRlXtwOIS4olMiESK1ashhWMK6vnZbIanjk+HBNWkt3883WBh03jDzBj0S1EJrpdObETVh8fm44REWHCajXh52fVAg/XcXV1IiEh2dFlSBFyM14zUVFRvPXW6/z99198+OEnaVPGFSybOzfjNSM3RteM2KIwXS8mE5Qr553jfrkaWbrW22+/zZAhQ2jUqBGzZs3Cz8+PjRs38vXXXzNlypRC3ywVBnuD9nAx5Pw1W9JPCUk0xYMBGFac4kMzPYYX4JUhjsgKWewP+Z+tNGPRLRxOrJquHMLydizlKImILQzDYMOGtUyYMIbz588xePArJCcn4+SUc26biIhIVmxulk6ePEnbtm2BlPuX2rRpA0D16tU5f/58dk+VKxKjUjKCTGYTllIpo0pXNuDqkciOS99AVMr3yW6ZjyxFxISn5DGZUvKVzJjxdvXG3dkj0/0LIlspMjml6TOTjJ/zGQxvbwz3zOvJjnKURMQWhmHQt293tm3bQps27ZgyZQa33Xa7o8sSEZFiwOZmyd/fn59++gkfHx+OHz9Os2bNAPjiiy+4/fbb7V1fsebp48VLc+bjFB+aborctIBVEAVWN98sp83VX1qLsOhQ/Lz807KVYq58OZqf+TS/hFpIGVrSVDoRyR+RkZdxcnLG09OTFi1a8+STA2jRorWjyxIRkWLE5mZpyJAhDB8+nOTkZB566CHq1q1LUFAQK1asYO7cuflRo4iISBrDMPj888+YNGkcffr0Y+TIsTz55ABHlyUiIsWQzc1Su3btuP/++4mIiKB27doAdOvWjQEDBlCuXDm7FygiIpLq998PMWrUMH78cQ+PPNKZPn36ObokEREpxmxulgA8PT357bffWLduHcnJyVSuXJl27drZuzYREZE0x46F0KLFA1SpUpVVq9bTtOnDji5JRESKObOtTzh69CitWrXi/fffJzQ0lNDQUBYsWEC7du04duxYftQoIiI3KavVyo4dWzEMg2rVqrNw4VK+/nqPGiURESkQNjdLgYGBNG7cmO3btzNnzhzee+89duzYQdOmTZk2bVp+1HjT2LBhLY0b1yciItzRpeTahg3ONG7sSUCAF2HWCo4uR0SKkV9/PUiHDq3o1asb+/fvA6BDh464uro6uDIREblZ2NwsHTx4kIEDB+LsfHUGn4uLCwMHDuSXX36xa3E3m6CgQEJCjqYsCQ5YLPmbi2QPQUGuhIQ4ERZmxkpKnom3KcrBVYlIUXbhwnlee+0VWrZsSnR0FOvWbaJ+/fscXZaIiNyEbG6Wypcvz8mTJzNsP3nyJF5eXnYp6mYVFZXSZJjNZqpXr8HIkfmbi2QPUVEmAMxmg4rmMGrxJ5O8X3dwVSJSlC1fvow1a1YxZcp0duz4jkaNmji6JBERuUnZvMBDjx49GDt2LC+99BJ33nknAMHBwcyePZtu3brZvcCbkY+PL7t3/+zoMmzi42PwD/VxCgsl2cOf87zh6JJEpAj55Zf9/PLLAfr3H8jTTz/LY489jo+Pj6PLEhGRm5zNzdKAAQOIjY1l1qxZXLp0CYBy5crx5JNP0r9/f7sXKCIixde5c+cIDJzIsmUfERBwF0888RSurq5qlEREpFCwuVkymUwMHjyYwYMHc+7cOdzc3IrEvTUiIlJ4GIbBkiWLmD59MoYB06a9Tr9+/dPdDysiIuJouf5Xaf369Wzfvh0XFxdatGhB+/btKVu2bH7WJiIixZTJZOKnn36gffuOjBkzUaHmIiJSKOVqgYelS5cyevRo4uLiiI2NZcSIEbz55pv5XZuIiBQjp0+fZvDgQaxatQKAd99dwFtvzVWjJCIihVauRpZWrFhBYGAgnTt3BmDbtm2MGjWKV155BZPJlJ/1FXuf74ll4gqIjAsn4mL2+244tpagvYFEJUYREVOwWUwbNjgTFORKVJQJU1wspshIsBpEWD0BMEdEYKbo5EOJSMFJSkpi8eIFBAVNw9nZiYceagaAk5OTgysTERHJXq6apX///ZeGDRumfd+sWTNiY2M5ffq0bsK9QRM/i+RwKIA1bVtW94AF7Q0k5OLRdNssLgVzv1hqnlIKrytfV3lbL2K68hoM3cMmIlecOvUfvXp14/DhP3jiif6MGjWWMmU0hVtERIqGXDVLSUlJ6W66dXZ2xs3NjYSEhHwr7GYRGZvSYJjN4OPjj8ViyTJfKSrxSg6TyYyPpy8WFwsjGxRMFtO1eUp+hIM1OeUBsxPepigmeb9Fsoc/hsVCdBHIhxKR/BUVFYXFYsHHx5eAgLuYPfs9AgLudnRZIiIiNtGyQ4WEXykzvwQfztW+Pp6+BPfL3b72li5Pyc+f88GHAQvwhrKVRITExEQWLHift956nbVrv6Ru3TuZPft9R5clIiKSJ7luljZv3pxuepjVamX79u2UKVMm3X6p9zWJiMjNZdeubxg9+jWOHQuhf/+BVKpUydEliYiI3JBcNUv+/v4sXrw43bayZcvyySefpNtmMpnULImI3ISWLFnE8OGv0KBBQ3bs+I477qjr6JJERERuWK6apZ07d+Z3HSIiUsTEx8dz9OgR6ta9k/btO2KxWHjssce1SqqIiBQbumepELp2ifBrFcRy4dcuEZ7u3BH68CMiV+3cuYPRo18jKiqK/fsPUb58ebp27e7oskREROxKzVIhlNkS4dfKz+XC0y8Rnsm5LQZEZfmwiBRzJ0/+w7hxo9i8eSNNmjzItGmv4+bm5uiyRERE8oWapULo+iXCr5Xfy4Vfu0S4j4+R/twWg5EjE0Arg4vctAYPHsSJE8dZsOBDOnXqoil3IiJSrKlZKsQcvUR4cHB05g+qWRK5qWzbtpkKFXy46657eOed9yhXrnyW4dkiIiLFiTkvT0pOTuabb75hyZIlXL58meDgYCIjI+1dm4iIONDx43/Tu3c3+vTpzurVqwC4/fbKapREROSmYfPIUlhYGAMGDODixYtcunSJ5s2b88EHH/DLL7+waNEiatasmR91iohIAYmNjeWdd97g3XffoXz5Cixe/Ant2z/i6LJEREQKnM0jS5MnT6ZevXp89913uLq6AvDmm2/SqFEjpk6davcCRUSkYMXFxbJ8+Se88MIQvv9+Hx06dNS9SSIiclOyuVn6+eef6d+/P05OV1dMc3Fx4fnnn+fQoUN2LU5ERArGsWMh9O/fl9OnT1O6dBl++ukgI0eOw9PT09GliYiIOIzN0/Dc3d05d+4clStXTrf9+PHjmseeS6bkWADM8eFgWB1Wx7WZSqa4WEyRkUQkpXwwMkdEUCagfqbPM0fkf96TiBSMqKgo3nrrdebNm4ufX0XCwk5RoUIF3N3dHV2aiIiIw9ncLPXo0YPx48czfPhwIKVJ2rt3L2+99RbdunWze4HFkSk5EvBM3yiZ8rTWxg1Jn6nkdeUrhbf1Ik5hodk+31BzLFKk7dr1DUOGPMf58+d45ZXXePHFl9UkiYiIXMPmZumFF16gRIkSTJw4kdjYWJ555hnKli3Lk08+yYABA/KjxuInQ5NkxXDyLvAyrs1U8iMcrMkAeDvFMsn7LZI9/LN8rmGxED1Sa4iLFEWJiYm4uLhQunQZ7rrrHiZNCuS22253dFkiIiKFTp5ylvr27Uvfvn2JiYkhOTkZb++8fdCPj49n0qRJbNu2DXd3d/r370///v2zfc5///3HI488wrx582jQoEGezltomMxY3XyBUAwnD4eV4eNj8A/1cQoLJdnPn/PBh4E3OM8bDqtJROwvMvIyr78+g927v2PLlp3UrXsnS5Ysc3RZIiIihZbNzdK6deuyfbxz5865PtbMmTM5dOgQS5cuJTQ0lBEjRuDv70+bNm2yfM7EiROJiYnJ9TlERG52hmGwevVKJk0aR2TkZV555TUMw3B0WSIiIoWezc3S7Nmz032fnJzMuXPncHZ25s4778x1sxQTE8OqVatYuHAhderUoU6dOoSEhLBs2bIsm6UNGzYQHR1ta8kiIje1Z555ivXr1/DII52ZNCmQW26p5OiSREREigSbm6WdO3dm2BYdHc348eNtCqQ9fPgwSUlJ3H333Wnb6tWrx7x587BarZjN6Rc8uHDhAq+//jqLFy+mQ4cOtpYtInJTuXTpIiaTlRIlytCtW3f69OlH06YPO7osERGRIsUuS7B5eXkxePBgPvzww1w/58yZM5QuXTot2BagXLlyxMfHc/HixQz7z5gxg0cffZTq1avbo2QRkWLJarWyYsUyGjasx7hxYwBo1aqtGiUREZE8yNMCD5k5fPgwVmvuM4NiY2PTNUpA2vcJCQnptu/Zs4f9+/ezcePGG67TxcUp550KkMl09b+urk5Zbrue87o1uE+biikqMsdzrIrtwMTIYURa0y/1HWG9mqlkJjzHc4pjOTvr5yLZO3jwF4YPf5W9e3+ia9duTJo0Wf8/i03094zYSteM2KIoXi82N0t9+/bFlPpp/oro6GiOHDnCk08+mevjuLm5ZWiKUr+/NucjLi6O8ePHM2HCBLvkfyQmJlOY7mtOrcUwICEhOctt1/MKnIJTyNFcnWMiL3OYrEfkvK0XMZHS6Fq9LFmeUxxPPxvJypkzZ2jdujlVqlRl3bpNNGrUBFdXJ10zYjNdM2IrXTNii8JyvVzXzmTJ5mYps+W6XV1dGTZsGA0bNsz1cXx8fLhw4QJJSUk4O6eUcebMGdzd3SlRokTafr/++iv//vsvQ4YMSff8gQMH0rlzZyZPnmzrSygWTFFRABhmM1Yf32z3jYwoBVYwk4yf+XS6x7xNUWmZSspOEilakpOTWbNmFR07Pkr58uVZtWo99erdi4uLi6NLExERKRZsbpYuXrzIE088wa233npDJ65duzbOzs4cPHiQ+vXrA7B//37q1q2bbnGHO++8k23btqV7bqtWrZg6dSqNGze+oRqKA6uP75VcpGz2CfCCMPDxM/FLsOW6Ry0oU0mk6Dlw4GdGjnyVgwd/oUSJkrRu3Zb772/k6LJERESKFZsXeNiwYUOGlerywsPDg86dOzNx4kR+/fVXduzYweLFi3niiSeAlFGmuLg43N3due2229J9QcrIVNmyZW+4DhGRouTs2bO88sqLtGnTjMTEJL74YhutW7d1dFkiIiLFks1dz5NPPsmkSZPYvXs3x48fJzQ0NN2XLUaNGkWdOnXo168fkyZNYvDgwbRq1QqAJk2asGnTJlvLExEp1n74YTcbN25g+vRZbN/+LQ0a3O/okkRERIqtPIfSfvfddwBpiz0YhoHJZOLPP//M9bE8PDwICgoiKCgow2NHjhzJ8nnZPSYiUtzs3fsTW7duYty4SXTo0JEHHniQUqVKO7osERGRYi9XzdK+ffu4++67cXZ25quvvsrvmkREBDh9+jRTpozns88+JSDgbqKiIrFYvNUoiYiIFJBcNUtPPPEE33//PWXLlqVixYr5XZOIyE3vww8/YOrUiTg7OzFr1jv07v0ETk5FL59CRESkKMtVs2QUpmAiEZFiLHVK85kzp+nSpRujRo2lTBktZiMiIuIIub5n6fogWhERsZ/w8DAmThxL1arVeO21Ubz22ij9vSsiIuJguW6WHnvssVwtGa57mkREci8hIYGFC+cxa9YMPDzcad68JaBfUImIiBQGuW6WnnrqKby9vfOzFhGRm8rFixdo374lf/11jP79BzJixBhKlizl6LJERETkilw1SyaTifbt2ysEVkTEDk6fPk358uUpVao0jzzSiQ4dOnPHHXUdXZaIiIhcJ1ehtFrgQUTkxsXHx/P227O477472bIlJXR75MhxapREREQKqVyNLD366KO4ubnldy0iIsXWzp3bGT16OP/8c4KBA5+jSZMHHF2SiIiI5CBXzdL06dPzuw4RkWJr/fo1DBz4JE2aPMiSJZ9Sq1ZtR5ckIiIiuZCraXgiImKbuLg4vv32awDatGnPkiWfsnr1F2qUREREihA1SyIidrZ162YeeOA++vXrxcWLF3Bzc6Nduw5aDlxERKSIUbMkImInf//9F717d6Nv3+5UrlyFHTt2UapUaUeXJSIiInmU65wlERHJ3syZ0/jzzz9YvPgT2rd/RCNJIiIiRZyaJRGRPDIMgy+//AJnZ2fatGlHYOBMPDw88PT0dHRpIiIiYgeahicikgfHjoXQvfuj9O/fh61bUzKTypYtq0ZJRESkGNHIkoiIDWJjY5k1awbz5s3Fz68iH3/8Ga1bt3V0WSIiIpIP1CwVIa4b1uIVFIgpKorV4U2YwEQiI0phDfDK9nkREbpvQsRenJ2d+e67bxg6dDgvvPAS7u7uji5JRERE8omapSLEKygQ55CjAExgIoepDVYgLHfPt1iM/CtOpBg7cuQwY8eOYMKEqdxxR102b96Jk5OTo8sSERGRfKZmqQgxRUUBYJjNRFIKrGA2WfHxzfm5FovByJEJ+VugSDETGXmZ11+fwQcfzKNSpVuJiYkBUKMkIiJyk1CzVARZfXyx4gNh4OMLwcHRji5JpNj56acfefrpJ4iMvMyIEWMYNOhF3NzcHF2WiIiIFCA1SyIi14iKisJisVC5chWaNn2YkSPHcsstlRxdloiIiDiAlg4XEQEuXbrI6NGvcf/9d3Px4gUqVKjA3Lnz1SiJiIjcxDSyJCI3NavVysqVy5k8eTyxsbEMGzYST8/sV5gUkaLBarWSnJzk6DKKOTOJiVZHFyFFRsFdL05OzpjNNz4upGZJRG5qr732Mh9/vIQuXboxYcIU/Pz8HV2SiNwgwzC4fPk8sbFRji5FRBzIw8NCiRJlMJnyHqOjZsnB4pJiAYiICSdgaa20P2fm89gOTORlIiNKEYGyk0Ty6sKF81y4cIEqVaryxBNP8dhjj9OoURNHlyUidpLaKFkspXF1dbuhD0qSPZMJDCWTSC4V1PViGAYJCfFERV0AoGTJsnk+lpolB4tMiARSpgqERYeme8ziYkn3/cTIYRymekq2Uuo+yk4SybXk5GSWLfuIadMmcccdAXz++XoCAu52dFkiYkdWa3Jao2SxlHB0OcWemiWxRUFeL66uKSvYRkVdwNu7dJ6n5KlZcjBraudjAj+vq9N/LC4WRjYYm27fSCOleTKTjI+fSdlJIjY4cOBnRo58lYMHf6F7916MGzfZ0SWJSD5ITk4Grn5QEpGbV+rfA8nJSZjNrnk6hpqlQsJsMhPc73Cu9vUzn+aXYEvOO4oIALGxsfTu3Q0/v4ps3Lid++5r4OiSRCSfaeqdiNjj7wE1SyJSLCUnJ/Pxx0vo0KET5cqVY+3aTVSvXgMnJydHlyYiIiJFhHKWRKTY2bv3J1q2bMqIEUP56qttANSqVVuNkogUerGxsSxc+D69ej1Gs2aNad++OWPHDufvv/+y63k2bfqCrl0fAVKmKTdpUt8ux01MTGTDhrVZPv7ii8/QpEn9tK+WLR9k6NAX+e+/f+1y/pzs37+PEyeOA+nfg/xgGAYvvvgM//xzIt32RYvm06RJfX7+eW+G57z44jMsWjQ/w/bMfkbx8fEsXryAnj270KxZYx5/vBOLFs0nPj7Obq8hPj6e6dMn06bNQ3Tq1Jrlyz/Jdv9vv/2a3r270rLlAzz33ACOHLk6ayo2NpYZM6bSrl1z2rR5mKCgQGJiYoCU66Z//95cuHDebrXbi5olESk2Tp8+zeDBg+jQoSXOzk5s3vwV3bv3cnRZIiK5EhMTw3PPDWDHjq08//wQPv30c954Yy6enl4891x/QkNP5ct569YNYP36LXY51o4dW/noo8XZ7tOjRx/Wr9/CunVbmD//Q0qUKMnIka9iFMCd/y+99Bznz58DoHnzlixc+FG+nWvz5o34+vpx2223p9u+Y8dWKla8hS1bvszzsRMTExkyZBDffvs1gwcP5ZNPVvLyy6+xbdtmxo0bdYOVX/Xee+9w+PCfvPPOPIYOHcmHHy7k6693ZLrv33//xaRJY+nT50mWLFlO9eo1GD78JeLiUpq32bPf4PDhP3jzzbm88877/Pnn78yZ8xYALi4uPPZYd957b7bdarcXNUsiUmyEh4fy1VfbeOON2WzevJN77rHPb0pFRArCkiULuXDhPB988DFNmjTF19ePWrVqM3r0BGrVqsNnny3Ll/O6uLhQtmw5uxwrNw2Ph4cHZcuWo1y5clSpUpXBg1/hxIm/+euvY3apIbfc3NwpXbp0vhzbMAyWLl1E585d020/cuQwp079R79+A/j2251pIyu2+vTTjwgNPcWcOfNo1KgJ/v4VadSoCYGBr/PDD9+zb9+PN/waYmNj+eKL9bz00qvUrFmLpk0fplevvqxevTLT/fft+5HKlavQtm0HKla8hUGDXuTcuXOcOPE3AM7OLgwdOpxatWpTs2Yt2rfvyG+/HUx7fqtWbfn++12Eh4fdcO32pGZJRIq0PXu+Z9Cg/iQlJXHnnXdx4MAf9O37pKbciUiRYrVa2bRpI92798bb2zvD4+PGTeb554cAKdPHnnuuP6NGDaN166Zs27aZ6Ogopk2bRIcOLXnoofvp1esxdu36Ju35Z8+e4dVXh9CiRRP69+/NqVP/pT12/RSviIhwRox4hebNG9O16yMsXrwgbZXBTZu+SJsq1r59c1q3fog5c97EMAwOHPiZadMmER4eRpMm9QkLSx+JkhUPD48M2zZt+oLevbvSrFljBgzoy8GDB9Iei4+P5733ZtOlS3tatGjCiBGvEBFxNaNy1aoVPPZYB5o1a8SAAX0JDj4IkDblbsiQQSxaND/DVMSuXR9h7drP6dy5LS1aNGHKlHEkJFxddXjbts08/ngnmjdvzMSJY5gwYXSmU+YA9u79kbi4OOrUuSPd9h07tlKtWnUeeqg5SUlJfPvtzly9R9fbvHkj7do9QokSJdNtr1atOnPnLqBOnTszPCcsLDTdFMhrvzZt+iLD/seOHSU5OYm6dQPStt1551388cfvWK3WDPuXKFGS48f/5tdfD2K1Wvnyyy/w8vLC3/8WAF59dQR33nlXWi3bt2/hrrvqpT3fxcWFe+9twPr1a/L0nuQXNUsiUiSFh4cxaFB/Onduxz///MO5cynTKtzd3R1cmYiI7U6d+o+LFy9kmf1Wrlw53Nyu/v3222+/UrlyFebPX8J99zXknXfe4N9//+Gtt+by8ccrCQi4m6CgKSQmJgIwduwIrNZkFixYSu/e/Vi5cnmm5zEMgzFjhlO6dBk+/HAZo0dPYPv2LXz88Ydp+xw69CsnT57g/fcXMXTocFatWsHPP/9E3boBDBnyKhUq+LB+/RYqVPDJ8XUnJCSwdOliqlatTtWq1YCURumtt2Zemc61jPr17+O1117izJnTAMyaNZ1du75m7NhJzJv3IUlJyYwa9SpWq5WjRw/z3nvv8OqrI1m27HMCAu5i/PgRWK3WtCl3gYEz6dmzb4Zazp49wzfffMUbb8whMPB1vvlmZ9pUueDgg0yfPplevZ5g8eJleHh4sHPn9ixf108/7aF+/XvTrcZmGAZffbWNxo0fxNPTk3r17mXz5o05vkfXi4uL47///qV27f9l+nhAwN14enpm2J76c8nsq3nzlhn2P3fuLCVLlsLFxSVtW5kyZUlIiOfSpUsZ9m/evBWNGjXm+eef5uGHG/Luu28zdWoQJUqkzzubOnUC3bp15MKF8zz11NPpHrv33gb89NOeXL0PBUWr4YlIkfPxx0sYP340Hh7uzJ79Po8/3jPPYXMicnPYcGwtQXsDiUqMKrBzpmYmPlK1c477Xrp0ESDdB8t9+35i9Ohhad/7+PjxyScpU6BMJhP9+vVPa6DuuuseevToTZUqKQ1Hz559+OKLdZw/f47o6GgOHfqVzz/fiK+vL1WqVOXIkT/ZuTPjvSf79+8jPDyMBQuWYDabufXW23nhhZeZNm0STz6Z8sHWarUyfPgYvLws3Hbb7axYsYw///yDe++9H4vFgtlsznZa38cff8iKFSkLBcTHx2MYBlOnBqU1Fp9/voKuXXvQtm0HAJ57bjAHDx5g9eqV9Or1BFu3bmLWrNlpU60nTJhCly7t2bfvJ+Li4jCZTPj6+uLn58/Agc/TqNEDWK3WtCl33t4lMm0mkpKSeOmlYVSpUpWqVavRoEEj/vzzDzp2fJS1a1fRrFlLOnd+DIBXXx3JTz/9kOVrPHr0CPfdd3+6bb/+Gszp0xE88EBTAJo2bcbMmYGEh4fh6+uX5bGuFxUVCYCXl20xMk5OTjZNt4yLi0vXKAFp3ycmZsz5vHz5EufOneOVV4ZTp05d1q37nGnTJrN48SeULl0mbb/evfvRuXNX5s2bw7BhQ1i06JO0f8Nvv70yx46FkJycXGhmiKhZEpEiIzExERcXF9zd3enZszcjRoyhZMlSji5LRIqAdw++Q8jFowV/3l/eyVWz5O2d0iSlfhCGlIUXPvzwUwC+/XYna9d+nvZY6dJl0o00tWnTnu+++4YNG9byzz8n0lYhs1qtnDhxnBIlSuLr65u2f61adTJtlv755ziXL1+ideumadusVivx8fFpDV3p0mXSfVD39PQiKSkpx9eYqnPnx+jatQcAsbEx/PDDbiZMGM2sWbOpX/8+Tpw4wVNPDUz3nDvuqMs//xzn339PYrVa+d//rk5vK1GiJLfeehv//HOcjh27UKVKNZ54ogc1atSkSZOmdOz4KM7OufvIW6nSrWl/9vLyIjk55XX99VcInTp1SXvM2dmZWrUyH9kBuHjxQoZ/n3bs2Iqvrx81atQCoEmTprz++jS2bPkyrRF1dnbO9L4vwzDSmofUayUyMjLDftkJDw+nb99umT722mujadWqbbptrq5uaSOTqVK/z2wWx/vvz6Zq1Wo89tjjAAwfPobevbvy5Zcb6NPnybT9KleuAsDkydPp3LktBw8eSGt8S5YsidVq5fLlS+kaLEdSsyQihd6pU/8xYcIYXFxceP/9D+jWrQfduvVwdFkiUoS8ePfLzPhpaoGPLL1w90u52rdixVsoWbIkv/32K7Vr1wFSPpDeckslgAwfHF1dXdN9P3XqBH777VfatGlH585dKVu2HIMGPZX2+PUfwF1cMv8ImJyczK233s6MGW9keCy1Qbp+tCGz42fH27tE2usCqF69JgcPHmDdus+pX/++DK8tpS4rycnWTB+79nF3d3cWLFjCwYMH2L17F5s2fcG6datZtOhjypevkGNt17+21Nfl5OTM9S8x+9dsSndfT3JyMl9/vYNLly7StOnVYHSr1cqWLZvSmiWLxZuoqIzXaFRUZNq9bG5ublSuXIUjR/6kWbMWGfadPn0y9evfR8uWbdJtL1euXFrzfb0yZTI2JuXLl+fSpYskJSWlNZvnz5/Dzc0NiyXjfXVHjhyma9fuad+bzWaqVatBeHg4iYmJ7N69i/vua4Cnp+XKOctSokTJtCY85f1IeU9NpsIzW0TNkogUWvHx8cybN5e33nodi8WbiROnOrokESmiHqnaOVcjPI7i7OxM+/YdWbVqOR06dMTT0yvd46n362QmOjqK7du3sGDBkrRG64cfvgdSPtBXqVKVyMjL/Pffv2lNytGjRzI9VqVKtxEREU6pUqWxWFI+1O7b9yObNm1k7NhJOb6Oa+/RsYVhGCQnpzQXt956G7//fogHHngo7fHff/+NgIC7qVjxFpycnPj9999o0KAhkDKF8b//TnLrrbdx6NCv7N+/j379BnDPPfV59tkX6dixFb/+epDmzVvlqTYgrTlJlZyczLFjR6lWrXqm+5cpUyZdE7B//z4uXrxAYODMdKNXe/f+yNy5b/Pbb8HUrRtAtWrV2b17V4bj/f77IapXr5n2fatW7Vi58lP69Hky3YIgISFH2bx5Iw891CzDMZydndM1qTmpXr0mTk7O/P77IQIC7gLg118PUrt2nUynvpctWz4twyrVyZP/0Lr1/zCZTAQGTmTEiDG0aJHSxIWHh3Pp0kVuu61y2v6XLl3EycmJkiXTL1zhSIWnbRMRuUZ8fDzNmzchKCiQfv0G8MMP+9P9xkpEpLjp3/9ZypQpy7PPPsXXX+8gNPQUf/xxiKCgQBYtmp/2gfV6rq5uuLt78M03OwkLC+Wnn37gzTdfB1KmTd1+e2Xq1buP6dMnc+xYCN99902Wyz/fd9/9+Pr6MnnyOP766xjBwb8wc+Y03N3dc3UPibu7O5GRl/n335NZTs2LjY3l3LmznDt3loiIcNasWcX+/fvSRkm6d+/N6tWfsWXLl5w8+Q/vvz+Hv/4K4ZFHOuPp6ckjjzzKW2/N5MCBnzl2LITJk8dToYIP997bADc3Nz78cCFffLGOsLCUOInY2FiqVk1pajw8PDh+/K9MR2+y89hjj/PVV9vYuHEdJ0+eYPbsNwgLC82yOaxevWa6pdB37NhK5cpVaNq0GVWqVEv7evTRbpQoUTJtoYd27R7hxIkTvP32LE6cOM4//5xg1aoVrF27iscfv5ob+PjjPShbthyDBz/LDz/s5tSp/9i5cwcjRrxC48YPcv/9jW16fZlxd3enbdv2zJo1jT///J1du75h+fKP083sOHfubFoIbseOndmwYR1btnzJf//9y/vvzyEiIoy2bTvg7OxMx45dmD//PYKDD3L48J9MmDCKJk2aUqVK1bTjHTsWQo0aNfPcdOcHjSyJSKHy778n8fHxxc3NjWeeeZ4GDRpSs2YtR5clIpLv3N3dmTt3AStXfsqSJYv477+TuLi48r//3cHUqTN58MGHMn2ei4sL48dPZu7ct/n88xX4+VWkX7/+LFz4PkePHua2225n8uRpBAUFMmjQU/j6+tGtWw++/HJDhmM5OTkxY8abvP326zzzTD88PDx5+OEWvPhi7qYT1qt3LxUrVqJfvx68994Hmd7Xs2LFJ2kLPLi4uFCxYiWGDh2eNm2sefOWnD9/jg8+mMf58+eoVq0Gb745Ny3c9cUXX2bu3LcZO3YEiYmJ1K9/H2+//R6urq5Ur16TUaPGs2TJB7z11kx8fHwZN24yt9+eMnrRtWsP3n13NqdO/Ue1ajVy9ZoA7rjjToYOHcHixQu5dOkiDz/cgjvuuDPTKYkA99/fkMDASRiGQWJiIrt2fZ3hPixImVLXrt0jbNy4npdeGkaFCj7MmTOfBQveuxKLkcjtt1dh7NjJNGzY+JrnpSxw9OGHH/Dmm0GcO3eOChV8eOSRzvTq1dduzcbgwUOZNWs6Q4YMwsvLwoABz9K06dVRq06d2jB69ATatXuE5s1bERsbw8cff8jp06epXr0G77wzL20K6bPPvoDZbGL8+BHExsbRtOnDvPzysHTn+/XXg3Zp9OzJZBREXHIhcvZsZIY5pwVtQZVJJEV5E2W6zCzeBAPMJc2Eh1zMsO+GDc4EBbkSFWUiIszAihMVzWH8Em7bCihS9Lm6OpGQkOzoMvJNXFwcc+e+zezZbzJu3CQGDnzO0SUVecX9mhH7Kw7XTGJiAufOhVG2rB8uLpnf3yL2YzLh8M9VBeGPPw5hsVi49dbb07b16fM4vXr1pV27RzLsn5ycTM+eXRg9egJ33XVPAVZauGV3vcTGxvLoo2358MNP8fPzt8v5svv7wGSCcuUy3nt1PU3Dc4DLMSlXidUArlwwJrfMfxRBQa6EhDgRFmbGypVVUEwFd3OqSEHYunUzDzxwH2+99ToDBz6Xaf6FiIiIoxw69BuvvfYyv/0WTGjoKT76aDGnT0ek3Td1PScnJ/r0ebLQBawWZtu2baZRowfs1ijZi6bhOYD1mo7aXNKM1cWKd+vMO9uoqJRhVLPZwI9wvK0XmeT9FpBxlRqRomjPnu/p27c7Dz3UjOXLV2d5s6yIiIijdOnSjbCwUMaMGU5UVBTVq9dg1qx3ss0t6tChE5s3b+TEieNp0wAlc4mJiaxZs4o335zj6FIy0DQ8B5jpMxGLUYIo02WWzVxBWHQofl7+BPc7nGHfgAAvwsLM+PlZ+ZdKOIWFkuznz/ngjPtK8VYcpsekiomJYfPmjTz22OMYhsHu3d/RuPEDheqGzuKgOF0zUjCKwzWjaXgF62aZhif2UdDXi6bhiUiRYhgGGzduoEmTe3n55Rf4558TmEwmmjR5UI2SiIiIFDpqlkSkQBw7FsLjj3emf/8+1K79P7799se0lY1ERERECiPdsyQiBWLlyuUcP36cjz/+jNat2zq6HBEREZEcaWRJRPKFYRisW7eaRYsWAPDKK6/x/fd71SiJiIhIkaFmSUTs7vDhP3nssUd45pmn2LfvJwzDwMPDA3d3d0eXJiIiIpJrapZExG6SkpIYN24UDz/ciNDQU6xYsZp58xZp8QYREREpktQsicgNMwwDwzBwcnIiNPQUI0eO5dtvf6RZs5aOLk1EpMho0qQ+Bw78nOljixbN58UXn8nVcQIDJxIYODHLxy9cOM/OnTvSbUtKSuLTTz+mX7+etGjRhDZtHuLVV4fw668H0/YJCwulSZP6aV8PPngfHTu24b33ZpOUlJS2X9euj9CkSX0OHjyQ4dw//riHJk3qZ1sfwPz57/LFF+vSbTtw4GeaNKnPwoXvZ9g/u/cns/d1y5YvGTiwHy1bPkCnTm2YOnUCERHh2dZkq5UrP6Vz57a0bPkg06dPJi4uLst9//77GC+++AwtWz5Az55d2L59S6b7/f77IR588D7CwkLTtg0e/CzHj/9t19rlKjVLInJDfv/9EJ06tWXr1s2YTCY++GApL730Km5ubo4uTUSk2OjZsy/Tpr1ul2O9//4cfvjh+7TvrVYrw4e/zPLlH9OrV18++ugz3n33A6pWrcrLLz/PoUO/pnv+woVLWb9+C59//gUTJkxhx46trFjxSbp9nJ2d+f77XRnOvWvX1znONjh58gS7dn1N27Yd0m3fsWMrFSvewrZtm7mRmNA5c95kzpw36djxUT788FOmTZvFuXNnefHFZ7lw4UKej3utb775isWLF/Daa6OZPft9fv/9EO+9906m+yYkJDBixFBq1KjJkiXL6d27H4GBEzl8+I90+yUlJTFz5lSsVmu67U89NZA33phhl7olIzVLIpInly5dZPTo12jevAnnzp2lVKlSAJpyJyKSDzw9PSlRoqRdjnV9o7Fu3Wp+/fUg8+d/SOvW7fD3r0jVqtV4/vmXaNWqLR999GG6/UuVKk3ZsuWoUMGHevXu5dFHu7Fz5/Z0+wQE3MPu3embpZQQ8l3UqVM32/o++WQpbdp0wNn56qLNSUlJfPPNTvr1G0BERDi//LI/Ly+d4OCDrFy5nGnT3uCRRzpzyy2VqFPnDqZPf4Pk5CRWrvw0T8e93qpVK+jWrSeNGz9A7dp1eO210Xz55YZMR5dOnPibsLBQnn76OSpWvIUOHTpRpUo1DhxI/xqXLVuKp6dXhuffc099Llw4T3DwL3apXdJTsyQiNvvtt19p2PAeli9fxrhxk/n66z3cf38jR5clIlJsXT/NbO/eH3niie40a9aYV18dwltvzUw3tS06OpoJE0bRvHljunRpz7ZtW9KOs3nzRjZv3kjXro8AsHHjetq1ewR//4oZzjto0GAmTJiSbW0eHhkX72nUqDGhoaf4558Tadt+//03vL1LUqnSrVkeKzIykq++2saDDz6UbvvevT8SHR1FkyZNqVPnDrZs+TLbmrKyZctGateuQ0DAXem2u7u7M2PGGzz2WPdMn5c6tfD6r8ymEyYnJ/Pnn39w1133pG2rU+cOkpKSOHbsaIb9U5vgjRvXYbVaOXToV06ePEGNGjXT9jl58h/WrFnFiy++kml9jRs/yNq1n+f08iUPlLMkIrl25swZypcvT/XqNejSpRsvvPASfn7+ji5LROSmcurUf4wcOZQnnuhPs2Yt2LZtC0uXLqJNm/Zp++za9TXPPz+EZ555gXXrVjNjxmQaNWpCz5590xqYV14ZTmJiIiEhR+jd+4lMz5U6ayArERHhfPHFetq2bZ9uu7d3CQIC7ub7779NCyDftetrHnigKWfPnsnyeAcP7qdkyVIZQsu/+mordesGUKJECZo0acqSJYt45ZXheHh4ZFvf9Y4dO8r//ndHpo/VqFEry+ctXPgRVmtyhu1ubhkbxaioSBIS4ilXrnzaNmdnZ0qUKMnp06cz7O/r68ezz77Ae+/N5t133yE5OZn+/Z+hfv37gJQRuZkzA+nf/xnKlCmTaX333tuACRNGYxiGZnjYmZolEcnRhQvnmTZtCp99toxvvvmBKlWqMnVqkKPLEhHJNdeItXj9FYgpKarAzmk4W4iuOpYEn852Pe7Gjeup/f/27josquwN4Ph3KFEJFVBBELsLY1FB3QXFVlTsxl4x1+5Cxe5cu3OttVDX7sLEQEFFVFABEaVmfn/wc9ZxRgVr1H0/zzPPOueee+5779wd7jvn3HMLFqZNm/YAtG/fmTNnTmnUKVKkGM2aJSdArVu3Y82aFYSEBFO4cBH1PaUZM2YkIiIClUqFhYWFet1790Jo166FRnv+/kfU/27ZshEKhQKlUklcXBz29g5UraqZLAG4ulbin3/20bx5awCOHDnE8OFj2LRp/Xv37ebNG1qJUlzca44cOUy7dsk9a5UquTF37kwOHTqgkSCmxIsXMaRPb5aqdSD5WKXUm6F2xsbGGuXGxsYkJMRr1U9MTCQkJJi6detTo0YdLl48x4IFcyhRoiQlS5Zmx46tJCUlUqdOPR49CtO5zZw5cxEdHcWjR2HyI+YXJsnSd2jbNiP8/EyIiVHw+LH8OiD0JykpiVWrljN27EgSEhIZMmQE2bM76jssIYRItXTB0zF6qT0E6quKS97ul06WgoJuUaBAIY2yIkWKEh0drX6fLdu/Q+rMzJKTg/j4OK22zM3NgeQk4g07u2wsWZJ87861a1cYNWqoxjoTJ07HxiYzSqWS58+fsnTpIrp2bc/SpWswMTFR16tQoRKzZ08jMjKS58+fERcXpxX3u54/f46lZQaNsmPHjhIb+5IKFX4FwN7egdy587Br19/qZMnIyEjnpA9vJkN4c/+TpaUlL168+GAMurRo0YjHj7UTFQ+P6vTtO0ijzMQkORlNSEjQKE9ISND5vMHdu3cQGHidFSvWoVAoyJ+/AMHBd1m1ajmOjjlYsGAO06fP+WCP0ZuhfM+fP5Nk6QuTZOk75Odnwq1bhhplZmYq+HY/hgkBwNixo5g5cyqNGzdjyJCRZMmSRd8hCSHEJ4nN0ZP0QWO+ec9SbI4eX7xdQ0NDQDMxeDdRMDDQvI7QVQcgTZo05M6dlytXAnBzqwwkJxb29g4APHnyWGudrFlt1Rfkjo6ODB6cHU/Papw5cwoXlwrqera2duTIkYvjx48QERGudR+SLm96rN62b98eAJo2ra8uUyqVKBR3ePz4EVmyZMXMzJyYGO3PNiYmOTEyM0tOCvPnL0Bg4HWd216/fg3Pnj2lc2cfrWWTJk3XmB79jfTptSdcsLS0xMQkDU+fRqh7yRITE4mOjsLKylqrfmBgILlz59ZIhvLmzc/lywGcPn2SqKhIOnVqC/z7GbZs2YhWrbxp1cr7/+XJx8zAQKYj+NIkWfoOxcQk/89iYKAiSxYVZmYqBgyIhyF6Dkz8J0RERBAScpdSpcrQtm17PDyq4+xcVt9hCSHEZ4nP4vnFe3j0JUeOXFy+HKBRduNGoM4JGnRRKBQaiVPduvVZsGA2jRs3J0uWrBp1P3R/0b+S29J1T0+FCpU4fvwIjx8/pkuXbh9tKVOmTNy7F6J+//JlDCdPHqdFizZ4eFRTl0dFRdGjRxf27NlJq1be5MmTl3v3gomOjtYYUnj16hVMTU3VoyI8PKrz118buXTpIsWKlVDXi42NZf361eqE8V1Zs9p+NPY3DAwMKFiwEJcuXaRkydL/j+MyhoZG5MmTT6u+tbU1ly9f1Ci7dy8YW1s7KlX6jaJFi6vLw8Of0K1bJyZOnE7u3HnU5ZGRkQBkymSV4jhFykiy9B3LkkVFQMDLfwskWRJfUWJiIsuWLWb8+DHY2WXj4MHj2Ns7qH9dFEII8fVdv36V+HjN+1renlUNkpObtWtXsnLlUipW/I2DB/cTEHCBbNnsU7QNU1NT7twJIjz8CTY2mfH0bMCZM6fo0qUdHTp0oWjR4rx69Qp//91s2LBGI6kAiIx8rh5uFx0dxYIFc8mQIYM6MXibq2sl1q1bRZo0aShe3OmjseXNm5/Nmzeo3x8+fJCkpEQaNmyi1Svj7FyOXbt20KqVN0WLFidnztwMHTqAjh1/J0OGDNy+fZNZs6bRoEFj9TC8IkWKUauWJwMG/MHvv3fHyakUT548ZuHCuRgYGKjvr/pc9ep5MXHiWHLlyoONjQ2TJo2jTh1P9TC86OgoDAwMMTMzw8OjOitXLmXOnBnUrVufy5cD2LZtC+PGTSJduvQa04Un9yomJ29vTyUfFHSbTJmssLHJ/EXiF/+SZEkIwalTJxkw4A+uXbtCixatGThwmMymI4QQejB37kytsrVr/9J4nzWrLaNH+zFr1jQWLZpPmTLOVKhQSeO5RB9StWpNBg36gzZtmrJjxz4MDAwYO3Yi27b9xebNG5gyZQIKhYK8efPRr99gPDyqa6zfocO/CUX69OkpVqwEU6bM0jlxQoECBTE3t6BUqTLqC/0PKVmyNDExL7h3L4Ts2R3x999DuXIuOoeveXp60b9/L65cuUyRIkWZPHkmc+fOYNCgP3jx4gU2NpmpU6eeVgLUt+9AcubMyfr1q5k2bSLm5haUKePMyJFjte6X+lSVK1clLCyMiRPHkpAQT6VKbnTp0l29fNCgvtja2jF48Ajs7LIxdepsZs+ezl9/bSRLlqwMGDAUZ+dyKd7epUsX+eWXsvK3+ytQqD7nEcg/oIiIF+h7jydkGYGZyoIYRTSrJqwl7OVDbNPbEdA6EIDixdMTFmaAra1So2cpU/ECGIY9JMnWjmcBgfoKX+iJiYkh8fHaQxw+l1KppGJFZ9KnT8+4cZN0/jIofkxf65wRP6+f4ZxJSIjn6dMwrKxsMTY2+fgKP6A7d26TmJioMdV13749KFCgEO3adfqmsSgUfPHrKl/f5ASibdsOX7bhn5RKpaJRI0+GDBmp9fyo783XOF8+5EPfBwoFWFubf7QNuQtMiP+gxMRE5s+fzfXr1zAwMGD9+i3s2nVAEiUhhPgBhIY+oGfPrpw5c5JHj8LYvn0L586doVIlN32H9kU0a9aKPXt26pxQQWg7c+YU1tbW332i9KOSYXhC/MccP36UgQP7EBh4nbFjJ1CwYKEU3xQshBBC/ypU+JU7d4IYN240kZHPcXBwZOTIseTJk1ffoX0ROXPmomLF3/j7723UrVv/4yv8xy1btog+fQbqO4yfliRL3zmTbX+R3s8XRUwMBo8f6Tsc8QN78uQJw4YNYPPmjZQqVQZ//0NaN+0KIYT4MbRu3Y7WrdvpO4yv5vffu3+8kgBg9uyF+g7hpybJ0ncuvZ8vRrc0H6KnMkv9k6eFUKmUXLhwnhkz5tKoUVN5FoMQQgghxEfI1dJ3TvH/B6ypDAxIsrUjMW8+Xg6QOcRFyhw69A+enjWIjo4iS5asHD9+jiZNmkuiJIQQQgiRAtKz9INQZskqM+CJFAsNfcCwYYPYvn0LZcuWJzIyEgsLyxRN2yqEEEIIIZJJsiTET2bt2lUMGPAHZmbmzJmzkAYNGslzF4QQQgghPoGMxRHiJxHz/yGbOXLkonXrdpw4cQ4vr8aSKAkhhBBCfCLpWRLiBxcSEszQoQMJD3/C33/7U7ZsOcqWTflTv4UQQgghhG567VmKi4tj0KBBlC5dGldXVxYvXvzeugcPHqRu3bo4OTlRu3Zt9u/f/w0jFeL78+rVKyZOHEeFCr8QEHCBzp27Si+SEEL8wFxdS3P+/Fm9th0a+oATJ44BEBb2EFfX0oSFPfyk7b39qljxF2rVqsLo0UN58eJFqtvTp0WL5uPj0/GrtR8XF4e3d3OioiI1yn19R+DqWprQ0Ada63h51Wbnzu1a5Tt3bsfLq7ZGWXR0NDNnTqVhwzq4u7vQvLkX69evRqlUfrF9iIqKZPDgvlSpUpGGDeuwZ8/O99b18qqtdX64upZmyZLkKdBjY2Px8xtDrVqVqVevBitXLtXYjrd3c+Li4r5Y7B+j156lCRMmcOXKFZYtW8bDhw/p378/dnZ2VKtWTaNeYGAgPj4+9OvXj0qVKnH06FF69OjBxo0bKVCggJ6iF0J/VCoVdepU49q1K3Tp0o2ePftgJlPKCyGEeI+tW3djYWH50Xrjx4+mRImSlCvnQubMWdi6dTcZMmT8pG36+k6gSJFiACQmJhIYeJ0JE8Ywc+YUBg0a/klt6kPTpi1p2LDJV2t/5cqluLhUxNIyg7osLi6Ow4f/IVs2e3bv/pt27Tp9UttRUZF06tQWa2sbBgwYiq2tHdevX2Xq1ImEhj6gV69+X2QffH1HEhcXx/z5i7l27Qp+fmNwcMhOoUJFtOouXLgcpTJJ/f7gwf0sXDiX6tVrAeDnN4YbNwIZO3YSKpWK0aOHYWRkRJMmLbC0zICLS0VWrlz6yccktfSWLMXGxrJhwwYWLlxI4cKFKVy4MLdu3WLVqlVaydKOHTsoW7YsrVq1AsDR0ZEDBw6wa9cuSZbEf8qdO0FkyJCBTJms6Nt3ALly5flpntguhBDi67Gysk5RPZVKpf63oaFhitfTxdzcQmP9LFmyEhx8hzVrVv5QyVK6dOm+WtvJ18NrWblyvUb5yZPHMDY2pl49LzZtWo+3d8dPGj0yb94sjI2NmTx5JmnSpAHAzi4badKYMnDgHzRo0Jjs2R0/ax9CQx9w/PgRNmzYhq2tHbly5eHKlcv89ddGnclSxoz/Jt8xMTEsWfInPj49yZrVlsjISPbv38uMGfMoVqwEAF26dGPGjCk0adICAE/PBjRv3pBmzVqRNm3az4o9JfQ2DC8wMJDExEScnJzUZaVKlSIgIECrW7BevXr06dNHq40frRtXiE8VGxvLmDEjqVjRmVmzpgPg4VFdEiUhhPgPOXbsCN7ezXFzc6FFi4YcOnRAvUypVDJ37kxq1nSnRg13li79k8aNPdVD794ehnfu3BnatGmGm1t5Gjasy5Ytm4DkYV8XL55nyZKF+Ph01BqG9/z5M4YNG4iHRyXq1KnK/PmzNZKrlDA2NsHI6N/HWDx+/Ij+/Xvh7u6Cl1dtFi9eQFLSv70Op0+fpFWrxri5ufDHH92ZOnUCvr4j1PH6+o6gdeum1KpVhfv37/HixQtGjx6Kh0cl6tatxtSpE4iLe61ub/782dStWxU3Nxd8fDpy504QkNzz5ec3hpo13alSpQL9+/ciPPwJoD0M78qVS3Tp0o7KlV1p2LAOW7ZsVC/z9R3BzJlTGDZsIO7uLtSvX5Pdu/9+7/HYu3cX2bM7Ym1to1Hu77+HYsWccHGpSFjYQy5ePJ+q4wwQHx/Pvn17adCgkTpResPFpQLTp88la1ZbrfXOnz+rc5jc+4ZyXr16hcyZs2Bra6cuK1asBFeuXPpojGvWrMDKypoaNeoA8PBh8pDDt5Os3Lnz8vRphPo8tLKyxsEhO3v37krBUfh8ekuWwsPDyZgxIyYmJuoya2tr4uLiiIyM1KibO3dujR6kW7duceLECcqVk5vYxc9NpVKxY8c2XF3LMGPGNHx8etCnzwB9hyWEEOIbO3fuDIMH96VatZosXbqaWrXqMmzYQAIDrwOwYsUSdu/+m+HDfZk2bTbHjx/l4cNQrXaSkpIYOnQAv/3mzqpVG+nQoTNTpvhx9+4devToQ5EixWjSpAVjx07UWnfgwD48fRrBrFnzGTVqHDt3bmPTpvVa9d7n1q0bbN68nl9/dQeS/8YNHtyPjBkzsWTJKgYNGo6//25WrFgCJPdYDBjQGze3KixduoqCBQuxefMGjTb37NlJhw5dmDhxGg4O2Rk/fhQxMTHMnbuIceMmcf36NaZMmQAkP6h927bNjBrlx4oV67CysmLcuJEAbNq0jgsXzjNlymz+/HMFsbGxzJgxRWsfgoPv0r17F0qUKMnixSvx9u7IrFnTOHToH3WdTZvWkz9/AZYvX0elSm5MnDhWPWPtu06dOkGZMs4aZbGxsZw4cRQXlwo4OGQnR46c7Nq1I8XH+Y3Q0Ae8ehVLgQKFtZYpFApKliytcR3+RtGixdm6dbfOV9GixbXqP30aoZXsZcyYSZ1svs/r16/ZtGkdrVq1xcAgOSXJlMkKQGPdJ08eA2jc01WmjDOnTp34YPtfit6G4b169UrrA3rzPj4+/r3rPXv2jG7dulGyZEnc3d1TvV1j4+/roZxvelQVCjAxMdQqU6C9XPx33LgRSLt2LfHwqMrff+/G0TGHvkMSP5C3f70VIiV+jnMm+aJLofj37ymAyda/SOfni+I9F61fg8rMjNgBQ4iv45nidd6N+41Nm9bz22/uNG7cDEi+JeH69ausXbuCkSPH8tdfG+nYsQvOzmUBGDJkBM2aeWm0p1BAbGwM0dFRZMpkhZ2dHXZ2dlhb22BtbY25uRnGxkakS5cWS0tLYmNfqtcLCrrFlSuX2LBhK3Z22QDo23cgr1690hkvQJ8+PTA0TP48EhISSJ8+PZUrV6Vr1+4oFMkJ4KNHYSxcuBQDAwMcHXPg49MTX9+RtG3bnh07tlKwYGHatm0PQIcOnTlz5pQ6JoACBQpRoUJFAB48eMCRI4fYteuA+j7eAQOG0KZNM7p3782jRw8xMjIma9asZM2alV69+nHvXggKBTx6FIapaRrs7GyxsLBkyJARREVFqY/fm9f27X+RL19+Onfu+v/PIQchIXdZvXo5v/76GwB58uSjRYvW/4+5Exs2rCE4OEhnonHzZiBubpU1juHRowdJTEzE1bUiCgVUqvQbGzas448/+mNqaqp1vuiiUMDLl8kjsMzNzd5bTxcTE2OsrVM+/DIu7jUmJiYa20iTxoSEhASt7SYPJUzujTxwYC9p06bj11/d1PVsbW0pXLgo06dPYvjw0SQkJLB48QIAEhP/bS9nzlzs3bvro/v1ZrmxscEnX0frLVlKkyaNVlL05v27J8IbERERtG3bFpVKxYwZM9RZaGokJCSRyh7jr+pNLCoVxMcnaZWp0F4ufm4xMTGsXLmUDh26kDNnXg4cOEbhwkUwMTGUc0CkmpwzIrV+9HMmISF5KL9Khcbf+7Szp2N06+Y3jyft7OnE1fZMcf13434jJOQudes20FhWpEhx/v57G8+fRxIREU6BAoXVy7Nnz4G5uYVGeyoVmJtb4unphZ/fGJYu/RMXlwrUrFlXo+7brzfrhYSEYGFhia1tNnW5q+uvKBS644XkRKVQoSJERj5n9uxpGBkZ07Hj75iYmKJSJffSREdH4eFRSb2OUqlUjzIKCrpFgQKF3tnnokRHR6vLbG1t1f8ODr6LUqnE07O6RhxKpZL79+9TuXJVNm1aT8OGdShcuCgVKvxKrVp1Uamgdu16+PvvoXbtqjg5laJixd+oUaOW1vEIDg6mUKHC78RUjC1bNqnL7O0d1P9Oly45aUtISNR5nCIjn2NpmUFjmb//HooWLa4ur1jRjWXLFnPw4AGqVq0BgJGREUqlUqtNpVKJkZERKhXqCT3ePl4pERBwgT59uutcNmnSDIoXd9IoMzExIT4+XmMbcXHxpEljqmO7KnXZP//sx82tCoaGRhr1hg4dxZAh/alRozLp05vRuXNXrly5RLp06dX1LCwsef782Uf3683y5O8Fze+2lCaQekuWsmTJwvPnz0lMTMTIKDmM8PBwTE1NsbCw0Kr/+PFj9QQPy5cvJ1OmTN80XiG+NpVKxdatmxk+fDDPnz+jXDkXihd3onBh7ZsjhRBCpE6sT0/Sjx/z7XuWuvb4Im3pGi6lVCahVCZhaJj8i/m79w+9736iPn0GUL9+Q44cOciRI4fYunUz48dPoVw5l/du/821WmpYW9tgb++Avb0Dfn5TadWqCaNHD2P8+OThbUlJSWTPnoPx4ydrrZs+vdn/9+vD+2Ri8u+9OElJSZiZmfHnnyu02rOxsSFNGlNWr97E6dMnOX78CGvWrGD79r9YsmQ1uXLlZuPG7Rw/fpTjx48wf/4s/P13M3v2wne2p/05JCUpSUr69357Y2NjrTrv+ywUCoXGPVpRUZGcOXOKpKQkKlXSHJ63a9cOdbJkZmbOy5fa53JMzAvMzMwBsLOzx8zMjBs3rlOwoPZQvAEDetOgQWOtYYAFChRkyZLVOuO1sbHRKrO2zsyzZ081yp49e4qVlZXONiC5g+TChXO0aNFGa5m9vQNLl67m+fNnpE9vRmjoAwwMDMiSJau6jlKp/KROk0+ht2SpYMGCGBkZcfHiRUqXLg3AuXPnKFq0qNbOx8bG0r59ewwMDFi+fLnOD0qIH1lg4HUGDerL0aOHqVGjNqNGjf3s2WmEEEL8K762J/Gp6OH53mTP7sjVq5eBpuqyK1cukz27I+bm5lhb23DjxnX1xD+hoQ+IidGeCOvp0wiWLl1Et269aN26Ha1bt6N3724cO3aYcuVc3jvjmr29A9HRUTx+/Eh90bphw1rOnz/DuHHayc67LCws6dmzD0OG9Gf/fn/c3avg4ODI48ePyJAho3rY3JkzJ9m5cwdDhowkR45cXL4coNHOjRuB6mGAuo5RTEwMCoWCbNnsAQgKus2ff85j0KDhnDt3lsePH1Gvnhfly7vStm0H6tatRlDQbe7dC8bExAR3dw/c3Cpz5cplOnduy/Pnz7S28e5kC1evXvrkv9kZM1oRHR2lfn/w4AFUKhWzZy/UeCTIzp07WL9+NU+ePCZz5izkzp0841zjxprtXbt2lbx58wPJCa67uwebNq2nZs26Gknc0aOHOXr0MJ06+WjFlCaNKfb2Dineh8KFi/DoUZg6NoBLly5SuHDR965z585tEhMTtZI4pVLJH390w8enF7lz5wHgxImj5MtXgPTp/z0eUVGR6vubvja9TfCQNm1aPD09GTFiBJcuXWLfvn0sXrxY3XsUHh7O69fJs5fMnz+fe/fu4efnp14WHh4us+GJn8b582d5+DCUtWs3s3TpKkmUhBDiP+r69aucPHlc4/X69WsaNWrOwYP7Wb9+Dffv32PdulUcPvwP9eo1BKBBg8YsWjSfs2dPc+vWTcaNGwWglfxYWFhy+PABZsyYQmjoAy5ePM/t2zfVF9hp06blwYP7WklCrly5KVWqDOPHjyYo6Dbnz59l5cqlWr0SH/Lrr+6UKePM7NnTeP36Nb/8UpasWbMyatRQgoJuExBwgQkTxmJqaoqhoSF169bn6tXLrFy5lHv3Qli+fDEBARfem9DlyJETZ+fyjBw5hOvXr3LjRiC+viN49SoWc3NzlEols2cnT8YQFvaQnTu3Y2pqioNDdl6+jGH69MmcPXuahw9D8fffRebMWTSefQRQr15Dbt26yfz5s7l3L4Rdu3awefMG6tdvmOLj8LZ8+fIRFHRL/X7fvj04O5ejWLES5MqVR/1q0qQ5CoVC/bDXevW8OHLkIMuWLeLBg/sEBd1m8eIFHDt2WCMWb++OvHz5kt69fbhw4RyhoQ/YsWMLvr4jaNiwKTlz5vqkuN+WLZs9v/xSjtGjh3H79i127NiCv/8edRxJSUk8fRpBQkKCep07d4Kws8um1VNnYGCAqakp8+bN5P79exw+fJAlSxbSsmVbjXpBQbfJl+/bPD5Irw+lHThwICNGjKB169aYmZnRrVs3PDw8AHB1dWXcuHHUr1+fPXv28Pr1axo21DwR69Wrx/jx4/URuhCfRaVSsXHjOq5evcKIEWNo0qS5zqk9hRBC/LfMnTtTq2zt2r8oXLgIQ4eOYvHiBcydO4Ps2R0ZNWocpUqVAaBp0xY8fRrBkCH9MDAwpEWLNgQEXNAaEmZsbMz48VOYPn0yrVs3IV269NSsWYfa/+91q1XLk3HjRhESchdfX80Z8YYOHc3kyePp1KkN6dObUadOvVQnCT179qV16yYsX76Yjh1/Z/z4KUybNpGOHVuTNm06fvutMj4+yUMXs2a1ZfRoP2bNmsaiRfMpU8aZChUqfXBI4NCho5g6dQI9evyOoaEhzs7l6NWrLwCurhVp164zM2dO4dmzp2TPnoNx4yZjYWFB/fqNePLkCaNHD+PFi2jy5y/I+PGT1UMc38iaNSsTJkxlzpzprF27kixZsuLj04uaNeuk6ji84excnp07twMQERFOQMAFRo/206pnbW1DhQqV2LVrBy1btqVAgUJMnDiNJUv+ZOXKZSgUCvLly8/kyTPJmzefej0rK2vmzl3E4sULGDVqKFFRUWTLlo327Tvh6en1STHrMnToSMaPH03Hjm2wsrJm4MCh6um/nzx5TMOGdZgxYx6lSiWPJnv27Cnm5tq33QD06TOQCRN88fZuQcaMGenZsy+VKv2mUefSpYvUqVPvi8X/IQpVaifI/8FFRLzQ+wQPE7KMwExlQYwimlUT1hL28iG26e0IaB0IQPHi6QkLM8DWVsl9HDAMe0iSrR3PAgL1G7j4Iq5cuczAgX04deoEnp71mTPnzxSNBZcJHkRqyTkjUutnOGcSEuJ5+jQMKytbjI217y/5WZ08eZz8+QuqH/j5/Plzateuon5Q6NfyoQkePteboVpv9yD07duDAgUK0a5dp6+z0W/s5csYGjSoxdKla3Q+8+hn8yXOl7Cwh3h7t2DTph0ffWDwh74PFAqwtjb/6Pb0NgxPiP8alUrF0KEDqVy5As+fP2Pjxm0sWLD0k26aFUIIId62detmxo0bxd27dwgOvsvkyeMpWLDQV02UvrbQ0Af07NmVM2dO8uhRGNu3b+HcuTNUquSm79C+mPTpzahXryFbt27Wdyg/jG3b/qJePa+PJkpfiiRLQnxlSqWSpKQkFAoFpqamDBs2mn/+OU7Fir/qOzQhhBA/id69+2FoaECXLt506tQGpVLJ2LGT9B3WZ6lQ4VcaN27GuHGjadasARs3rmPkyLHqSSx+Fq1bt+P48aMaD10VukVFRXL8+FFatfL+ZtuUYXh6IMPw/jsCAi4wYEAf6tSpR5cu2jPOpMbPMDxGfFtyzojU+hnOmf/qMDx9+ZrD8MTP51ufLzIMT4jv1PPnz+jbtxceHr8SGxuLk1NJfYckhBBCCCFSSW6W0LNXAbVgd08eJ2Sg+JT0ADx+nMJHCovvUnDwXapV+42EhETGjBlP27Yd5L4kIYQQQogfkFzB6dmLvX0gIi9KICxKc5mZmQq+3YPGxWe6c+c2uXLlwdExB7//3p0mTVqQOXNmfYclhBBCCCE+kQzD0zNV3P+fRqxIwtZWqX7lzZvEgAHx+g1OpEhERAQ9e3alXLlSnDp1EoVCQffuvSVREkIIIYT4wUnP0nfCwOIJAQFm2guGfPtYRMokJiaybNlixo8fA8C4cZMoXbqMnqMSQgghhBBfivQsCfGJFi6cx6BBfalTx5MTJ87j7d1B60nfQgghhBDixyXJkhCp8PjxY/bu3QVAy5Zt2L37AJMnz8Da2lrPkQkhhPjRubqWZsSIwVrlO3dux8urth4igvPnz+LqWlrnsrCwh7i4lGb+/NlayxYtmo+PT8cUbUOlUrF584bPijMlPrQv77N162YWLJijUfbwYSiurqUZPXqoVv0PfVZeXrXZuXO7Rtnx40fp1q0TVatWolatygwc2Ie7d++kKsaP8fffTaNGdXF3d2HgwD5ERkbqrLdz53ZcXUtrvSpU+HfUzN69u2jSpD5ubi507uzNtWtX1Mvmz5/Ntm1/fdHYvweSLAmRAgkJCcybN4ty5UoyYEAf4uPjMTMzw8mplL5DE0II8RPZt28P586d0XcYqbJ27UpCQoI/ef2LF88zZYrflwvoC4mKimTlyqU0bdpSo3z//r1ky2bP4cMHiY2N/eT2169fw7BhAyhfvgILFixj6tQ5mJqa0rVrB+7dC/nc8AG4du0K48ePpm3bDsyfv5QXL6IZO3aEzrru7lXYunW3+rVp0w7s7R1o2LAJkPzsyOS22rNixTqKFClGnz491MegWbNWrFix9Kd7uK4kS0J8xLFjR3B3d2X48MF4eTVi//4jmJjIgw6FEEJ8eba2dkyZ4kdCQoK+Q0kxa2ubz0p2VN/pU203b97AL7+Uxdxc88Gl+/btoUGDxhgZGXPw4P5Pajs09AFz586gb99BNG3aAkfHHOTNm4+hQ0eRLVs2lixZ+CV2gU2b1uPmVoXq1WuRJ09ehg4dxYkTx3j4MFSrbpo0plhZWatfe/fuQqVS0blzNwCePn1K69btqFq1Btmy2dO2bXuio6MIDk7uCTM3N8fZuSx//bXxi8T+vZBkSYiPmDFjCmZm5vj7H2LChKlkzJhJ3yEJIYT4SXXo0IXw8HBWr17+3jqPHz+if/9euLu74OVVm8WLF5CUlAToHgbm49ORRYvmA+DrOwJf3xG0bt2UWrWqcP/+Pe7evUPv3j5UqVIRN7fy/P57e4KD76Y4Zh+fXly8eF49TF2XO3du061bJ9zcXGjatL562F1Y2EO6d+8MJA9DPHToH2rVqqJOoC5duoira2nOnz+rbsvTszpnzpxS72/z5l64ubnQrl1LLl48r67n5VWbOXNmULduVdq2baYV08yZU6hfvyaPHj3SWqZUKtm6dTMVKvyqUX737h2Cgm5TsmRpypYtz65dO1J4lDTt27cHCwtLqlSpplFuYGDA4MEj6dChi871fHw66hwq974hj1evXqF4cSf1+yxZspIlS1auXr38wfiio6NYtWoZnTv7qH8gdnOrTOvW7QCIi3vNunWryZgxEzly5FKv5+JSka1bN6NUKj9+EH4QMhueEO+Ij49n4cJ5FCxYEDe3KixYsARzcwsMDOS3BSGEEF+XtbUN7dp1ZMGCOVSpUg07u2way1UqFYMH9yNPnrwsWbKKiIgIJk4ci4GBAW3atE/RNvbs2cnYsZOwsrIiWzZ7mjSpR5kyzvzxxwBiYmKYMsWPuXNn4Oc3NUXt5cuXn3r1GjJ79jTKl6+AmZnm7L5xca/p06cH1avXol+/wYSEBDNhgi/p0qWjSpVq+PpOYPDgfmzduhtTU1NevIjm7t0gcuXKw8WL51EoFFy6dJGSJUtz504QL1/GULy4Ezt3bmfq1An07t2fwoWL8Pff2+nbtwerV2/Cxib58R3+/ruZMmU2SqWSFy+i1TGtXbuSPXt2Mnv2n2TNmlVrn4KCbvP8+TNKltS8x2nfvj1kzWpLnjx5cXWtxIgRg3j0KIysWW1TdKzeuH37FvnzF9R5bZEjR873rjd27ESdvY7GxsY66z99GoG1tY1GWcaMmQgPf/LB+P76ayPW1jb89ltlrWVnz56md28fVCoVw4aNJl26dOplJUuW5tmzp9y5E0SePHk/uI0fhSRLQrzl0KF/GDSoL0FBtxk0aDhublWwtMyg77CEEEJ8pm3bjPDzMyEmRvHNtmlmpmLAgHhq105M1XpeXk3YuXMH06ZNYsIEzYTl3LkzPHoUxoIFSzEwMCB79hx07dqTsWNHpjhZKlCgEK6uFQF49eoVnp4NqFevIWnTpgWgevVaH+zZ0qVDh878888+FiyYTe/e/TWW+fvvJkOGjOreEgeH7Dx69JD169dQrVpNzM0tALCySp4sqVChIly4cO7/ydIFypYtz+XLl4DkC3Unp1KYmJiwceNavLyaUL16LQC6dOnGxYvn2bRpPZ07+wDg4VGd3LnzAKh7p/bv38uSJQuZPn0ujo45dO7PzZuB2Npm0xp2v3//XlxdKwFQrpwLxsYm7N79d4qP/RsxMS8+aaSKhYVlqurHxb3W2gcTExPi49//LE+VSsWOHVtp1qyVzuW5cuVm0aIVHDt2hLFjR2Jrm40iRYoCkCZNGmxts3HzZqAkS0L8TJ4+fUrfvj3ZsWMrZcuWZ8GCpRQuXETfYQkhhPhCZs824datb/94h9mzTVKdLBkaGtKnzwB+/709hw8f1FgWEnKX6OgoqlatpC5TKpXExcWl+MZ6W9t/e0HSpk2Lp6cXu3f/TWDgNe7dC+bGjRtkypS6C/n06c3o1q0Xo0YNpUaNOhrLgoODCQq6RZUqFdRlSUnK9z5uw9m5HBcunMPT04urVy8xduwkBg/ui1Kp5OzZ0zg7l1O327ZtB411ixQpSkjIv0MI397XN3x9R2JiYqzufdIlMvI5GTJk0Ci7fv0qDx7cp2LFXwFIly4dZcr8opEsGRkZvXcImlKpxMgo+dLbwsJSo6crpf74ozuXLl3QKi9WzInJk2doletKjOLj4zE1NX3vNgIDr/HkyWPc3T10Ls+UyYpMmazImzc/165dYevWTepkCcDS0pLnz5+ldJe+e5Isif80lUqFQqEgXbp0PH78iDlzFtKgQSMUim/3y6MQQoivz8cnnvHjv33PUteu7/8F/0OKFi1OzZp1mD59ksYv/ElJSWTPnoPx4ydrrZM+vZnOv19v7md6w8QkjfrfsbGxdOjQCkvLDLi6VqRy5arcuxfMmjUrUx1z5cpV2bFjK5Mnj8PZubzG9kuVKqPV4/Q+ZcqUZePGtdy8GYi1tc3/Z55VcPPmDS5ePE/37r3/vx/aky0lJSlJSvo3WXl7X98YNmwUq1YtZ/bs6QwbNlpnDAqFQuu47du3B4Bevbqqy5RKJSqVikuXLlKsWAnMzMx5+TJGZ5svX8ZgZpY8WUT+/AVZt26l+jrkbfv3+3Pq1HEGDRqu1caAAUOIi4vTKk+TRns/AaytM/Ps2VONsmfPnqp78XQ5deoEJUqUxMLCQqP8+vWrGBgYkj9/AXVZjhw5te5vUyqVKBQ/z60LkiyJ/6z9+/cyatQwli5dTc6cudixY68kSUII8ZOqXTsx1T08+talSzeOHDnI2rX/Ji4ODo48fvyIDBkyqu8NOnPmJDt37mDIkJEYGRlpTGetUqkIC3v43m1cuHCOiIhwli1bq+71OHPm5CfPUNe7d39at25CZGQkWbIk3wuUPbsjR48ewtbWTt2btGfPTq5fv0bPnn20/vYWLFgIpVLFtm1bKFbMCQMDA4oVK86aNSvImDEj9vYO6navXr2iMQnD1auXNSY00OXXX92xsclCly7e1KlTjxIlSmrVyZgxE9HRUer3SqWSAwf2UbVqDZo310xefXw6smvX3xQrVoLcufPw8uVL7t69Q86c/058EBx8l5cvX5I3bz4gebKEhQvn4O+/Bw+PahrtrV278r33QH2oN0yXwoWLcOnSRWrUSJ704/HjRzx58pjChYu+d51r165QtGhxrfIdO7YSFvaQKVNmqctu3AgkX74CGvWioiKxsrJKVZzfs58n7RMihUJCgmnVqilNm3phbW2j/oMgiZIQQojviaVlBrp06aaR7PzyS1myZs3KqFFDCQq6TUDABSZMGIupqSmGhoYUKFCI6OgoNm5cS2joA2bOnEJ09PuHe1laWvLq1SuOHDlIWNhDtm/fwqZN6z956vLs2R1p1qyVRsxVq1bn9evXTJw4lpCQYE6cOMq0aZPImDEjgPpeqcDA68TFxWFgYECpUmXYvXsHxYolX7QXK1aCAwf81UPwABo3bs6mTevYvftv7t0LYe7cmQQF3aJ2bc+Pxlm4cBGqVq3BlCl+JCZqJ9H58hUgLOyhOvEMCLhAePgTGjZsQq5cedSvvHnz4+FRg3/+8ScuLo4sWbJSoUIlRo0awvnzZwkLe8jp0ycZMWIw7u5V1MlO1qy2tG3bgfHjR7Nu3Sru37/HtWtXGDKkH6GhD9T3XH2uevW82LNnJzt2bOH27VuMGTOc8uVd1ROHxMTEaCSFAHfuBOmcZKJOnfqcO3eG9evXcP/+PRYtms+1a1dp1Kipuk5s7EsePQrTSqB+ZJIsif+Ubdv+okKFXwgIuMDChUvZuHEbuXLl1ndYQgghhE41a9alaNFi6veGhoaMHz8FlUpJx46tGTy4H2XLutCzZx8gefKErl17smzZYry9m6NSwW+/ub23/SJFitGmTXsmT/ajdeum7Ny5nd69+/P8+bOPzpj2Pq1atdWYxS9duvRMmjSD+/fv0bZtM/z8fGnQoBEtW7YFIFeuPJQp40yXLt6cPHkMAGfnsiQkJFCsWAkAihd3QqVSaQzvc3evQseOXfnzz3m0adOUCxfOMWXKrPdO2vCuzp19ePToERs3rtValjt3HqysrLlyJXliiX379pA7d14KFCikVbdevQbExMRw5MhBAIYNG0Px4iUZM2Y4zZo1YPz40ZQp48ygQSPeOU7e9Os3CH//PbRr15L+/XtjYGDAvHmLyJbNPkX78DFFihSjb99BLF68kC5dvDE3t9AY3jd9+iQGDeqrsc6zZ8/Uk268LX/+AowdO4m//95K69ZNOXHiGFOmzNTo7bp8+RI2Npk1etV+dArV9/oksK8kIuIF+t7jCVlGYqYyJ5oYpijGg8oQA8swHt0y06qbqXgBDMMekmRrx7OAQD1E++NTqVRERERgY2NDSEgwK1cuo0ePP7SmNv3emZgYEh+f9PGKQvyfnDMitX6GcyYhIZ6nT8OwsrLF2FgeIP61KRTo/brqa1m0aD6PHz/See+Q0G3s2JHY2WV77+yA3/p8+dD3gUIB1tbm71nzX9KzpAdK1VvDvVTJY3cVaXTfDCg+z507QTRv3hAPj0q8evUKR8ccDB48/IdLlIQQQgjxbTVo0JgzZ05pDVMTukVFRXLmzCnq1fPSdyhflCRLemZgGQbW1zGvOlHfofxUXr58ybhxo6hY0ZnAwOuMGeP3wWkyhRBCCCHeliFDBlq18v6kmQH/i9asWUnr1t4/3fMpZTY8PcsypDRhLx+SNr0doD0NqPg0rVs34+TJY/j49KB79z80ni4thBBCCJESP1svydf0pSal+N5IsiR+Grdu3cTQ0JBcuXIzePAwLC0zyOQNQgghhBDik8kwPPHDi4l5wahRw/j113JMnZo8nNHJqZQkSkIIIYQQ4rNIz5L4YalUKrZs2cTw4YOJjHxO79796Nq1h77DEkIIIYQQPwlJlsQPKzw8nF69ulGp0m+MHj2O7Nkd9R2SEEIIIYT4iUiyJH4oL15EM2vWNHx8epI5c2aOHj2Nvb2DvsMSQgghhBA/IblnSfwQVCoV69evoWzZksyfP4eLFy8ASKIkhBBCCCG+GkmWxHfv+vVr1K5dFR+fTpQv78qxY2epUKGSvsMSQgghvpi+fXswduxIjTJ//924upZm0aL5GuVLl/5JmzbNPtrmokXz8fHpmKLt+/qOwNd3xHuXP3/+jAMH9qWordS2v3PndlxdS+t8pTT+b+Vjx9THp6PW55UacXFxeHs3JyoqUqPc13cErq6lCQ19oLWOl1dtdu7crlW+c+d2vLxqa5RFR0czc+ZUGjasg7u7C82be7F+/WqUSuUnx/yuqKhIBg/uS5UqFWnYsA579uz8YP0tWzbSsGFdPDwq0bt3N537qFKp6NWrq8Z+RkVF4u3dnLi4uC8Wuy4yDE98954/f0ZUVCSbNm2XJEkIIcRPqVgxJ/bu1byoPH/+HNbWNly4cE6j/OrVyzg5lfpom02btqRhwyZfJL65c2eiUqlwc6v8Rdp7V+bMWVi4cJlWubGx8VfZ3vdq5cqluLhU1Hiwa1xcHIcP/0O2bPbs3v037dp1+qS2o6Ii6dSpLdbWNgwYMBRbWzuuX7/K1KkTCQ19QK9e/b7IPvj6jiQuLo758xdz7doV/PzG4OCQnUKFimjVPXXqBHPmzGT48OQ68+fPYtCgvixbtkZdR6lUMn36JM6cOUWVKtXU5ZaWGXBxqcjKlUs/+ZikhPQsie+OUqlk9eoVtGnTHJVKRfnyrhw8eEISJSGEED+t4sVLEBISTGxsrLrswoWzNG3agqtXLxMX91pdfvXqFUqUKPnRNtOlS4eFheUXiU+lUn2Rdt7HwMAAKytrrdeXiv9HEBsby4YNa6lbt75G+cmTxzA2NqZePS927/77kz+LefNmYWxszOTJMylVqgx2dtlwd/dgwIChbN68gXv3Qj57H0JDH3D8+BEGDBhCrlx5qFXLEw+P6vz110ad9U+cOMYvvzjj4lKB7Nkd8fbuRFDQLSIjIwEID39Cjx5dOHr0MGZm5lrre3o2YMOGtbx69eqzY38fSZbEdyUg4AI1a1amZ8+umJqaqv9oGBoa6jkyIYQQ4uspWLAwRkbG3LhxHYAnTx7z6FEYtWvXI316My5dCgDg3r0QXryIpkQJJwDu3LlNt26dcHNzoWnT+mzevEHd5rtDxk6fPkmrVo1xc3Phjz+6M3XqBI2hcS9fvmT48IG4u7tQv35N9u7drW5n164d7Nq1Qz2s68WLF4wePZQqVSpRt241pk6doJHQBQRcoG3bZri5uTB06ABev/532afYuXO7eohbzZruVKv2KzNnTlEnDo8ePaJXr65UqVKBWrWqMHXqBBITE4HkRG/p0j+pW7ca1ar9Sr9+vXj06JG6bVfX0hw4sI/mzb1wd3dh+PBBPHwYSvfunXF3d+H339sTHv5EXT8pKZHx40fj7u5Co0Z12b/f/71xb9myiYYN61ClSgV8fDoSFHT7vXX37t1F9uyOWFvbaJT7+++hWDEnXFwqEhb2kIsXz6f6+MXHx7Nv314aNGhEmjRpNJa5uFRg+vS5ZM1qq7Xe+fNn3ztE8vz5s1r1r169QubMWbC1tVOXFStWgitXLumMy9LSkosXLxASEkxiYiK7d/+Nra0d5ubJidGNG4FkzpyFRYtWYmZmprW+lZU1Dg7Z2bt3V6qOR2rIMDzx3Rg1ahizZ0+nQIFCbN26i3LlXPQdkhBCiJ/Etm1/4efnS0xMzDfbppmZGQMGDKF2bc+P1jU2NqZQocJcv34VJ6dSnD9/lgIFCpEuXTpKlHDi/PmzlCnjzNWrl8mVKzeWlhmIi3tNnz49qF69Fv36DSYkJJgJE3xJly4d1arV1Gg/NPQBAwb0plUrb9zcKrN3726WLVukUe/w4X/4/ffudOzYlS1bNjF+/CjKl3eladOWhIQEA6iHao0fP4rExETmzVvE69evmTZtElOmTGDgwGE8f/6cfv16UrdufUaOHIu//x6WLFlI9eq1Put4XrlyCSsrK+bOXcT169fw9R1B2bLlKVOmLNOmTSBt2nQsWbKa58+fMWRIPxwdc1K/fkM2bVrH3r27GD58DFZW1qxZs4LevbuyfPk6jIySL4UXLZrHoEEjiIt7Te/ePly8eI4ePfrSrVsvhgzpz6pVy+nZsw8Aly9fwtExJ4sXr+LYsSOMGjWE/PkLaE06dfToYZYsWUC/fkPInt2R3bv/pnv3TqxZ8xcWFhZa+3fq1AnKlHHWKIuNjeXEiaP07t0fB4fs5MiRk127dqRoGObbQkMf8OpVLAUKFNZaplAoKFmytM71ihYtztatu3Uu09Xr9/RphFaylzFjJo1k820NGjTm7NnTNG/uhaGhIaampsye/af6R3JX14q4ulb84L6VKePMqVMntHrkvhRJloReJSUlERcXR7p06ciVKzdjxoynbdsO6i8vIYQQ4kuYPXs6t27d1Mt2U5IsAZQoUZJr164Cyb/ov7kgdnIqhb//HiD5fqU3Q/D8/XeTIUNGOnToAoCDQ3YePXrI+vVrtJKlHTu2UrBgYdq0aQ9A+/adOXPmlEadIkWK0axZKwBat27HmjUrCAkJpnDhIureiIwZMxIa+oAjRw6xc+cBzM3NUKmgf/8htG3bjG7denPggD8ZMmSkS5fuKBQK2rXrxMmTxz64748fP6JKlQpa5X37DsLDozqQPEy/X7/BpE9vRvbsOVi3bhXXr1+jTJmyhIWFkT9/AbJmtcXe3oGJE6djbp6ckKxevYLevfurE4K+fQdRt241Tp48rr4Qb9SoGYULJ99TkzdvfrJnd1Tfn1Wpkhu3b/977lhb29Cnz0CMjIxwdMzBiRNH2b59C126dNOIffXq5bRs2RYXl+T96tChCydOHGPv3p14eWnfS3bzZiC//aZ5T9iRIwdJTEzExSU5zooVf2PjxnX07t0fU1PTDx7Tt8XEvADQ2TvzIcbGxlhZWae4flzca0xMTDTKTExMSEhI0Fk/IiKc+Pg4hg0bg729PcuWLWL06KEsWLBMqwfsfXLkyCU9S+LndO7cGQYM6IOTU0kmTJhKixat9R2SEEKIn5SPT0/Gjx/zzXuWunbtkeL6xYuXYNeuHQBcuHCOfv0GAcnJ0qxZ04iPj+fKlcu0bu0NQHBwMEFBtzSSjKQkpc6h60FBtyhQoJBGWZEiRYmOjla/z5Ytm0bsAPHx2jONBQffRalUUq9edY1ypVLJgwf3CQ6+S548eVEoFOplBQoU5vXr999XYm1tw8yZ2rPIZcqUSf3vjBkzkT79vxf76dKlVw+1a968FWPHjuTw4X9wdi6Pu7sH+fIVIDY2lidPHjN8+EAMDP69+yQuLo779++p39vZ/bvvadKk0RhGliZNGuLj49Xv8+bNp/Gjbr58BQgJuasVe0jIXebMmcn8+bPVZfHx8RrbfVtk5HMyZMigUbZv3x6KFi2uLq9UyY3lyxdz6NABqlatAYCRkZHO2eyUSqU6TkvL5F6gFy+itep9SEDABfr06a5z2aRJMyhe3EmjzMTERONYQfI+p0mjO7GbNGkclSq54eGRPHHD8OG+1K9fk6NHD+Hu7pGiGC0tLXn+/FmK6n4KSZbENxcREcGYMcNZvXoFRYsW1/nrihBCCPEl1a7tmeIeHn0pUqQ4ERHhBAZeIzz8CUWLlgAgZ87cpE9vRkDAee7eDaJEieQep6SkJEqVKkPv3v0/2nZyAqU5McC7EwUYGGgnWbomE0hKSsLMzIw//1yBQgFvV7Gxsfn/eprrGBsb8aHblgwNDT/67ERdM+O9ic/DozqlSpXhyJGDHD9+lKFD+9O8eWuaNm0JwOjRfmTP7qix7ttD4d5NMN9O9N71dtKVHIMSIyPt2JKSkujevTelS/+iUZ4+fXqd7SoUCpKSktTvo6IiOXPmFElJSVSqpDk8b9euHepkyczMnJcvtX8EiIl5oZ4Uwc7OHjMzM27cuE7BgtpD8QYM6E2DBo21hgEWKFCQJUtW64z3zWf9NmvrzDx79lSj7Nmzp1hZWels48aN67Rq5a1+ny5dOhwcHHj0KExnfV2USqXWZ/IlyQQP4pt6/vwZLi6l2LlzO35+U9i79yC//OL88RWFEEKIn1zatGnJly8/W7dupmDBwuphVgqFghIlnPj77+04ODiSMWNGALJnd+T+/XvY2tphb++Avb0DV69eZuPGdVpt58iRixs3AjXK3n3/IW8nD9mzOxITE4NCoVBvNy4ujtmzpxMfn0CuXLm5eTNQ48L/5s0bqToWqTV//myePXuGp6cXEyZMo337Lhw6dABzc3MyZszEs2cR6lizZMnKnDkzPnn2tzt37mi8v379Ko6OObTqOTg4Eh7+RL1de3sHli9fzNWrl3W2mzGjFdHRUer3Bw8eQKVSMXv2QpYsWaV+NWnSgvPnz/LkyWMAcufOw5Ur2m1eu3aVvHnzA8m9T+7uHmzatF5rSNzRo4c5evSw1r1GAGnSmGrE//ZLV29R4cJFePQoTB0bwKVLFylcuKjOfba2tiE4+N/jGR8fT1jYQ2xts+msr0tUVCSZMulOxr4ESZbEN3Hx4nmSkpLImDETo0aN48SJC7Rt215muRNCCCHeUrx4Sfbt26N1A7+TUymOHj2kMWV41arVef36NRMnjiUkJJgTJ44ybdokdTL1trp163P16mVWrlzKvXshLF++mICACx/sQXmbqakpYWEPCQ9/Qo4cOXF2Ls/IkUO4fv0qN24E4us7glevYjE3N8fd3YPXr18zffok7t0LZvXq5Vy+HPDB9pVKJU+fRmi93u2leJ9794KZOnUCt2/f4s6dIE6ePKZOFBo3bsaCBXM5evQw9+/fY/z40Vy+HED27DlS1Pa7Hj8OY+rUCQQH32Xp0j+5ceMGnp4NtOo1adKc9evXsHv334SGPmDOnBkcOOCPo2NOne3my5ePoKBb6vf79u3B2bkcxYqVIFeuPOpXkybNUSgU6oe91qvnxZEjB1m2bBEPHtwnKOg2ixcv4Nixw9Sv31Ddnrd3R16+fEnv3j5cuHCO0NAH7NixBV/fETRs2JScOXN90vF4W7Zs9vzySzlGjx7G7du32LFjC/7+e9RxJCUl8fRphDphq13bk+XLl3Ds2BHu3UueoCRt2vTq+7xSIijoNvnyFfjs2N9HhuGJr+rx48eMGjWUDRvWsnDhUurWrU/jxh9/6rgQQgjxX1S8eAnWrl2pM1l6/fo1Tk7/Jkvp0qVn0qQZzJgxmbZtm2FhYUmDBo1o2bKtVrtZs9oyerQfs2ZNY9Gi+ZQp40yFCpVSPKFS1ao1GTToD9q0acqOHfsYOnQUU6dOoHv33zE0NMTZuRy9evUFkoe3TZ48k0mTxtGmTTOKF3eiatUaH3w+0JMnj6lbt5pWuaGhIYcOndKxhqY+fQYyefJ4fHw6kpSURPnyLvTsmRxP06YtiY2NZeJEX16+fEmBAoWYMmWmzhnpUqJsWReioqLw9m6Bra0tfn6TsbHJrFXP3d2DZ8+e8eef83j27Bk5c+bCz28qDg7Zdbbr7FyenTu3A8kTHwQEXGD0aD+tetbWNlSoUIldu3bQsmVbChQoxMSJ01iy5E9WrlyGQqEgX778TJ48k7x586nXs7KyZu7cRSxevIBRo4YSFRVFtmzZaN++E56eXp90LHQZOnQk48ePpmPHNlhZWTNw4FD1A2mfPHmMl1cdZsyYR8mSpWnatCUqFUybNono6EiKFCnOtGmzUzy5AyT3XNWpU++Lxf8uheprP2XsOxMR8UJrHO23Nj7zKCwwI5oY1kxcTdjLh9imtyOgtXZ3eKbiBTAMe0iSrR3PAlLeXa5vCQkJLF68gAkTxmFsbMSQISNp1qzlVx1T+rMzMTEkPj7p4xWF+D85Z0Rq/QznTEJCPE+fhmFlZYuxscnHV/iPuHPnNomJiRq/wPft24MCBQrRrl2nT2733XuWxKd7+TKGBg1qsXTpGp3PPPoZfOnzJSzsId7eLdi0aQfp0qXTWv6h7wOFAqyttR90+y65chVfxfbtWxg+fDBeXo04ceI8LVq0lkRJCCGE0JPQ0Af07NmVM2dO8uhRGNu3b+HcuTNUquSm79DE/6VPb0a9eg3ZunWzvkP5YWzb9hf16nnpTJS+FBmGJ76YsLCH7Nu3l5Yt21C3bn0KFixMwYKFPr6iEEIIIb6qChV+5c6dIMaNG01k5HMcHBwZOXIsefLk1Xdo4i2tW7ejU6e2NGnSHEvLDPoO57sWFRXJ8eNHmT9/yVfdjiRLevY49pG+Q/hs8fHxLFgwl8mT/UiXLh116nhiaZlBEiUhhBDiO9K6dTtat26n7zDEB5iamrJs2Rp9h/FDsLTM8E2OlYyL0jOlKvkhYmbGqXui8vfi0KF/+O238owZM5xmzVpw/PhZ+SVECCGEEEL8FKRnSc9s09thZmzGAOch+g7lk+zatQMrK2sWLFhK4cJF9B2OEEIIIYQQX4wkS3qmawa871lcXBxz584kY8ZMtG7tzYgRvqRJkybFz2kQQgghhBDiRyHD8ESK7d+/l4oVnZkwYSzh4U+A5LG1kigJIYQQQoifkfQsiY+KiXnB7793ZPfuv6lQoRLLl68lf/6v96RkIYQQQgghvgeSLIn3SkhIwNjYmPTpzUib1pQ//1xG7dqe0pMkhBBCCCH+E2QYntCiUqnYvXsn5cqV4tixIygUCubPX0KdOvUkURJCCCG+Al/fEbi6ln7v6/z5s+9dd+fO7Xh51f7g8rfbqlChDB4elRgypB8hIcHqeu/b9qhRQwHw8elIo0Z1iYuL02g/LOwhrq6lCQt7+N4Y4uLi8PZuTlRUpM79Dg19oLWOl1dtdu7cnqL9jY6OZubMqTRsWAd3dxeaN/di/frVKJXK98aUWlFRkQwe3JcqVSrSsGEd9uzZ+cH6W7ZspGHDunh4VKJ3724a+3jzZqDWcW7XriUAISHB+Ph0RKVSfbHYxaeTnqXvkMm2v0jv54siJgaDx9/2OUx37gQxeHA/9u/357ff3LG1tf2m2xdCCCH+i3r06EPnzj4A7N/vz9q1K1m4cJl6uYWF5We1nzlzFnV7KhVER0cydepE+vfvxerVmzAwSP793Nd3AkWKFNNYN00aU/W/Hz4MZcWKJbRv3zlV21+5cikuLhU1Hi8SFxfH4cP/kC2bPbt3/027dp0+ad+ioiLp1Kkt1tY2DBgwFFtbO65fv8rUqRMJDX1Ar179Pqndd/n6jiQuLo758xdz7doV/PzG4OCQnUKFtGcDPnXqBHPmzGT48OQ68+fPYtCgvurnAgUH3yVv3nxMmjRDvY6RUfJluaNjDrJmtWXXrh3UqPH+JFh8G5IsfYfS+/lidOumRpnK7Os/h+nAAX9atWpKlixZWbp0NdWr15SeJCGEEOIbMDMzw+z/f+vNzMwwMDDAysr6i7X/bnvW1tZ06uRD585tCQq6Rd68+QEwN7f44HZtbe1YvXo5VavWwMEhe4q2HRsby4YNa1m5cr1G+cmTxzA2NqZePS82bVqPt3fHT7rumDdvFsbGxkyePJM0adIAYGeXjTRpTBk48A8aNGhM9uyOqW73baGhDzh+/AgbNmzD1taOXLnycOXKZf76a6POZOnEiWP88oszLi4VAPD27kTr1k2IjIwkQ4YMBAffxdEx53uPdb16DRk9eijVq9eSazE9k2F43yFFTAwAKgMDkmztSMybj5cDvs5zmFQqFXfuBAFQpowzffoM4MiR09SoIf9zCiGEEN+LS5cu0qVLO9zdXahc2ZU+fboTERGhUWf+/Nl4eFTC07M6Gzeu/WibRkaG//+vcYrjqFq1Brly5WHKFL8Ur7N37y6yZ3fE2tpGo9zffw/Fijnh4lKRsLCHXLx4PsVtvhEfH8++fXtp0KCROlF6w8WlAtOnzyVrVu1RMufPn03VkMerV6+QOXMWbG3t1GXFipXgypVLOuOytLTk4sULhIQEk5iYyO7df2Nra4e5uTmQ3LP0oWSzUKHCvHoVy5kzp1J0HMTXIz1L3zFllqw8C/h6z2G6ffsWAwf24cyZ05w9exlra2t69uzz1bYnhBBC6MvtbTc57XechJj4b7ZNYzMTnAeUJ3ftfJ/VTkxMDP369aRx4+YMHTqKiIhwxo4dxcqVS+jZsy8Ajx6FERR0i3nzFnPjxnUmTPAlV648lCxZWmeb4eFPWLhwHo6OOVLV66JQKOjTZwAdO7Zh//69VK7s8dF1Tp06QZkyzhplsbGxnDhxlN69++PgkJ0cOXKya9cOnJxKpTgWSO7xefUqlgIFCuuM9X37X7RocbZu3a1zma4hj0+fRmglexkzZlI/SuVdDRo05uzZ0zRv7oWhoSGmpqbMnv0nhobJCWpIyF1UKiWtWjUmJiaGsmXL07VrD9KnN1PHXqrUL5w6dZxffin7/gMgvjpJlv6DYmJimDJlAvPnz8bOLhsLFizG2vrLdfULIYQQ35uLs88QeevZN9/uhdlnPztZiot7TevW7WnSpDkKhQI7u2z8+qsb169fVdcxMUnD4MEjsLTMQK5cublw4Rxbt25SJwuPHz+iSpXkIWFJSUri4+PImzcfI0b4qi/gAfr06YGh4b8DjywtM7Bxo+YkCwUKFKJu3QbMnDmVcuVcPhr/zZuB/PZbZY2yI0cOkpiYiItLRQAqVvyNjRvX0bt3f0xNTbUbeY+YmBcA6iGMKWVsbJyqYY5xca8xMTHRKDMxMSEhIUFn/YiIcOLj4xg2bAz29vYsW7aI0aOHsmDBMgwNDQkNfYCtrR2DBg3nxYtoZsyYwujRwxg/foq6jRw5cnL69MlU7Zf48iRZ+g/q27cnf/+9jd69+9G1a49UfSkJIYQQPyInnzKcGv/te5acuuru2UgNKytrqlevxbp1q7h16ybBwXe5ffsmRYsWV9exs8umMXlCvnz52b59q/q9tbUNM2fOB5J7LSwsLNVDwt42YMAQjXtw3kz88K6OHX/n0KEDLFw4j0aNmn4w/sjI52TIkEGjbN++PRQtWlxdXqmSG8uXL+bQoQNUrVoDSJ7wQNdsdkqlUj0ZgqVlci/QixfRH4zhXQEBF+jTp7vOZZMmzaB4cSeNMhMTE+LjNc+d+Ph4jckvNNsYR6VKbnh4VANg+HBf6tevydGjh3B39+Dvv/eRJo2pej8GDx5J+/YtiYgIV/dgWVhYEhn5PFX7Jb48SZb+IwIDrxMT84LSpX+hf//BDBw49LNvdhRCCCF+FLlr5/vsHh59CQ9/Qvv2LcmfvyClSztTp049jh8/ytWrl9V13u4NAlAqVRgbG7+13BB7e4ePbsva2iZF9czNzenatQdjx4786NA5hUJBUlKS+n1UVCRnzpwiKSmJSpU0h+ft2rVDnSyZmZnz8mWMVnsxMS8wM0tO9Ozs7DEzM+PGjesULKg9FG/AgN40aNBYaxhggQIFWbJktc54bWxstMqsrTPz7NlTjbJnz55iZWWls40bN67TqpW3+n26dOlwcHDg0aMwAPVwuzdy5MgJJH/Wb5IllUol949/B2SCh59cdHQUQ4cO5LffyjN16kQg+X9ISZSEEEKIH8Phw/9gbm7JhAnTaNSoKcWLO/HwYahGndDQB7x+/Vr9/vr1qzg6ft2/9VWr1qB4cSdmzpzywXoZM1oRHR2lfn/w4AFUKhWzZy9kyZJV6leTJi04f/4sT548BiB37uQZ59517dpV9ex9RkZGuLt7sGnTeq0hcUePHubo0cNa9xpB8nTo9vYOOl+6eosKFy7Co0dh6tggedKNwoWL6txna2sbgoPvqN/Hx8cTFvYQW9ts3L17hypVKmp8hrdu3cDQ0JBs2f5NVKOiIsmUSW6T0DdJln5SKpWK9evXUK5cKVasWMLAgUNZvHilvsMSQgghRCpZWFjy+PEjzp49TWjoA1auXMqhQwc0hoXFx8czZsxw7twJYsuWTfzzzz4aNmz21WP744/+753k4I18+fIRFHRL/X7fvj04O5ejWLES5MqVR/16c0/Wm4e91qvnxZEjB1m2bBEPHtwnKOg2ixcv4Nixw9Sv31Ddnrd3R16+fEnv3j5cuHCO0NAH7NixBV/fETRs2JScOXN99n5my2bPL7+UY/ToYdy+fYsdO7bg779HHUdSUhJPn0aoE7batT1ZvnwJx44d4d69YCZM8CVt2vS4uFTA0TEH9vb2+Pn5cufObQICLuLn50vt2vWwsLBQb/P27Vvkz5//s2MXn0eG4f2kXr9+zfjxYyhf3pURI8aQLZu9vkMSQgghxCdwc6tCQMAFhgzpj0KhoGDBQvj49GTRovnqhClPnnzY2GSmU6c2WFpmYNCg4RQoUPCrx5YjR06aNm3JihVL3lvH2bk8O3cmTxIRERFOQMAFRo/Wnnrc2tqGChUqsWvXDlq2bEuBAoWYOHEaS5b8ycqVy1AoFOTLl5/Jk2eSN++/QyqtrKyZO3cRixcvYNSooURFRZEtWzbat++Ep6fXF9vXoUNHMn78aDp2bIOVlTUDBw5V39/15MljGjasw4wZ8yhZsjRNm7ZEpYJp0yYRHR1JkSLFmTZttnp68/HjpzB9+mR+/70DBgYGeHhU4/ffe6i3pVKpuHLlkkZSKPRDoVKpVPoO4luKiHiBvvd4fOZRWGBGNDEMeDJMa3mm4gUwDHtIkq1dqqYOj4x8zsSJ4+jY8XccHXP8/4bKjF8ydKFHJiaGxMcnfbyiEP8n54xIrZ/hnElIiOfp0zCsrGwxNjb5+ArisygUfPS66uXLGBo0qMXSpWt0PvNIaLtw4Rx+fr6sXr3xvZNs/IhScr58SR/6PlAowNpae5KTd/08R/8/TKlUsnr1CsqXL8Xq1Su5fv0agCRKQgghhNC79OnNqFevIVu3btZ3KD+MrVs307x5q58qUfpRySfwg7txI5CaNSvTs2dXKlVy4+TJ81SrVkPfYQkhhBBCqLVu3Y7jx48SFRWp71C+e8HBd3n8+BG1atXVdygCuWfph/VmOkkTExOUSiVbt+5K0YPhhBBCCCG+NVNTU5YtW6PvMH4IOXLkZO7cRfoOQ/yf9Cz9YJKSkli2bDEeHr8SGxtLzpy52L37H0mUhBBCCCGE+MIkWfqBnD17mmrV3OjbtycFCxYiPj4OQB5YJoQQQgghxFcgydJ3xGTbX2R0KY3B40dayyZMGEuNGpVRKpXs2OHPjBlzZQIHIYQQQgghviK5Z+k7kt7PF6NbN9XvE9Kn59mzp2TKZMUvv5Rl/PjJtG7tjaGhoR6jFEIIIYQQ4r9Bepa+I4qYGABUBgYcsnegTFwcv//eAYBff3XD27uDJEpCCCGEEEJ8I5IsfWfCgFZp0vDrg/sYW1vTv/9gfYckhBBCCCHEf5Jeh+HFxcUxcuRI9u7di6mpKd7e3nh7e+use+3aNYYPH87NmzfJkycPI0eOpEiRIt844q8rTqWiFBD/Oo4pU2bSrFlLeRiZEEII8R/h5VWbR4/C1O8VCgVmZuYUL16CXr36kSVL1q+2XW/vjtSoUfurtP+u48ePsmbNCm7eDMTY2JiiRUvQsePv5MyZC4BFi+Zz4cI5Zs1a8E3iEeJD9HolPmHCBK5cucKyZcsYPnw4s2bNYvfu3Vr1YmNj6dixI6VLl2bz5s04OTnRqVMnYmNj9RD1l3fixDFiYmJIo1CwBLieOTMtWrSWREkIIYT4j+ne/Q+2bt3N1q272bz5b0aNGsudO0H4+o7Qd2hfxPr1axg2bADly1dgwYJlTJ06B1NTU7p27cC9eyH6Dk8ILXq7Go+NjWXDhg0MHjyYwoULU6VKFdq3b8+qVau06u7cuZM0adLQr18/cufOzeDBg0mfPr3OxOpH8oIXdOrUlrp1q7N27UoAqgKZJEkSQggh/pPMzMywsrLGysoaG5vMlClTlvbtO3P+/Fli/n9v848qNPQBc+fOoG/fQTRt2gJHxxzkzZuPoUNHkS1bNpYsWajvEIXQorer8sDAQBITE3FyclKXlSpVioCAAJRKpUbdgIAASpUqpX6ekEKhoGTJkly8ePFbhvzFJJHEUY6ylMUcOXKYGTPm4u3dUd9hCSGEEOI7ZGxsDKAecXL37h169/ahSpWKuLmV5/ff2xMcfBeA8+fP4uVVm7/+2oinZ3UqV3Zl9OihxMfHq9vbsmUT9evXxMOjEkuX/qmxLaVSyerVy2nYsC5ubi5069aJoKDb6uWurqU5cGAfzZt74e7uwvDhg3j4MJTu3Tvj7u7C77+3Jzz8ic792LdvDxYWllSpUk2j3MDAgMGDR9KhQxd1WVJSIpMn++HhUYnatT3UPyoDvHwZw9ixI6lVqwq//lqWZs0acPjwQY0Y9+zZScuWjfjtt3L8/nt7Hj4MVS+/fv0qXbq0w93dhSZN6rNv3x71soCAC7Rr1xI3NxdatWrMwYP7P/zhiJ+e3pKl8PBwMmbMiImJibrM2tqauLg4IiMjtepmzpxZo8zKyopHj7SfR/QjCCOM/eynCEUJNDTk93GjsXYqpPP5SkIIIYT4Mh4/fsSlSxc1XiEhwQC8fv1aa9mlSxfV696+fUtr2fPnzwCIiIjQWnbnzm0dEaReaOgDVqxYirNzedKlS4dSqaR//17Y2tqxdOlq5s5dTFJSEnPnzlCvExERzsGD+5k8eSa+vhM5ePAAu3f/DcCpUyeYMWMyHTv+zrx5iwkMvKZxn9SSJQtZs2YlPXr0ZvHilWTNassff3Tj1atX6jqLFs1j0KARTJw4nYMHD9Clizeenl7Mm7eYp08jWLVquc59uX37FvnzF9R5m0GOHDmxs8umfn/58iWMjY1YsmQVLVq0ZtasaeqEcPr0ydy/H8LUqbNYsWI9xYs74ec3moSEhLdinE/Pnn1ZtGgFUVGRLFw4F4Dnz5/Rq1dX8ubNx5Ilq2jVqi2+viO4desmT59G0K9fT2rUqMXy5Wtp3rw1vr4jCQi48CkfnfhJ6G2Ch1evXmkkSoD6/du/fnyo7rv1UsLYWP9Tb2cmKz3oQSJg9XiadgVzc0xM9B+n+L4YGck5IVJHzhmRWj/HOZN8Ia5QJL/etnz5YiZOHK9R5uXViLlz/yQsLJTKlStqtRYeHg1A9+6dOXv2jMayOXMW0LBhE7Zt28yAAX00lv36qxsbNmxJdfSTJo1j6tQJACQlJWFkZEyFChXp0eMPFAqIj4/D07MB9es3JG3atADUqFGLVauWq/c5MTGRnj37kCtXbvLkyYOzc3kCA69Rt249duzYgodHNapXrwnAoEHD8PSs+f+tq9i0aT2dO3elQoVKAAwYMIRGjeqyd+9OPD0bANC4cTP1JFv58uUne3ZH3N0rq/f71q2bWsceICbmBRkzZtK57G0KBdjYZKZ7994oFAqaNGnO0qV/EhR0i5w5c1KiREmaNm1Orlx5AGjatAXbt2/h+fOn6kkwmjRpTunSZQCoV8+LTZvWo1DA/v17sbCwpFevvhgYGODomIMXL6KIj49j8+YNlC79C15ejQFwcHDg1q0brF+/mhIlnHQHK1IleZSY6htuL/m/xsYGn3xtrbdkKU2aNFrJzpv3pqamKar7br2USEhIQvXtPiOdEvO8JP62KebGD0myttNYpjIz42X/wcTHJ+kpOvE9k/NCpJacMyK1fvRzJiEheSi/SoXW3/tWrbypWrWGRpmlZQZUKrC1zca+fYe12nvTxowZ84iNfamxzMEhOyoV1KlTn9Klf9FYZmZm9knXG+3adaJSJTdiY1+yePECwsLC6NTJBwuL5DhNTdPi6enFrl1/Exh4jXv3grlx4waZMmXS2Gd7++zqf6dPn57ExERUKrh79y6envXVyywsMqh7dJ49e0Z0dBQFCxZRLzc0NCJ//kIEBwery2xts6n/nSZNGrJmtVO/NzFJvmbTte8WFpa8eBH90eOS/HnYAYq39sFM3W61ajU5cuQgW7f+RUhIMDduBAKQlKTUuf/p0v27/yEhIeTNmw+FwkC9vHHjFgCsWbOCY8eOULlyBXUsiYmJ6s9ZfAmqb3os32wr+XtB87vtY0n7G3pLlrJkycLz589JTEzEyCg5jPDwcExNTbGwsNCqGxERoVEWERGhNTTvRzHq+GRMTAyJj0/imb6DEUIIIf4jsmTJ+t7pt01NTSlWrMR7182TJ+97l1lbW2Ntbf254QGQMWMm7O0dABg92o/27VsxYMAfLFiwFCMjI2JjY+nQoRWWlhlwda1I5cpVuXcvmDVrVmq08+Y+pzdUb12hvnuxamycfB1mYpJGZ0xKZRJK5b8XmoaGmr/QK1J41Zk/f0HWrVuJSqXSWmf/fn9OnTrOoEHDAXQO1XuzD2PGDOfy5UtUq1YDT08vrKys6dy5rUbdN9eW7677bvnbkpKS8PCoTqtWmo+x+dA64uent3uWChYsiJGRkcYkDefOnaNo0aJa/4MUL16cCxcuqE90lUrF+fPnKV68+LcMWQghhBDimzE2NmbAgCHcvn2TdeuSZwu+cOEcERHhzJgxj2bNWlGmjDOPHz/SSIY+JFeu3AQGXlW/j419yYMHD4Dk3rBMmay4evWyenliYiI3bgSSPbvjZ++Pm1tloqOj8fffo1GelJTE2rUrNe6Lep+XL2Pw99/NqFFj/98L9xsvXkQBpOgY2Ns7EBR0W6PusGEDWb16OQ4Ojjx4cB97ewf168iRQ+zduyuVeyp+JnpLltKmTYunpycjRozg0qVL7Nu3j8WLF9OqVSsguZfp9evXAFSrVo3o6Gh8fX25ffs2vr6+vHr1iurVq+srfCGEEEKIr65gwcLUrFmXpUsXERERjqWlJa9eveLIkYOEhT1k+/YtbNq0XmNygw9p0KARBw7sY9u25CFsfn6+xMW9Vi9v3LgZixbN5+jRwwQH38XPbwzx8XG4uXl89r5kzWpL27YdGD9+NOvWreL+/Xtcu3aFIUP6ERr6gM6dfT7aholJGkxN03Lw4AHCwh5y6tQJpkyZCJCiY+DhUZ2oqCjmzJnB/fv32LlzO0ePHqJMGWfq129IYOB1FiyYw/3799i7dzcLFswma1bbz9538ePS6wN9Bg4cSOHChWndujUjR46kW7dueHgk/8/o6urKzp07geRfOubPn8+5c+eoX78+AQEBLFiwgHTp0ukzfCGEEEKIr65Tp64YGRkxZ84MihQpRps27Zk82Y/WrZuyc+d2evfuz/Pnz947Zffbihd3YuDA4axYsZT27VuSMWMm8uTJp17epEkLatf2ZMIEX9q1a8GTJ0+YOXM+GTNm/CL70qqVN/36DcLffw/t2rWkf//eGBgYMG/eIrJls//o+sbGxgwbNoqDB/fTokVDZs6cSuvW3lhZWXPzZuBH1zc3N2fixGlcvHieVq0as2rVMoYPH0PevPnJmtUWP78pnDx5nFatGrNw4Vx8fHri4SE/zv+XKVQp7bf9SUREvPgubtJ7c8+SECkl54xILTlnRGr9DOdMQkI8T5+GYWVli7GxycdXEJ9FodC+B0qI9/nW58uHvg8UCrC2Nv9oG3rtWRJCCCGEEEKI75UkS0IIIYQQQgihgyRLQgghhBBCCKGDJEtCCCGEEEIIoYMkS0IIIYT46fzH5q8SQujwJb4HJFkSQgghxE/D0NAQgPj4OD1HIoTQtzffA4aGRp/cxqevKYQQQgjxnTEwMCRtWjNiYp4DyQ8xVSgUeo7q5yVTh4vU+Fbni0qlIj4+jpiY56RNa4aBwaf3D0myJIQQQoifioVFJgB1wiSE+G9Km9ZM/X3wqSRZEkIIIcRPRaFQYGlphbl5RpKSEvUdzk/N2NiAhASlvsMQP4hveb4YGhp9Vo/SG5IsCSGEEOKnZGBggIGBib7D+KmZmBgCSfoOQ/wgfsTzRSZ4EEIIIYQQQggdJFkSQgghhBBCCB0kWRJCCCGEEEIIHf5z9yx9T7OHfk+xiB+DnDMiteScEakl54xILTlnRGp8L+dLSuNQqOQR10IIIYQQQgihRYbhCSGEEEIIIYQOkiwJIYQQQgghhA6SLAkhhBBCCCGEDpIsCSGEEEIIIYQOkiwJIYQQQgghhA6SLAkhhBBCCCGEDpIsCSGEEEIIIYQOkiwJIYQQQgghhA6SLAkhhBBCCCGEDpIsfSVxcXEMGjSI0qVL4+rqyuLFi99b99q1azRs2JDixYvToEEDrly58g0jFd+L1JwzBw8epG7dujg5OVG7dm3279//DSMV34vUnDNvPHjwACcnJ06dOvUNIhTfm9ScMzdu3KBp06YUK1aM2rVrc/LkyW8YqfhepOac8ff3p3r16jg5OdG0aVOuXr36DSMV35P4+Hhq1ar1wb81P8r1ryRLX8mECRO4cuUKy5YtY/jw4cyaNYvdu3dr1YuNjaVjx46ULl2azZs34+TkRKdOnYiNjdVD1EKfUnrOBAYG4uPjQ4MGDdiyZQtNmjShR48eBAYG6iFqoU8pPWfeNmLECPl++Q9L6Tnz4sULvL29yZMnD9u3b6dKlSr4+Pjw9OlTPUQt9Cml58ytW7f4448/6NSpE1u3bqVgwYJ06tSJV69e6SFqoU9xcXH07t2bW7duvbfOD3X9qxJf3MuXL1VFixZVnTx5Ul02e/ZsVYsWLbTqbtiwQeXm5qZSKpUqlUqlUiqVqipVqqg2bdr0zeIV+peac2bixImqdu3aaZR5e3urpkyZ8tXjFN+P1Jwzb2zdulXVpEkTVb58+TTWE/8NqTlnli1bpqpcubIqMTFRXVa/fn3VwYMHv0ms4vuQmnNmyZIlqnr16qnfv3jxQpUvXz7VpUuXvkms4vtw69YtVZ06dVS1a9f+4N+aH+n6V3qWvoLAwEASExNxcnJSl5UqVYqAgACUSqVG3YCAAEqVKoVCoQBAoVBQsmRJLl68+C1DFnqWmnOmXr169OnTR6uNFy9efPU4xfcjNecMwPPnz5k4cSKjRo36lmGK70hqzpnTp0/j7u6OoaGhumzTpk1UqlTpm8Ur9C8150yGDBm4ffs2586dQ6lUsnnzZszMzMiePfu3Dlvo0enTp3F2dmbdunUfrPcjXf8a6TuAn1F4eDgZM2bExMREXWZtbU1cXByRkZFkypRJo26ePHk01reysvpg16X4+aTmnMmdO7fGurdu3eLEiRM0adLkm8Ur9C815wzA+PHjqVevHnnz5v3WoYrvRGrOmfv371OsWDGGDh3KgQMHyJYtG/3796dUqVL6CF3oSWrOmRo1anDgwAGaNWuGoaEhBgYGzJ8/H0tLS32ELvSkWbNmKar3I13/Ss/SV/Dq1SuNLxZA/T4+Pj5Fdd+tJ35uqTln3vbs2TO6detGyZIlcXd3/6oxiu9Las6Z48ePc+7cOX7//fdvFp/4/qTmnImNjWXBggXY2NiwcOFCypQpQ7t27QgLC/tm8Qr9S8058/z5c8LDwxk2bBjr16+nbt26DBw4UO5zEzr9SNe/kix9BWnSpNH6sN+8NzU1TVHdd+uJn1tqzpk3IiIiaN26NSqVihkzZmBgIP87/5ek9Jx5/fo1w4YNY/jw4fK98h+Xmu8ZQ0NDChYsSPfu3SlUqBB9+/YlR44cbN269ZvFK/QvNefMpEmTyJcvH82bN6dIkSKMHj2atGnTsmnTpm8Wr/hx/EjXv3J19RVkyZKF58+fk5iYqC4LDw/H1NQUCwsLrboREREaZREREWTOnPmbxCq+D6k5ZwAeP35M8+bNiY+PZ/ny5VpDrsTPL6XnzKVLl7h//z7du3fHyclJfe9Bhw4dGDZs2DePW+hPar5nbGxsyJUrl0ZZjhw5pGfpPyY158zVq1cpUKCA+r2BgQEFChTg4cOH3yxe8eP4ka5/JVn6CgoWLIiRkZHGTWrnzp2jaNGiWr/+Fy9enAsXLqBSqQBQqVScP3+e4sWLf8uQhZ6l5pyJjY2lffv2GBgYsHLlSrJkyfKNoxXfg5SeM8WKFWPv3r1s2bJF/QIYM2YMPXr0+MZRC31KzfdMiRIluHHjhkbZnTt3yJYt27cIVXwnUnPOZM6cmaCgII2yu3fvYm9v/y1CFT+YH+n6V5KlryBt2rR4enoyYsQILl26xL59+1i8eDGtWrUCkn+Vef36NQDVqlUjOjoaX19fbt++ja+vL69evaJ69er63AXxjaXmnJk/fz737t3Dz89PvSw8PFxmw/uPSek5Y2pqiqOjo8YLkn/Vs7Ky0ucuiG8sNd8zTZo04caNG8ycOZOQkBCmT5/O/fv3qVu3rj53QXxjqTlnGjVqxPr169myZQshISFMmjSJhw8fUq9ePX3ugviO/LDXv3qduPwnFhsbq+rXr5+qRIkSKldXV9WSJUvUy/Lly6cxj3xAQIDK09NTVbRoUZWXl5fq6tWreohY6FtKz5mqVauq8uXLp/Xq37+/niIX+pKa75m3yXOW/rtSc86cPXtWVa9ePVWRIkVUdevWVZ0+fVoPEQt9S805s379elW1atVUJUqUUDVt2lR15coVPUQsvhfv/q35Ua9/FSrV//u/hBBCCCGEEEKoyTA8IYQQQgghhNBBkiUhhBBCCCGE0EGSJSGEEEIIIYTQQZIlIYQQQgghhNBBkiUhhBBCCCGE0EGSJSGEEEIIIYTQQZIlIYQQQgghhNBBkiUhhBBCCCGE0EGSJSGEELi5uZE/f36tV9OmTVO0fv78+Tl16tQXjenBgwda8RQrVoymTZty6NChz25/8+bNuLm5qd+fOHGCoKAgncu+lM2bN2vtU9GiRalatSqrV69OcTsxMTFs2bLli8cnhBBCk5G+AxBCCPF9GDRoEDVq1NAoMzY21lM0/9qwYQO2trYAvH79mmXLltG1a1d27txJ9uzZP7ndGjVq8Ouvv6rft2nThuXLl5M7d26tZV9S1qxZ2bhxo/r9ixcv2LhxIyNHjiRPnjz88ssvH21j6dKlnDp1Ck9Pz68SoxBCiGTSsySEEAIAc3NzbGxsNF4ZMmTQd1hkypRJHY+DgwP9+/fHxMSEAwcOfFa7pqamZMqUKdXLPpehoaHGMc6VKxf9+vXD0dGRffv2pagNlUr1VWITQgihSZIlIYQQHxUTE8PAgQMpV64cRYoUoVq1au+9sD9x4gR169alaNGiuLu7s3btWvWy6Oho+vbtS8mSJXF1dWX06NG8fv06VbEYGSUPinjT6xUVFcXQoUMpX748pUqVom/fvkRFRanrT5kyBVdXV4oVK0bLli25desWoDnU7s1/W7VqxcyZMzWWNWrUiBkzZmjE0KRJE+bMmQPAzZs3admyJcWKFaNq1aqsWrUqVfvzhomJCYaGhkByMjRv3jzc3NwoUqQIrq6uzJo1Sx33rFmzOH36NPnz5wcgPj6eMWPG4OzsjLOzM3369CEyMvKT4hBCCPEvSZaEEEJ8lK+vL3fv3mXx4sXs2LGD0qVLM3jwYOLj4zXqJSUl0bNnT6pVq8auXbvo0aMHI0eO5Pbt2wAMHjyYFy9esGbNGubMmcPly5cZNWpUiuN4+fIlU6dOJSEhgQoVKgDg4+PD9evXmTdvHkuWLCEoKIgBAwYA4O/vz7p165g2bRo7duzA2tqagQMHarX7ZljczJkz8fb21lhWo0YN/P391e8fP37MxYsXqVmzJq9fv6ZDhw6UKlWKbdu20b9/f+bMmZOq+4ni4+NZtWoVt2/fxsPDA4AtW7awbNkyfH192b17N127dmXmzJlcvXqVGjVq4O3tjZOTE0ePHgWSE8IrV66wcOFCli9fTkxMDD169EhxDEIIIXSTe5aEEEIAMHz4cEaPHq1RduzYMdKlS0eZMmVo27Yt+fLlA8Db25sNGzbw9OlT9f1EkHz/TWRkJNbW1tjb22Nvb0/mzJmxsbHh3r177Nu3j9OnT2Nubg7A6NGj8fT0ZODAgeqyd9WqVQuFQoFKpeLVq1dkyZKFcePGkT17dgIDAzl9+jS7d+8mZ86cAEycOJEaNWpw584dQkNDMTY2xs7ODjs7O4YOHcqdO3e0tvFmyJ2lpSXp06fXWFa9enX8/PwIDg4mR44c7N27l0KFCuHo6MiGDRuwsrKiZ8+eAOTIkYPQ0FCWL1/+3vuJHj58iJOTk/r969evyZUrF1OnTlWX29raMm7cOMqVKwdA06ZNmT17Nrdu3aJw4cKkS5cOY2NjbGxsePXqFStXrmTTpk3qnqYJEybg7OzMjRs31GVCCCFST5IlIYQQAHTv3l3ds/FG2rRpAfD09GTfvn2sX7+eO3fucPXqVSC5J+ltGTJkoGnTpgwZMoQ5c+bw22+/0aBBAywtLTl//jxKpZKKFStqrKNUKgkJCaFIkSI641qwYAFZsmRBoVCQLl06rK2t1cvu3LmDhYWFOlECyJ07N5aWlty5c4eaNWuycuVK3N3dKVGiBJUrV8bLyytVxyVLliyULl2avXv30rFjR/bu3aueCOPOnTsEBgZqJD9JSUnq4XS6ZM6cmRUrVqBSqQgICGDs2LE0aNCA6tWrq+uULVuWgIAAJk+eTFBQENevXyc8PBylUqnV3v3790lISKBJkyYa5UqlkuDgYEmWhBDiM0iyJIQQAgArKyscHR11LuvXrx8XLlygbt26NG3aFBsbGxo3bqyz7ogRI2jevDn79u1j3759rFu3jjlz5pCUlIS5uTmbNm3SWidLlizvjcvOzg57e3udy0xMTHSWJyUlkZSUhI2NDbt27eLYsWP8888/LFq0iPXr16d62u0aNWqwceNGGjRowPnz5xk/fjwAiYmJlCtXjmHDhqW4LSMjI/VxzpEjB0ZGRvTu3Rt7e3t1srphwwbGjh1Lw4YN8fDwoH///rRq1eq9+wqwevVq0qVLp7HMysoqVfsphBBCk9yzJIQQ4oNiYmLYsWMHU6dOpXv37lSpUkU9gcK7s7KFh4czcuRIHB0d6dKlC5s2baJs2bIcOHCAnDlz8uLFCxQKBY6Ojjg6OvL69WsmTJigde9TSuXMmZPo6GiNoXW3b98mJiaGnDlzcvDgQTZs2MCvv/7KyJEj2bp1K8HBwdy8eTNV26latSo3btxgw4YNFC1alGzZsqm3f/fuXezt7dX7dPHiRVasWJHitmvWrMlvv/3GyJEjiYmJAWDNmjV07dqVQYMG4enpScaMGXn69Kn6eCsUCvX6Dg4OGBoaEhkZqY7BzMyMcePG8fTp01TtpxBCCE2SLAkhhPggExMT0qZNy969e3nw4AFHjhxRT8rwbpJjaWmJv78/Y8eO5d69e5w5c4bAwEAKFSpE7ty5qVChAn369OHSpUtcvXqVgQMHEhsbi4WFxSfFljt3bipWrEj//v25dOkSly5don///pQpU4Z8+fKhVCqZMGEC/v7+PHjwgM2bN5M2bVpy5Mih1Va6dOm4desWL1680FqWKVMmnJ2dmT9/vsZwuTp16vD69WuGDRtGUFAQhw4dwtfXN9U9OoMHDyY6Olo9413GjBk5ceIEd+/e5cqVK/Tq1YuEhAT18U6bNi1PnjzhwYMHmJmZ0bBhQ0aMGMGpU6e4ffs2/fr1IyQk5L09ckIIIVJGkiUhhBAfZGJiwsSJE9mzZw81a9Zk/PjxdOnSBRsbG65fv65Vd86cOQQGBlKnTh169uyJl5cXDRs2BJInHrC3t6dNmza0bduWnDlzMmXKlM+Kz8/PDwcHB9q0aUO7du3Imzcvs2fPBpKnBO/evTvjxo2jevXq7Ny5kzlz5mBpaanVTsuWLZkwYQIzZ87UuZ03s9+9nSyZmZmxcOFCgoOD8fT0ZMiQITRv3pxOnTqlah8cHBxo164dK1euJCgoiEGDBhETE0PdunXp1q0b+fPnp0qVKurjXaVKFZRKJTVr1uTp06cMGDCAcuXK0b17dxo1aoSRkRELFiz44L1TQgghPk6hkifbCSGEEEIIIYQW6VkSQgghhBBCCB0kWRJCCCGEEEIIHSRZEkIIIYQQQggdJFkSQgghhBBCCB0kWRJCCCGEEEIIHSRZEkIIIYQQQggdJFkSQgghhBBCCB0kWRJCCCGEEEIIHSRZEkIIIYQQQggdJFkSQgghhBBCCB0kWRJCCCGEEEIIHf4HuMKkI8/puq0AAAAASUVORK5CYII="
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m10/10\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 911us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjIAAAHFCAYAAADosxNlAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAABqo0lEQVR4nO3deVxN+f8H8NdtuaXF1kYhhBClRQ1CYuzZjW2yj13WsWVP+tqzL2OZGYy1GGQMIYbBEKEhUklZK0LrbTm/P/w64yp0dUt1X895nO/X/ZzP+ZzPOd3q3ef9+ZwrEQRBABEREVEJpPa1O0BERET0pRjIEBERUYnFQIaIiIhKLAYyREREVGIxkCEiIqISi4EMERERlVgMZIiIiKjEYiBDREREJRYDGaJSjs+8VD7e0y/He0fKxkCGlOb27dv48ccf4eLiAmtra7Rp0wZz5sxBTExMoZ3z559/RrNmzWBtbY0NGzYopc0rV67A0tISV65cUUp7+TmXpaUlLly4kGediIgIsU5sbGy+25bJZFi8eDGOHj362bqWlpZYu3Ztvtv+mIyMDPTo0QN///03AGDGjBli33M2KysrODs748cff8TTp08LfM6iduDAASxZsuSrnX/16tWYP3/+Fx3r7u4Od3f3T9aZMWMGXF1dFWo3P8e8efMG06ZNw7Vr13Lte/bsGZYvX47OnTvD1tYWtra26N69O7Zs2YLU1NRc1/Dhe8rBwQEDBw7EP//8k6tflpaWaNGixUcDqOXLl8PS0vKz94WKL42v3QEqHXbv3o3FixfDyckJU6ZMgbGxMaKjo7Ft2zacPHkSv/zyC+rWravUcyYlJWHJkiVwcXHB0KFDUaVKFaW0a2VlhX379qFWrVpKaS8/1NTUcOLECTg7O+fad/z48S9q88WLF/jll1/g4+Pz2br79u1DpUqVvug879u0aRMqVaqEpk2bimVGRkZYt26d+DozMxNRUVFYvnw5bty4gWPHjkFbW7vA5y4qGzduhKOj41c7/4gRI9CuXTu0a9cOTZo0UXr7Y8aMwcCBA5Xe7t27d/H777+jZ8+ecuVXrlyBh4cHypUrh/79+8PS0hLZ2dm4cuUKNm7ciJMnT2L37t3Q0tISj6lfvz7mzZsHAMjKysKrV6+wZ88eDBs2DP7+/qhdu7ZYV01NDc+fP8f169dhb2+fq19f+v1FxQcDGSqw4OBgeHt7Y8CAAfD09BTLnZyc0KZNG3Tr1g2zZs2Cv7+/Us/7+vVrZGdno02bNmjcuLHS2tXT00OjRo2U1l5+2NnZ4dSpU5g/fz40NOS/LY8fP4569erh7t27hXZ+ZVzvixcvsGXLFuzZs0euXCqV5mrfwcEBmpqamD59Ok6fPo1OnToV+PyqokyZMhg0aBB8fHxw5MgRpbdfrVo1pbf5MS9fvsSkSZNQvXp17NixAzo6OuK+Zs2aoXXr1ujXrx9++eUXjBgxQtyX1/do06ZN0aRJE/j7+2P69OlieeXKlSEIAv74449cgUxISAieP3+OOnXqFM4FUpFgaokKbNu2bdDX18fkyZNz7atYsSJmzJiB1q1bIyUlBcC7v6B2794NNzc3WFtbw8XFBcuXL0d6erp43IwZMzB48GD4+fmhXbt2aNCgAbp27Yrz588DAPz9/cWh7FmzZsHS0hIA4OrqihkzZsj1wd/fXy4tk5aWhvnz56NFixZo0KAB2rdvj23bton180ot3b59G8OGDYOTkxPs7OwwatQohIeH5zrm0qVLGDp0KGxsbNCsWTMsW7YMWVlZn72HHTt2RGJiIi5fvixXHhYWhocPH6JDhw65jgkMDET//v1ha2srXsfu3bsBALGxsWjdujUAYObMmeK9mjFjBgYNGoR58+bBzs4OHTt2RFZWllxqady4cWjYsCEiIyPFc61duxb16tXLNXT/vh07dsDU1BQNGjT47PUCQMOGDQEAjx8/FsuuXbuG77//HjY2NnB0dMT06dPx8uVLcb+/vz/q16+PAwcOoFmzZnB0dMSDBw8AAIcPH0b37t1hY2MDFxcXrFixAjKZTDz2/v37GDlyJOzs7GBnZ4exY8fKpT3z8zV0dXXF48ePcejQIbn31NWrVzFs2DA0btwYDRo0gKurK9auXYvs7Gyx/RcvXmDSpElwdHRE48aNMXfuXKxatSpXSubAgQPo1KkTGjRoABcXF6xduzbXe6hz584IDw9HUFCQWObu7q5wSigvH6aJMjIysHz5crRo0QLW1tYYNmwYDh8+nGeq09/fH+3atUPDhg3RpUsXnDt3Try3OaM8AwcOFNM4v/32GxISErBo0SK5ICaHjY0NBg0alOe+D5UpUwZaWlqQSCS59rVv3x4nT57MlV46fvw4mjZtivLly3+2fSq+GMhQgQiCgAsXLqBJkyYoU6ZMnnU6duyIsWPHij+M5s6dCx8fH7Rp0wYbN27EgAEDsGvXLowZM0buB01oaCi2bdsGDw8PrF+/Hurq6hg/fjxev34NFxcXMV0xevRo7Nu3L999Xrx4Mc6fP4/p06dj27ZtaN26NZYuXQo/P78861++fBn9+vUTj120aBGePn2Kvn37IiIiQq7u1KlTYW9vj02bNqFz587YunUrDhw48Nk+1apVC7Vr18aJEyfkygMCAuDo6AgjIyO58qCgIIwdOxZWVlbYsGED1q5di6pVq2LhwoW4efMmjI2N5e7P+6mda9eu4enTp1i/fj2mTJkCdXV1ubbnz58PHR0dceg+NDQUmzZtwtChQz+ZUjl69CjatWv32WvNERUVBeC/EYCrV69i8ODB0NbWhq+vL2bNmoV//vkHAwcORFpamnhcVlYWtm/fDm9vb8ycORMWFhbYvXs3pk+fDisrK6xbtw4jRozAzp07sWjRIvFcffv2RUJCApYsWQJvb2/ExMSgX79+SEhIkOvXp76G69atg5GREVq2bIl9+/bB2NgYYWFhGDx4MMqXL49Vq1Zh48aNcHBwwLp16/DHH38AeDdfadCgQbh+/TpmzZoFHx8fhIWFYfv27XLn3rx5M+bMmYMmTZpg06ZNGDBgAH766SfMmTNHrp6JiQkaNWokN/9p3rx5cl9nZZk7dy5++eUXfP/991i/fj0MDQ1z9QcAnj59ii1btmDChAlYu3YtJBIJPDw8kJCQACsrK8ydO1dsL+e9dfr0aVhaWsqlgj40ffp0fP/993JlgiAgMzMTmZmZyMjIQFxcnBi4fpi6At79DMpJL+XIzs7GiRMnOBpYCjC1RAXy6tUrpKen53t+yoMHD3Dw4EFMmTJFHCpu1qwZjI2NMW3aNJw/fx4tW7YEALx9+xb+/v7iLzodHR18//33uHz5Mtq1a4d69eoBePeLUJHUyD///INmzZqJP8CcnJygo6MDAwODPOuvWLEC5ubm2LJli/hL39nZGd9++y3WrFmD1atXi3V79+6NsWPHAgCaNGmCwMBABAUFoW/fvp/tV4cOHfDrr7/KpZeOHz+OUaNG5ar74MEDdO/eXS6VZ2trCycnJ1y5cgU2NjZy96d+/fpivczMTCxcuPCjc2IMDQ0xb948TJo0CQcOHMAvv/yCOnXqYMKECR/te0REBOLi4mBtbZ3n/szMTPHfSUlJuH37Nnx8fFClShW4uLgAeHefa9Sogc2bN4v32cbGBp06dYKfnx8GDBggtjFq1CjxuOzsbKxfvx5t2rQRAxcASE1NRUBAADIyMrBu3TqUKVMGP//8M/T09AC8+/q0adMGW7dulUtFfOprWL9+fUilUlSsWFF8z4WFhaFp06ZYtmwZ1NTe/W3YrFkznDlzBleuXEGnTp1w5MgRREZGws/PTxyx+uabb9CmTRvxvG/fvsWGDRvQp08fzJ49G8C791n58uUxe/ZsDBkyRO4XfsOGDXHs2DHxdWHM6Xr06BEOHTqE6dOnY8iQIQCA5s2bIz4+Ptfk9Jyvg4WFBQBAS0sLgwcPRkhICFq3bi32r1atWuK/Hz16hGbNmuU67/vvlxzvp1yvXr0KKyurXHUmT54snv99DRs2RNWqVeXSS9euXUNiYiLatGnz0T9iqGRgIEMFkvMLJz/pEwBiauLDv4I6deqEmTNn4sqVK2IgU7FiRbl8fc4v3g9XMSjKyckJe/fuxbNnz9CyZUu0bNlS/MX1oZSUFNy+fRvjxo2TG7koW7YsWrVqJQ6d57C1tZV7XalSJTGl9jkdO3bEmjVrcPnyZTg7O+PmzZt4/vw52rZti9OnT8vVHT58OAAgOTkZUVFRePToEW7fvg0AcumUvJQvX/6zE3s7duyIEydOYO7cuZBKpfD394dUKv1o/ZwUTV4B7ePHj/P8pWNjY4OFCxdCW1sbqampuHnzJoYNGyb+tQ0AVatWhYWFBS5evCgXyOQEacC70ZaEhAR8++23cu0PGzYMw4YNA/BuVM3R0RHa2tpi23p6enBwcBBXWOVQ9GvYrVs3dOvWDenp6YiKikJ0dDTu3r2LrKwsZGRkiOevWrWqXNpNT08PrVq1ElOYN27cQFpaGlxdXeV+keekeS5evCgXyJiZmSEhIQGpqakfHQ0tqCtXrkAQBLRv316uvHPnzrkCmQoVKsgFETnvhbdv3360/fdTbzkyMzPzfL/cu3dP/LeVlRUWLFgA4N3ozJs3b3D+/HmsWrUKKSkpmDRpUq7jO3bsiMOHD8PT0xMSiQQBAQFwcXERA1squRjIUIGUK1cOurq6ePLkyUfrpKSkICMjA+XKlcPr168BIFeqRENDAxUqVJD7offhD+ec3HdeP/wU4enpiUqVKuHIkSPw8vKCl5cXbG1tMX/+/Fwrq96+fQtBEGBoaJirHUNDw1w/pD9cfaOmppbv52bUqFED9erVE1cvHT9+HM7OzihXrlyuui9fvsS8efMQGBgIiUQCc3NzODg4APj8czp0dXXz1Z/u3bvjzz//RPXq1VGjRo1P1s25D3n9QjUyMsLGjRvF11KpFJUqVZK7rjdv3iA7Oxs//fQTfvrpp1xtvL9iBYDcnInExEQA+OiIWk6d48eP57lCpWLFinKvFf0apqWlwcvLC7///jsyMzNRpUoV2NraQkNDQzzu1atXefbv/bKc63h/Uuv7Xrx4Ifc65x68ffu20AKZnPlJH/Y9r2v5cB5Lfr5fzczM5OZIAe9+Fhw8eFB8vX//fuzfv1+ujq6urjjHKoezszNSUlKwdetWDBw4MFcfO3bsiM2bN+P69eto1KgRTp48+cXL2Kl4YSBDBebs7IwrV64gPT091y8c4N0PoiVLluDgwYPiL6+4uDiYmZmJdTIyMvDq1StUqFChwP35cHTow7+mpVIpRo8ejdGjR+PJkyc4e/YsNmzYgClTpiAgIECurr6+PiQSCeLj43OdJy4uTumTBDt27Iht27Zh3rx5OHHiBKZOnZpnvalTpyIyMhI///wzbG1tIZVKkZqamusH/pdKTU2Fj48P6tSpg/v372P79u3iKFBecr5ub968ybVPKpXm+qXzIV1dXUgkEgwePDjPOQuf+kVdtmxZAJCbFAy8Cx7u3LkDW1tb6Ovro2nTpmJ65H0frhJTlLe3N/7880/4+vqiadOm4i/095dGm5iY4OHDh7mOfX9+Ts51LF++HNWrV89V98Ng+vXr15BIJIU6UdXExAQAEB8fD1NTU7H8w3v9pVxdXbFlyxbExMSgatWqYvn775f3JzR/ToMGDXDgwAHExsbmCmTq1q2LGjVq4MSJE0hLS0N6erqYnqSSjZN9qcCGDh2KxMRE+Pr65toXFxeH7du3o1atWrCyshIni34YMAQEBCArKyvP5zwoQk9PD8+ePZMrCw4OFv+dlpaGdu3aiZMsTU1NMWDAAHTq1CnPUSUdHR00aNAAf/zxh1yA9PbtWwQFBRW4vx/q0KEDEhMTsWnTJrx+/VpcefSh4OBgtG3bFk5OTmLKJ2dFV85fwB9O4lXEihUr8OzZM6xduxbff/891qxZk2ti8/tyfsl9eO/zS09PD/Xr10dkZCQaNmwobrVr18batWs/+XDCmjVrokKFCjh79qxc+e+//44RI0YgIyNDXN1Ur149se0GDRrg559/xqlTpxTqa848mBzBwcHiowZygpjQ0FC8fPlS/Fo4OjoiNjZWbgl9Wloa/vrrL/G1jY0NNDU18fz5c7l7oKGhgZUrV+ZaIfTs2TMYGhp+MuVXUPb29lBXV891j06ePKlwW3m9HwcMGIDy5ctjxowZSEpKyrU/KytLbvXc59y6dQvq6upyQdH7OnbsiJMnT+L48eP49ttv8/zDi0oejshQgTVq1AgTJkyAr68vIiIi0K1bN1SoUAHh4eHYtm0b0tPTxSCnVq1a6N69O9asWYPU1FQ0btwYd+/exbp16+Dk5ITmzZsXqC+tWrXC5s2bsXnzZtjY2ODMmTNyS5q1tbXFlS2ampqwtLREVFQUDh069NEVN1OmTMGwYcMwYsQI9O/fHxkZGdiyZQtkMtlH59Z8qapVq6Jhw4bYvHkzvv32248uO7W2tsbRo0dhZWWFSpUq4fr169iyZQskEok4h0hfXx8AcOnSJVhYWMDGxiZfffjnn3+wa9cu8fkeEydOxKlTpzBjxgzs3bs3z19INWvWhKmpKYKDg3PNVcmvyZMnY8SIEZgyZQq6dOkirk66efMmxowZ89HjclazLVy4EAYGBnB1dUVUVBTWrFmDAQMGoFy5chgzZgz69u2LkSNHol+/ftDS0sK+ffsQGBiINWvWKNTPsmXL4s6dO/jnn39gbW0Na2tr/PHHH9izZw8sLCwQFhaGjRs3yn0tOnfujC1btmDs2LGYMGECypYtix07diAhIUEMAitUqIDhw4dj9erVSEpKgpOTE54/f47Vq1dDIpHkSntev35d7vvlwYMHkMlkchO78/Ls2TP8/PPPucrr1Kkj9yBD4N37sWfPnli5ciUyMjJQt25dnDp1SgwaPwzqPiXn/RgUFIRy5cqhbt26MDExwbp16zBhwgR06dIFffr0gZWVFdTU1BAaGgo/Pz88fPgQXbp0kWsrKSkJISEh4muZTIYzZ87Az88Pffr0yZUuzNGxY0esX78ev//+u9KeBE5fHwMZUorRo0ejfv364hN+X79+jcqVK8PFxQWjRo1C5cqVxbre3t4wNzeHn58ffvrpJxgbG2PgwIEYM2aMQj8Y8zJy5Ei8fPkS27ZtQ0ZGBlxcXODt7Y3Ro0eLdRYuXAhfX19s374dcXFxMDAwQK9evT66KqdJkybYsWMH1qxZg8mTJ0MqlcLBwQFLliz55LLRL9WxY0fcvn37k8tC//e//4nzewCgevXqWLBgAY4cOSI+Al5PTw9DhgzBvn37cO7cOVy8ePGz505JScHMmTNRp04dcaKsrq4u5s6di9GjR2Pr1q0YOXJknse2a9cO58+fz/Ucn/xydnbGtm3bsG7dOnh4eEBTUxNWVlbYsWPHZ1elDRgwADo6Oti2bZv4lOIffvgBP/zwA4B3aYXdu3dj1apVmDZtGgRBQJ06dbB+/fqPjnp9zNChQ7F48WIMGzYMO3bswIwZM5CRkQFfX1/IZDJUqVIFo0ePxoMHD3DmzBlkZWVBQ0MD27Ztg7e3t7gqrUuXLihfvry4DB0AJk6cCCMjI/z222/YunUrypUrhyZNmmDy5MliIAC8my8TFhYm955dsGABHj9+jDNnznyy/48ePcrzac+9evXKFcgAwJw5c6Cjo4Pt27cjKSkJTZo0wejRo7F+/fp8Pd8lR+3atdG5c2fs3r0bf/31l7jiysHBAUePHsWePXtw4sQJ/PTTT5DJZKhcuTK++eYbrFq1KldwdufOHfTp00d8raWlhWrVqmHSpEni+zYvtWrVQp06dRAXF5fntVLJJBH4CV5EpATPnz9HmzZtsH37dqU+abk0CA8PR2RkJNq2bSv3wLZevXqhUqVKCj//Zf369Th16hQOHTqU5wPglCUxMRHnz59H8+bN5eavLVmyBP7+/kXyeWREn8MRGSJSChMTEwwePBg//fQTA5kPpKSkYMKECejfvz++/fZbZGVl4fjx4wgNDf3ohO6PSU5Oxp49e7B48eJCDWKAd5Osvb29Ua9ePfEJuyEhIdi1a9dHR+aIihpHZIhIaWQyGXr37o0ff/wxzw/AVGUnTpzAtm3bEBERAUEQUL9+fYwePVrh+7Rq1Sq8evUKCxcuLKSeyrt79y58fX0REhKC1NRUVKtWDX379sWAAQMKPZAiyg8GMkRERFRicfk1ERERlVgMZIiIiKjEYiBDREREJRYDGSIiIiqxGMgQERGpgKdxr792FwoFVy2VIjXbeiIpJf1rd6PU09PRQuRJb97vIvYoaPnX7oJK0dYA0jK/di9Ui3YRPNmtVrvZeJOcVqA2yupq48Gfi5TUo4LjA/FKkaSUdLwt4BuU8o/3m4hKmjcpMrxNkRWsEUnxSuYwkCEiIlIVEgAFfZBhMXsOIgMZIiIiVSFRK/iISjEbkSlevSEiIiJSAEdkiIiIVIVEooTUUvHKLTGQISIiUhVMLREREREVHxyRISIiUhVMLREREVHJpYTUUjFL5hSv3hAREREpgCMyREREqoKpJSIiIiqxuGqJiIiIqPhgIENERKQqclJLBd0UkJCQAA8PDzg4OODbb7+Fv7+/uC8mJgaDBw9Go0aN0LFjR1y4cEHhS2JqiYiISFUUcWpJEASMHTsW2dnZ+PXXX/H8+XNMnz4denp6+PbbbzF27FjUqVMHfn5+CAwMxLhx43D8+HGYmprm+xwMZIiIiFRFEU/2DQ0NxY0bNxAYGIiqVauifv36GD58OLZt2wZ9fX3ExMRg79690NHRgYWFBS5dugQ/Pz+MHz8+3+dgaomIiIgKRUxMDCpWrIiqVauKZZaWlggNDUVwcDDq168PHR0dcZ+9vT1CQkIUOgdHZIiIiFSFElNLSUlJcsVSqRRSqVSuzNDQEG/fvkVqairKlCkDAHj27BkyMzMRFxcHY2NjufoGBgZ49uyZQt3hiAwREZGqkEj+C2a+eHuXWmrRogXs7e3FbfPmzblOZ2NjA2NjY3h5eSElJQXR0dHYsWMHAEAmk+UKfKRSKWQymUKXxBEZIiIiUtj58+flXn8YlACAlpYWfH19MXHiRNjb28PAwADDhw+Hj48PJBJJrqBFJpNBW1tboX4wkCEiIlIVapJ3W0HbAKCnp5ev6tbW1jhz5gzi4uJQoUIFXLx4ERUqVEC1atVw8eJFubrx8fG50k2f7Y5CtYmIiKjkKnBaSbE5NomJiejXrx9evXoFIyMjaGhoICgoCI6OjrCxscG///6LtLQ0sX5wcDBsbGwUuiQGMkRERFQoypcvj5SUFCxbtgwxMTE4cOAA/Pz8MHz4cDg6OqJy5cqYOXMmwsPDsWXLFty6dQu9evVS6BwMZIiIiFTFV3iy76pVqxATEwM3Nzf88ssvWL16NaytraGuro4NGzYgLi4OPXr0wJEjR7B+/XqFHoYHcI4MERGR6vgKHxpZs2ZN7Ny5M8995ubm2LVrV4G6wxEZIiIiKrE4IkNERKQqivgjCooCAxkiIiJV8RVSS4WNgQwREZGqKIUjMsUrrCIiIiJSAEdkiIiIVAVTS0RERFRiMbVEREREVHxwRIaIiEhlKCG1VMzGQBjIEBERqQqmloiIiIiKD47IEBERqQqJRAmrlorXiAwDGSIiIlVRCpdfF6/eEBERESmAIzJERESqohRO9mUgQ0REpCpKYWqJgQwREZGqKIUjMsUrrCIiIiJSAEdkiIiIVAVTS0RERFRiMbVEREREVHxwRIaIiEhFSCQSSAo4olLQ45WNgQwREZGKKI2BDFNLREREVGJxRIaIiEhVSP5/K2gbxQgDGSIiIhXB1BIRERFRMcIRGSIiIhVRGkdkGMgQERGpiNIYyDC1REREpCJyApmCbop4+vQpRo4cCTs7O7i6uuLnn38W9925cwe9e/eGjY0NevbsidDQUIWviYEMERERFZqJEydCR0cH/v7+mDVrFnx9fXHq1CmkpKRgxIgRcHBwgL+/P2xtbTFy5EikpKQo1D4DGSIiIlUhUdKWT69fv0ZISAhGjx6N6tWro02bNmjevDkuXbqE48ePQ0tLC9OmTYOFhQU8PT2hq6uLEydOKHRJDGSIiIhURFGnlrS1tVGmTBn4+/sjIyMDkZGRuH79OurVq4ebN2/C3t5ebE8ikcDOzg4hISEKXRMDGSIiIlJYUlKS3CaTyXLV0dLSwty5c7Fv3z7Y2NigQ4cOaNGiBXr37o24uDgYGxvL1TcwMMCzZ88U6gdXLREREakIiaTgq45yDm/RogWSk5PF8nHjxmH8+PG56kdERKBVq1YYMmQIwsPD4eXlhSZNmiA1NRVSqVSurlQqzTMg+hQGMkRERCpCAiUsv/7/STLnz5+XK/8wKAGAS5cu4eDBgzh37hy0tbXRsGFDPH/+HBs3bkTVqlVzBS0ymQza2toK9YepJSIiIlKYnp6e3JZXIBMaGgpzc3O54KR+/fp48uQJTExMEB8fL1c/Pj4+V7rpcxjIEBERqYiinuxrbGyM6OhouZGXyMhIVKlSBTY2Nrhx4wYEQQAACIKA69evw8bGRqFrYiBDRESkKop4+bWrqys0NTUxe/ZsREVF4cyZM9i0aRPc3d3Rvn17vHnzBt7e3njw4AG8vb2RmpqKDh06KHRJDGSIiIioUOjr6+Pnn39GXFwcevXqBR8fH4wePRp9+vSBnp4eNm/ejODgYPTo0QM3b97Eli1boKOjo9A5ONmXiIhIVSjhs5ag4PG1atXCjh078txnbW2NQ4cOFag7DGSIiIhURGn80EgGMkRERCqiNAYynCNDREREJRZHZIiIiFSFgquOPtpGMcJAhoiISEUwtURERERUjHBEhoiISEWUxhEZBjJEREQqojQGMkwtERERUYnFERkiIiIVURpHZBjIEBERqYpSuPyaqSUiIiIqsTgiQ0REpCKYWiIiIqISi4EMERERlVilMZDhHBkiIiIqsTgiQ0REpCpK4aolBjJEREQqojSmlhjIkMoyrKCH5dP7wMXREgmJyVi+/QT2HLsiV6esrjYu7Z+NRRuP5toHAFUrV8StIwvzbL/TiFX4+0YE6llUxrJp38GmbjU8jUvEki3H4XcyuFCuiehjjp29CfdpP8mVdXFthF+WDM9Vd/eRS1izMxCPn79C3ZqVsWhSD3xjYwEASJdlYNHGY/A/GYzk1HQ429fGkqm9YWZSoUiug+hDXzWQsbS0ROfOnbFixQq5cn9/f6xbtw5nzpz54rYvXryItWvX4u7du9DQ0ICtrS0mTpyIBg0aFLTbVErsWvYD1NTU4DZqDUyNy2PjfHe8TU7DsbM3xTrzx3eDqXH5j7bx+PkrWLafKVfmPbEnalQ1xD+3oiDV1MCeFSMRcO4Wxi3cjWZ2tbBhvjsiYuIQcvdRYV0aUS5hUU/RvnkD+M7qL5Zpa+X+FRD49x38uHQ/1s/tD5t61bHn2BV8N2EjrhyYjcpG5eGz+TgCgm5iy8JBMKigh3lrD2PgtK0I/HlqsftLnXIrjSMyX32y77Fjx3Dp0iWlthkaGooxY8bAzc0NR44cwZ49e2BqaoqBAwciNjZWqeeikqlRvWpwsrHAD3N+xu37sfjzQihW/3oK479vI9b5xqYmWjSug2fxrz/aTna2gBcJb8Wtupkh3FxtMHreTmRmZcOyRiWYmxli8aZjePg4HruPXsadB0/gbF+7KC6TSHQ/6jnqWZjCxLCsuJXT18lV77djl9G3sxP6dWyMmlWN4Dm6M0wMyuLkhX8BAHuOXcbs0W5oZl8bdWtWxmrP/rh+JxqRMXFFfUn0BSSQiMHMF2/FbJLMVw9kzMzMsHDhQshkMqW1efToUTRr1gwDBgyAubk56tSpgwULFsDIyAjHjx9X2nmo5KpuZoC4l28R/ThBLPv3wRPY1q8GDXU1SDU14OvZHz8u3Q+ZLDPf7c4b2xW/Hv4b4dHPAQCJb5IBAO7dmkIikaBxwxqoXd0Et+7FKPeCiD7jXtRT1Kpm/Nl6EwZ+i7H9XXOVv0lKRXZ2NjYvHIRWTnXz3E/0NXz1QGbixIl4/vw5tm3b9tE6z549w4QJE+Do6AgnJycsWrTok4GPmpoa7t27h4SE/35JSSQSbN++Hd999x0AYO3atXB3d5c7ztXVFf7+/gCAzMxMrFy5Es7OzrC3t4eHhwdevXoFAEhJScHcuXPh5OQEJycnzJkzB+np6QCAN2/e4Mcff4SdnR2cnZ3h5eWFtLQ08Rw5bVpbW8Pd3R3h4eEAgIyMDMyePRtOTk6wtbXFqFGj8Pz5c0VuJSngxcu3KKdfBmW0NMUyM5MK0NRQR1m9Mpg8pC1u34vF2Sth+W7TybomGlvXwKqfT4plMc9eYeH6I1gwvhte/O2Lk9unYM2vgTh/9b5Sr4foUwRBwIPoFzh9+S4cei6Abbf5mL/2d8gycgfpNnWrwuK9gCfw7zt48OgFWjS2hJqaGlyc6qJCOV1x/6Y9QTAorwer2mZFci1UMAUejVFCakrZvnogY2JiAg8PD2zatAkxMbn/SpXJZBg0aBBSU1Oxc+dO+Pr6IigoCEuXLv1om7169cLLly/RqlUrjB49Gjt37sSjR49gZmaG8uXL56tfq1evxqFDh7B48WLs27cPCQkJmDdvHgBg9uzZCA4OxoYNG7B9+3YEBwfD19cXAODp6Ym3b99iz5492LBhA27fvo2FC99NBj116hT27dsHX19fHDt2DIaGhpg58938it27d+Pq1avYvn07Dh48iOTkZCxevFiBO0mKCA59iGdxr7Hkx97Q0ZaiRhVDjOnfCgBQy9wYQ3o4Y9YqP4XaHNS9GY6eDcHTuP9SURrqaqhtboKf/S+i9eBlmLXSDxMGfYtmdkwtUdGJefYKKWkyaGlqYMfiYfCa0B0HTlzF3NWHP3lcVGwcxi7Yid7tG8OmbtVc+4+fu4V1u09j7tgukGpy7UiJIFHSVowUi3eeu7s7/P394e3tjU2bNsnt++uvv/D8+XPs378f5cqVAwDMnTsXo0ePxqRJk6Crq5urPQsLCxw4cACbNm1CUFAQzpw5g0WLFqF9+/b43//+hzJlynyyP4IgYP/+/Zg+fTpatGgBAFiwYAH++OMPvH79GidOnMCOHTtgb28PAFi4cCHu3r2LR48eITAwEP/88w/09fUBAF5eXujWrRtmzpyJx48fQ1NTE6ampjA1NcWcOXMQGRkJAIiNjYWWlpYYbP3vf/9DYmKiQvdRT0dLofqqbsyCXdgw3x2PgpYjITEJG/ecxbxxXbF8Wh+s+vkk0tIzoK+rDYmaBNpamtDX1Qbw331+/36rq6uho4s1JnnvEesBQJ+OjrC3MkebIcsBAFGx8WhYxwxThrZjeomKTLXKFREZuATly+pAIpGgoWUVZAvZGDn3V3hP6gF19dx/04ZHP4fbqHWoXsUIqz375dofEHQTQ2ftwIjvWmJgt6ZFcRlEeSoWgYy6ujrmz5+P/v37IzAwUG5fREQEqlevLgYxAGBnZ4fMzEw8evQIK1asQHDwf0tZb9y4AQCoVasWli9fjszMTNy4cQMBAQHYv38/jIyMMHv27E/259WrV0hMTISVlZVYVqtWLYwfPx63bt1CVlaW3D4HBwc4ODjg7NmzyM7OFoOfHNnZ2YiOjkanTp2wa9cutG7dGo0aNUKbNm3Qq1cvAECfPn0QEBAAZ2dnODo6ok2bNujRo4dC9zHypLdC9ekdQQB0jMtikUdXZGQDDS2rwMeyF3ym9BLrrJzRF74z+0Kq/t9x79/vbAGQZQG7lw7F+6OuGVmAAODFheViWWY2kJUtX0ZU2CobyP/R19CiEtLSM5CSnAyjivpy++5EPEXHkWtQ3cwQR9aPRlk9qdz+/SeuYdicXzG8pzNWTutZ6H0n5SmNq5aKRSADvAtOevbsCW9vbwwf/t9zDbS0co8yZGVlif/v7e0tNwcFAJYsWYKuXbuibt260NDQQOPGjdG4cWPo6enh7NmzAPL+QmRmvssXa2h8/LZoamp+dF9WVhb09fXh55c7JWFiYgJtbW388ccfuHjxIs6ePYtt27Zh//79OHz4MGrXro0zZ84gKCgIQUFBWLlyJY4dO4bdu3fn+01Ts60nklLS81VX1ZXTL4Pti4dimOcOJL5JAQB4TegOE8Oy8N50TK7uft/R2OF3AYcCr+N5/Bvo6Wgh8qS33P0e0aclvm1qhd4TNsgdO969NTq72KDdsJVi2YLxXVHN1BBDZn58Xhjl9iiIgd+XOn3pDn6Y8zNCjy2Cjva7oOTa3VhULKcL/bL6SHtvqsyz+NfoPHodLKoZY5/vGEi1teT2n/vnHobN/hU/fNcSiyf3lNtHBaNdBL+RGcgUsqlTp6J9+/ZyE39r1KiBhw8fIjExUZzfEhISAg0NDVSrVg1ly5bN1c6FCxeQmZkJT09PufKyZcuiYsWKAN4FJMnJyeK+5ORkvHz5UqxXoUIFhIWFwdLSEgBw9+5djBw5EgEBAVBXV0dYWBgcHBwAAIGBgVi/fj2WL1+Ot2/fQiKRoFq1agCAe/fuYc2aNfDx8cHly5fx5MkT9O/fHy4uLhg3bhycnZ1x//59REVFQSqVomPHjujQoQNCQkLQp08fJCQkwNDQMF/3LyklHW+T0z5fkfA2OQ3aWpr4cVh7rNjxJ1o41MF3HRqj00hfhN5/LFc3IyMLsc9f4UH0CwCAllQDgiB/v2tWNcKdB09y3f9dRy5jTH9XTBnSDj8fuggn6xro28kJ3//4E79WVGQcrWuijJYUHot2Y/rwjnj4OB7z1hyGx8A2yMrKRvyrJFQopwOppgbm+B5CVlY2Ns3rj+SUdCT/f7Cuq6MFbakGxnntQjO72pgw8Fs8j38jniPneCreJBKgoHFIMYtjilcgU6FCBUydOhWzZ8+Gmdm7GfDNmjVD1apVMW3aNEyZMgWvXr2Cl5cXOnfunGcQAwBjxozB5MmToaWlBTc3N2hqauL69evYunUrfHx8AAANGzbE6tWr8ccff6Bu3bpYt24d1NT+yxO7u7tj9erVMDExgYGBAby9vdGoUSPo6+ujW7du8Pb2xoIFCyCRSLBq1Sq0aNECFhYWaN68uXgN6urqmDNnDsqVK4eyZcsiOzsbS5cuhZGREerVq4eAgACUKVMG1atXx61bt7Bp0yZUqFABVapUwdGjR1GpUiVUqMCnZRaWobO2Y9XMfri4ZxYePUnAkJnbcePO5x9S5+baCOlZ8mXGFfVx+4MACAAePUlAj3Hr4DWhO4b2ao7Y56/gseg3nLl8V1mXQfRZ+rraOLhmLGatPAjXQUuhp6OFwT2c4eHeBjFPX8Km6zwc3eSBZna1ERB0E6npGbDu5iXXxvQfOqB1k/qIffYKsc9eoW6HWXL7j27ygLN9naK8LCIAgEQQBOFrndzS0hK//vornJycxDJBENCvXz+8ePFCfLJvTEwMvLy8cOXKFejq6sLNzU0MVD7m9OnT2L59O8LCwpCRkQFLS0uMHDkSbdq0Ec+zbNkyHDhwAGpqahgyZAguXryI7t27o0ePHsjIyMCKFStw+PBhZGZmwsXFRQxKkpKS4O3tjZMnT0JTUxMdO3bEjBkzIJVK8fLlSyxatAhBQUHQ0NBA8+bNMXv2bDEg2b59O3bt2oW4uDjUrFkT06dPR9OmTZGdnY0VK1bg999/x+vXr9GgQQPMmTMH9evXz/f9NHaeyr/yi4C+rjZeXFjO+13EXl1d97W7oFK0NcC0UREritSS7exTSPrwLzEF6Wmp48aib/NV19/fX1yd+z6JRIKwsDDcuXMH8+bNw/3791GrVi0sWLBA4Sfwf9VAhpSLv1iLBgOZr4OBTNFiIFP0iiSQmXMKyQUMZHS11HHDK3+BTFpaGt6+fSu+zszMxKBBg+Di4oKJEyeibdu2cHNzQ69evbBnzx788ccfOHXqFHR0cj91+mO++nNkiIiIqHTS1taGkZGRuB05cgSCIGDq1Kk4fvw4tLS0MG3aNFhYWMDT0xO6uro4ceKEQudgIENERKQivuaTfRMTE/HTTz9hypQpkEqluHnzJuzt7cX2JBIJ7OzsEBISolC7xWqyLxERERUeZa5aSkpKkiuXSqWQSqV5HPHOnj17YGxsjPbt2wMA4uLiUKtWLbk6BgYG4kf35BcDGSIiIlJYixYt5B5jMm7cOIwfPz7PuoIg4MCBA3LPiUtNTc0V+EilUoU/RJqBDBERkYpQU5NATa1gQzI5x58/f16u/FOjMbdv38bz58/RqVMnsUxLSytX0CKTyaCtrf3h4Z/EQIaIiEhFKDO1pKenl+9j/vrrLzg4OMh93JCJiQni4+Pl6sXHx8PY2PjDwz+Jk32JiIioUN26dQt2dnZyZTY2Nrhx4wZyngIjCAKuX78OGxsbhdpmIENERKQivtaqpfDw8FwTe9u3b483b97A29sbDx48gLe3N1JTU9GhQweF2mYgQ0REpCJyUksF3RQVHx+f62OF9PT0sHnzZgQHB6NHjx64efMmtmzZotDD8ADOkSEiIlIZX+vTr2/dupVnubW1NQ4dOlSg/nBEhoiIiEosjsgQERGpiK81IlOYGMgQERGpCGUuvy4umFoiIiKiEosjMkRERCpCAiWkllC8hmQYyBAREakIppaIiIiIihGOyBAREakIrloiIiKiEoupJSIiIqJihCMyREREKoKpJSIiIiqxSmNqiYEMERGRiiiNIzKcI0NEREQlFkdkiIiIVIUSUkvF7MG+DGSIiIhUBVNLRERERMUIR2SIiIhUBFctERERUYnF1BIRERFRMcIRGSIiIhXB1BIRERGVWEwtERERERUjHJEhIiJSEaVxRIaBDBERkYrgHBkiIiIqsUrjiAznyBAREVGJxREZIiIiFcHUEhEREZVYTC0RERERKUAmk2HBggVo3LgxmjZtipUrV0IQBADAnTt30Lt3b9jY2KBnz54IDQ1VuH0GMkRERCpCgv/SS1+8KXjORYsW4e+//8a2bduwYsUK7N+/H/v27UNKSgpGjBgBBwcH+Pv7w9bWFiNHjkRKSopC7TO1REREpCLUJBKoFTA1pMjxiYmJ8PPzw44dO2BtbQ0AGDp0KG7evAkNDQ1oaWlh2rRpkEgk8PT0xPnz53HixAn06NEj//1R+AqIiIiI8iE4OBh6enpwdHQUy0aMGAEfHx/cvHkT9vb24pwbiUQCOzs7hISEKHQOBjJEREQqosBpJQVXPcXExMDMzAyHDx9G+/bt0bp1a6xfvx7Z2dmIi4uDsbGxXH0DAwM8e/ZMoWtiaomIiEhFKHPVUlJSkly5VCqFVCqVK0tJSUF0dDT27t0LHx8fxMXFYe7cuShTpgxSU1Nz1ZdKpZDJZAr1h4EMERGRilCTvNsK2gYAtGjRAsnJyWL5uHHjMH78eLm6GhoaSEpKwooVK2BmZgYAePLkCfbs2QNzc/NcQYtMJoO2trZC/WEgQ0RERAo7f/683OsPR1cAwMjICFpaWmIQAwA1atTA06dP4ejoiPj4eLn68fHxudJNn8M5MkRERKpC8l966Uu3nPXXenp6cltegYyNjQ3S09MRFRUllkVGRsLMzAw2Nja4ceOG+EwZQRBw/fp12NjYKHRJDGSIiIhURFFP9q1ZsyZcXFwwc+ZMhIWF4a+//sKWLVvQr18/tG/fHm/evIG3tzcePHgAb29vpKamokOHDgpdEwMZIiIiKjTLly9HtWrV0K9fP0yfPh0DBgyAu7s79PT0sHnzZgQHB6NHjx64efMmtmzZAh0dHYXa5xwZIiIiFSH5//8K2oYi9PX1sXTp0jz3WVtb49ChQwXqDwMZIiIiFaHMVUvFBVNLREREVGJxRIaIiEhFKPOBeMUFAxkiIiIVoeiqo4+1UZwwtUREREQlFkdkiIiIVISaRAK1Ag6pFPR4ZctXILNu3bp8Nzhu3Lgv7gwREREVntKYWspXIHPlypV8NVbcJgARERHRf1R2su/OnTsLux9ERERECvuiyb4xMTFYsmQJxowZgxcvXuDgwYMIDg5Wdt+IiIhIiYr6s5aKgsKBzNWrV9GlSxc8fvwYf/31F9LT0xEZGYlBgwbh5MmThdFHIiIiUoKcyb4F3YoThQOZZcuWYcqUKVizZg00NN5lpqZNm4apU6dizZo1Su8gERER0ccoHMjcv38fLVu2zFXeunVrPHr0SCmdIiIiIuWTKGkrThQOZMzMzHD79u1c5UFBQTAzM1NKp4iIiEj5clYtFXQrThR+IN7EiRMxY8YM3L59G1lZWTh8+DBiY2MREBDw0Y/pJiIiIioMCo/IfPvtt9i9ezcSEhJQu3ZtnD59GjKZDLt370bHjh0Lo49ERESkBGoS5WzFyRd9REHdunU5+kJERFTCqOwD8T50+PBh7N27FxEREdDU1ETNmjUxePBgtGnTRtn9IyIiIvoohQMZX19f/Pbbbxg4cCBGjhyJ7Oxs3Lp1C9OmTYOHhwcGDx5cCN0kIiIiZShmAyoFpnAgs2/fPixZsgStWrUSy1q3bo26devC29ubgQwREVExxdQSAEEQULly5VzlNWrUQHp6ulI6RURERMqnjMm6xW2yr8KrlsaNG4d58+YhIiJCLHv69Cm8vb0xatQopXaOiIiI6FPyNSJTt25duaEkQRDQuXNnlClTBmpqakhOToZEIsGDBw8wbNiwQussERERfTmVTS39+uuvhd0PIiIiKmTK+IiB4hXG5DOQcXR0zFdjL168KFBniIiIiBSh8GTfyMhILF++HA8ePEBWVhaAd6kmmUyGly9f4s6dO0rvJBERERWcmkQCtQKmhgp6vLIpPNl3zpw5ePnyJYYNG4b4+HgMHToU7du3R1JSEry9vQujj0RERKQEEolytuJE4RGZ27dvY9++fahXrx4OHz6MmjVrYsCAAahRowYOHjyI7t27F0Y/iYiIiHJReERGQ0MD+vr6AICaNWvi7t27AICmTZvi3r17yu0dERERKU3OqqWCbsWJwoGMra0ttm3bhrS0NDRo0ABnzpyBIAgIDQ2FlpZWYfSRiIiIlKA0ppYUDmRmzpyJCxcu4LfffkPXrl2RkJAAR0dHTJ48Gf379y+MPhIREVEJderUKVhaWsptHh4eAIA7d+6gd+/esLGxQc+ePREaGqpw+wrPkalVqxZOnjyJtLQ0lClTBn5+fvjnn39Qvnx5NGrUSOEOEBERUdH4GquWHjx4gFatWsHLy0ss09LSQkpKCkaMGAE3Nzf873//w549ezBy5EicOnUKOjo6+W4/X4HMkydP8ix/9eoVAKBOnTpiPVNT03yfnIiIiIqOMlJDih4fERGBOnXqwMjISK784MGD0NLSwrRp0yCRSODp6Ynz58/jxIkT6NGjR77bz1cg4+rqmusjCj6c7JNTljP5l4iIiIqXr/ERBREREWjatGmu8ps3b8Le3l5sTyKRwM7ODiEhIcoPZE6fPp3vBomIiKj0S0pKknstlUohlUrlygRBQFRUFC5cuIDNmzcjKysL7du3h4eHB+Li4lCrVi25+gYGBggPD1eoH/kKZMzMzBRqlL6OmYvHQJYlfO1ulHpS9Xd/PfB+F63vfw3+2l1QGWU01bDT3RY/7LmB1Izsr90dlZBzzwubGr5glU8ebQBAixYtkJycLJaPGzcO48ePl6v75MkTpKamQiqVwtfXF7GxsVi0aBHS0tLE8vdJpVLIZDKF+qPwZF8iIiIqmZSZWjp//rxc+YdBCfBuIOTKlSsoV64cJBIJ6tWrh+zsbPz4449wdHTMFbTIZDJoa2sr1B8GMkRERKQwPT29fNUrX7683GsLCwukp6fDyMgI8fHxcvvi4+NhbGysUD8KOsJEREREJYREAqgVcFNkQOevv/6Ck5MTUlNTxbK7d++ifPnysLe3x40bNyAI71L0giDg+vXrsLGxUeiaviiQycrKQlBQEH7++We8efMGN2/exNu3b7+kKSIiIioiBQ1icrb8srW1hZaWFmbPno3IyEicO3cOS5cuxfDhw9G+fXu8efMG3t7eePDgAby9vZGamooOHToodE0Kp5aePn2KYcOGITExEa9fv0br1q2xdetW3LhxA9u2bYOlpaWiTRIREVEppKenh23btmHx4sXo2bMndHV10bdvXwwfPhwSiQSbN2/GvHnzsH//flhaWmLLli0KPQwP+IJAZuHChbC3t8f8+fPh4OAAAFi5ciU8PT2xaNEi7Ny5U9EmiYiIqAh8jefI1K5dGzt27Mhzn7W1NQ4dOlSg/iicWrp27RqGDh0KdXV1sUxTUxNjxoz5os9IICIioqJR1KmloqBwIKOtrY2EhIRc5VFRUfmewUxERESkDAoHMn379sXcuXMRFBQE4F0A4+fnhzlz5qBXr17K7h8REREpSc5nLRV0K04UniMzduxYlC1bFvPnz0dqaipGjBgBAwMDDB48GMOGDSuMPhIREZESfI1Pvy5sX/RAPHd3d7i7uyMlJQVZWVnQ19dXdr+IiIhIyZT5EQXFhcKBzOHDhz+5v1u3bl/YFSIiIiLFKBzIrFmzRu51VlYWEhISoKGhAWtrawYyRERExZQy5rgUs8yS4oHMmTNncpUlJydj7ty5fBgeERFRMaYGJcyRQfGKZJSS6tLV1cX48eM/+sAbIiIiosKgtE+/DgsLQ3Z2trKaIyIiIiVjagnvVix9+Hji5ORk3Lt3D4MHD1ZWv4iIiEjJlPFk3uL2ZF+FAxknJ6dcZVKpFFOnTkWTJk2U0ikiIiKi/FA4kElMTMTAgQNRrVq1wugPERERFRKJpOAPtCtuqSWFJ/seOXIEamrF7XE4RERE9Dn8iAIAgwcPxoIFCzB48GCYmppCS0tLbr+pqanSOkdERET0KV/8QLy//voLAMSJv4IgQCKR4O7du0rsHhERESmLyk72vXr1KmxtbaGhoYHTp08Xdp+IiIioEEggKfDj7AregnLlK5AZOHAgLly4AAMDA5iZmRV2n4iIiKgQlMYRmXzN2hUEobD7QURERKSwfM+R+fAheERERFSylMYRmXwHMj179szXsmvOoSEiIiqeJBKJEj6ioHhFMvkOZIYMGQJ9ff3C7AsRERGRQvIVyEgkEnTq1AkGBgaF3R8iIiIqJCqbWuJkXyIiopKvNH76db5WLXXv3j3XE3yJiIiIvrZ8jcj4+PgUdj+IiIiokKlJJEpILRWvIRmFP6KAiIiISqbSOEeGH2NNREREJRZHZIiIiFSFEib7FrOPWuKIDBERkapQg0Qp25caMWIEZsyYIb6+c+cOevfuDRsbG/Ts2ROhoaFfcE1ERESkEnKWXxd0+xIBAQE4d+6c+DolJQUjRoyAg4MD/P39YWtri5EjRyIlJUWhdhnIEBERUaFKTEzE0qVL0bBhQ7Hs+PHj0NLSwrRp02BhYQFPT0/o6urixIkTCrXNQIaIiEhF5KxaKuimqCVLlqBr166oVauWWHbz5k3Y29uLn90kkUhgZ2eHkJAQxa5J8e4QERFRSfTuOTIF3wAgKSlJbpPJZHme89KlS7h27RrGjBkjVx4XFwdjY2O5MgMDAzx79kyha+KqJSIiIlJYixYtkJycLL4eN24cxo8fL1cnPT0d8+bNw9y5c6GtrS23LzU1FVKpVK5MKpV+NCD6GAYyREREKkKZn7V0/vx5ufIPgxIAWLduHRo0aIDmzZvn2qelpZUraJHJZLkCns9hIENERKQi1KCEjyj4/+XXenp6n60bEBCA+Ph42NraAoAYuPz555/o3Lkz4uPj5erHx8fnSjd9DgMZIiIiKhQ7d+5EZmam+Hr58uUAgKlTp+Lq1av46aefIAgCJBIJBEHA9evXMWrUKIXOwUCGiIhIRSgztZQfZmZmcq91dXUBAObm5jAwMMCKFSvg7e2Nvn37Yu/evUhNTUWHDh0U6g9XLREREakINSVtyqCnp4fNmzcjODgYPXr0wM2bN7Flyxbo6Ogo1A5HZIiIiKhI/O9//5N7bW1tjUOHDhWoTQYyREREKkIikSghtVS8PjWSgQwREZGKkKDgH15dvMIYBjJEREQq492TeQveRnHCyb5ERERUYnFEhoiISIUUr/GUgmMgQ0REpCKK+jkyRYGpJSIiIiqxOCJDRESkIrj8moiIiEosZTyZt7ilcopbf4iIiIjyjSMyREREKoKpJSIiIiqxSuOTfZlaIiIiohKLIzJEREQqgqklIiIiKrFK46olBjJEREQqojSOyBS3wIqIiIgo3zgiQ0REpCJK46olBjJEREQqgh8aSURERFSMcESGiIhIRahBooRVS8VrSIaBDBERkYpgaomIiIioGOGIDBERkYqQQKKEVUvFa0iGgQwREZGKYGqJiIiIqBjhiAwREZGKkChh1RJTS0RERPRVlMbUEgMZIiIiFVEaAxnOkSEiIqJCEx0djWHDhsHW1hYuLi7YunWruC8mJgaDBw9Go0aN0LFjR1y4cEHh9hnIEBERqQiJkv7Lr+zsbIwYMQIVKlTAoUOHsGDBAmzcuBFHjx6FIAgYO3YsDA0N4efnh65du2LcuHF48uSJQtfE1BIREZGKUJMAQgFTQ2oKHB8fH4969eph/vz50NPTQ/Xq1dGkSRMEBwfD0NAQMTEx2Lt3L3R0dGBhYYFLly7Bz88P48ePz39/vuAaiIiIiD7L2NgYvr6+0NPTgyAICA4OxtWrV+Ho6IibN2+ifv360NHREevb29sjJCREoXMwkCEiIlIRykwtJSUlyW0ymeyT53Z1dUX//v1ha2uLdu3aIS4uDsbGxnJ1DAwM8OzZM4WuiaklIiIiFaHMVUstWrRAcnKyWD5u3LhPpoTWrFmD+Ph4zJ8/Hz4+PkhNTYVUKpWrI5VKPxsQfYiBDBERESns/Pnzcq8/DEo+1LBhQwBAeno6pk6dip49eyI1NVWujkwmg7a2tkL9YGqJiIhIRUigjPTSO3p6enJbXoFMfHw8AgMD5cpq1aqFjIwMGBkZIT4+Plf9D9NNn8NAhoiISEWoSZSz5VdsbCzGjRuH58+fi2WhoaGoWLEi7O3t8e+//yItLU3cFxwcDBsbG4WuiaklUnm7fzoMHb0y6N6vHQDgVvBdBJ28jDev3qJSFWO07+qCKuaVPtvOxTPX8M/Fm5g0Z5hYlpKciqP7AxFxPxo6umXQqn1T2DjUK7RrIcrhULU8JraykCv7J/oV1pyLRJXy2hjyjTlqVNTB87fp+PWfR7j7POmjbfWwqYzWdYygribBP9GvsPOfGGRkCwAAIz0phjUxRy1DXSQky7DzagxCn74t1GujkqNhw4awsrLCrFmzMHPmTDx+/BjLli3DqFGj4OjoiMqVK2PmzJkYM2YMzp49i1u3bsHHx0ehc5TaEZmMjAysXbsWrVu3RoMGDeDi4gIfHx8kJX38m5VUz+0b9xB+N0p8HR0Zi9/3nULLb7/BmOkDUbW6KXb/dAjp6Z+efPYyIRFBJy/lKj+850+kpaVjmEdftGjjhCP7TyE2WrEZ+URfwrS8Nq7HJGLs/pvitvXvaJTRVMOMb+vgcWIqZh69g6uPXmFiKwuU1c7771q3BiZoY2mE9X9FYWlgOKwq6aO7TWVx/0QXC7xOzcDc42G4EPkSE10sYKCrWVSXSQoq6gfiqaurY8OGDShTpgz69OkDT09PuLu7Y+DAgeK+uLg49OjRA0eOHMH69ethamqq0DWV2hGZ5cuX4++//8aiRYtQtWpVxMTEwNvbG9HR0di0adPX7h4VAynJaTh19DxMq5qIZUlvUtDyWydx1KRlWydcCgpG3LOXnxyVOXbgNCqZGeNN4n+B8sv4RNy/E4UJs4eiQsVyMKlsiJjop7j69818jfAQFYRZOW3EJqbidVqmXHnbukZIz8jCjiuPIAiA/82naGRWDjUMdHDz8Ru5uhIJ0L6eCfZci8WdZ+9GWfxuPkVzCwMAQP1K+jDR18LCE/eQnpmNJ6+fwaqyPlrWMoT/zadFc6GkkK/xWUsmJiZYt25dnvvMzc2xa9euAvWn1AYyhw4dwuLFi9GkSRMAQJUqVTB//nwMGDAAL168UHgyEZU+J4+eh7V9Pbx989/yQatGdcR/Z8gycfncdejq6cCoUsWPthNy9Q4yMjJh59QAQX9eFstjo5+hbHl9VKhYTiyrVsMUF05fVfKVEOVmVq5Mnimeeib6CI55DUH4r2zu8bA82zAtpw19bQ1ci0kUy/6Oeom/o14CAGoZ6uLhyxSkZ2aL+++/SEItI13lXAQpneT/t4K2UZyU2tSSRCLB5cuXkZ393zeYra0tAgICUKFCBbi6usLf31/cd+XKFVhaWoqvP/yQq19//VXcd+vWLfTr1w82NjZo164dAgICxH3Xrl1Djx49YG1tDTc3N/z555/ividPnmDo0KGwtbVFkyZN4OXlhYyMDABAWFgY+vbtCxsbGzRv3vyj0SspR2T4I0RHxKLlt9/kvf/+IyyeuQ5BJy+jfbeW0NLKe1lhclIKAo/9BbferXPtS3qbDP2y8j/Q9fR18OY15w9Q4atUVgvWpmWxrJsVVnRvgD52ZlBXk8BYXwtv0zMw9JtqWNfbGvM7WKL2RwIPI10tJKdnobaRHhZ1rofVPRvie4cq0Pj/2Z7ldTTxKiVD7pjXqRmoqPPpZbhEylRqR2QGDhyINWvWIDAwEC1btkTTpk3h7OyMWrVqffbY9PR0DB06FFZWVti/fz9iYmIwZcoUVK1aFdbW1hg6dCi6dOkCb29vhISEYPr06bCwsICBgQFGjhyJSZMmoXnz5ggJCcGMGTNgYGAABwcHeHl5QUdHB4cPH0ZCQgI8PDxQs2ZNDBgwANOmTYO9vT2WLVuGqKgoeHh4oGHDhmjZsmW+r1mqXtzi5OIpIyMTAQdOo2vv1tAto4mc2/b+/TMzM8TYqQMQ9m8kDu85CSOjcqhW3VSunlRdglO/n4O9kxWqmBnheewLSCT/7c/OzISmprpcu9pSDWRlZvFr9QXKaJbav7uUrqKOJrQ11SEIAn66+BCGelL0tauCMppqKKOpDrcGlXD6XhzWnIuAY7UKmPFtbcwJuCsGJdr/f6/1tNUh1VBDP/sq2H8jFhKJBN87VIVUQw17gmOho6kOAYLc1+bd94Aav14K0i6i+6UGScE/a0k5XVGaUhvIjB07FlWrVsVvv/2G/fv3Y+/evdDV1YWnpyd69uz5yWMvXLiAly9fYvHixdDT00Pt2rUxe/ZsqKmpISAgAOXKlRNf16xZE69fv0ZaWhp2796Npk2b4vvvvwfwLvd39+5d/PLLL3BwcMDjx49hZWUFU1NTmJubY8uWLShbtiwA4PHjx2jdujXMzMxQtWpV7NixA1WqVFHsmpvV+LKbpWLmrPkdrvYW2DjSFQBwJ1AfADCpRc08an+DHm9fI+lBFCYNdJbbU0eSitfP43Fm3QiU0ZZiZ+ILXD2rIbYjREXidcwTuXb/VEvF/jJaHzkXkfIIAtCqtgFc67ybz5KVDbSxNH6XWpAAvW0ro7ftu0m76ZnAyu4NoPHBb6jh35gjIxuwMCwDz7a1xXaM9Y3QoZ4RMrMBAYBrbQPxmMzsd3V2utsWxWWSgkpjaqnUBjIA0KVLF3Tp0gWvXr3ChQsXsGvXLnh6esqlkPISFRWFGjVqQE9PTyzLCX4WLFiA+vXrQ03tv+/4IUOGAAC2b9+Os2fPwtb2v2/gjIwM1KjxLsAYPnw4Zs2ahVOnTqFFixbo2LEj6tevDwAYOXIkVq5ciX379sHFxQVdu3aFkZGRQte7/mIUZFnC5yuquG2/X8Hbt8ko5zQJAJCZmQUA2P/ndQwf1xsSNQnM3p8ArFkGkREvsOp8JIB3Iy5jm9XAwl/P4dHTl6jUcjqAdx9Xn5WVhXJOkzBoVHe8fpWO8McvxeMAIPhyBLR0y8iVUf5cj379tbtQolUuq42Fnerh7vO3ePYmDbuvxYr7RjStjmRZplimramGn/raYPmZB5jgUgvjDt7G2/+fNFyprBa8OtXH2IO30bymAepX0sfyMw/Etro0qISahrrwDYoo2gss4XLuOSmuVAYyYWFhOHz4MGbMmAEAqFChAtzc3NCuXTu0bdsWly9fznVMVlaW+G8NjY/flk/ty8zMhJubG0aNGpXnMV26dEGTJk0QGBiIoKAgeHh44IcffsCkSZMwYsQIdOjQAYGBgThz5gwGDRoELy8v9O7dO9/XLcsSGMjkw6AxvZGV9d/cqcBjfwEA2nRujr+DriHx5Ru4j+wh7o+NeY7KZsa57m1bt+Zo4uoovr57OxxX/grB4DG9UbacHsro6yLx5RvEJbxBufLvRn0iIx7DrFolfp2+QGpG9ucrEQCgoWlZjGleAxMO3hLfayZltfA2LRP3XyShrom+3P001tfCpajkXPf4QVwyMrKyYaynhRdv3z2CwEBXC6myLMQnyXD3eRLa1zNBZraAjP8/T01DXdx7kcSvV3GljOGUYjYkU9xSXUqRlZWFHTt24M6dO3LlUqkU2traqFixIjQ1NeU+7ComJkb8d/Xq1REdHS33GRBLlizBokWLUL16ddy7dw/Ce1P+J06ciK1bt6JGjRqIjo6Gubm5uJ0+fRpHjx4FAKxatQoJCQno168fNm/ejIkTJ+LkyZNIT0/HokWLIJVKMWTIEOzcuRPfffed3ERhUp7yFcvCwKi8uEm1pZBqS2FgVB72TRoiKjwGl89fR0LcK5w98TceP3qGb1rYAXi3kilnlZOevo5cO7p6OlBTU4OBUXloSjVQ0aA8LCzNcWj3CTx7Eofrl0Nx+3oYGjdr9BWvnlRB+IskZGRmY3jT6qj8/5N++9lXwbF/n+H0/XhUq1AGPWwqw0RfCz1tKsNYXwsXI9+tRNLSUIOe1rs/vtIys3E2PB6DHKvBwlAXtQx10dfODEEP4pEtAHefv0VCigwjmlaHWTltuDUwQU1DXZx7EP+p7tFXVNTPkSkKpTKQsbKygouLC8aMGYOjR48iNjYWISEhmDdvHmQyGdq2bYuGDRvi4MGDuH//Pq5cuYLt27eLxzs7O8PQ0BBz585FREQETp8+jb1798LZ2Rlubm5ITEzE0qVL8fDhQ/j7++P06dNo1qwZ+vfvj9DQUKxatQoPHz7E0aNHsXLlSvHhPpGRkVi4cCHCwsIQHh6Oc+fOoX79+tDS0sL169fh5eWFyMhI3L59G9euXRPTTlR0TKuYoM8QN1y/8i82LtuJ8LsP8f2IHihb/l2aMTTkHnzmbM53e937t4dUW4qtvntwPvAKuvZty2fIUKFLy8zGksBw6GtpYGGnevihqTnO3o9HwL/PkZAsw5LAcNhWKQefLvVhW7U8lp9+gFep7yb6drIygWfb/x5DsPtaLG4+fo0fW9fCj61r4daTN9h3/TGAd/NwVp2NQPkymvDqXA9NaxjANygCCckZefaLqDBIhPeHFkqR1NRUbNq0CSdOnMCTJ0+go6MDZ2dnTJkyBaampoiNjcXMmTNx48YN1KxZE6NGjcKkSZNw7949AEBERAQWLlyIGzduwNDQED/88AP69esHALhx4wYWL16Mu3fvomrVqpg0aRLatm0LAPj777+xfPly3L9/HyYmJhgyZIg4+TchIQELFizApUuXkJmZCRcXF8yZMwcVK1ZEdHS0eD4NDQ20b98es2bNUuhTQFedj2TKoghI1SWY1KIm73cRuxqV+LW7oDLKaKphp7st3HfeYIqoiOTc88J2Leo1sgv4Y0tNAjjUKPf5ikWk1AYyqoi/WIsGA5mvg4FM0WEgU/SKKpAJVlIgY1+MAplSmVoiIiIi1VAqVy0RERFRHkrhqiUGMkRERCpCGWuOilkcw0CGiIhIVUgkSniybzGLZDhHhoiIiEosjsgQERGpCH7WEhEREZVcpXCyL1NLREREVGJxRIaIiEhFcNUSERERlVhctURERERUjHBEhoiISEVw1RIRERGVXFy1RERERFR8cESGiIhIRXDVEhEREZVYpXHVEgMZIiIiFVEaJ/tyjgwRERGVWByRISIiUhWlcNUSAxkiIiIVURon+zK1RERERIXm+fPn8PDwgKOjI5o3bw4fHx+kp6cDAGJiYjB48GA0atQIHTt2xIULFxRun4EMERGRipBIlLPllyAI8PDwQGpqKnbv3o1Vq1bh7Nmz8PX1hSAIGDt2LAwNDeHn54euXbti3LhxePLkiULXxNQSERGRiijqVUuRkZEICQnBxYsXYWhoCADw8PDAkiVL0KJFC8TExGDv3r3Q0dGBhYUFLl26BD8/P4wfPz7f5+CIDBERERUKIyMjbN26VQxiciQlJeHmzZuoX78+dHR0xHJ7e3uEhIQodA6OyBAREakKJa5aSkpKkiuWSqWQSqVyZWXLlkXz5s3F19nZ2di1axe++eYbxMXFwdjYWK6+gYEBnj17plB3OCJDRESkIiRK+g8AWrRoAXt7e3HbvHnzZ8+/bNky3LlzB5MmTUJqamquwEcqlUImkyl0TRyRISIiIoWdP39e7vWHQcmHli1bhl9++QWrVq1CnTp1oKWlhcTERLk6MpkM2traCvWDgQwREZGKUOZnLenp6eX7GC8vL+zZswfLli1Du3btAAAmJiZ48OCBXL34+Phc6abPYWqJiIhIRUiUtCli3bp12Lt3L1auXIlOnTqJ5TY2Nvj333+RlpYmlgUHB8PGxkah9hnIEBERqYoijmQiIiKwYcMG/PDDD7C3t0dcXJy4OTo6onLlypg5cybCw8OxZcsW3Lp1C7169VLokphaIiIiokJx+vRpZGVlYePGjdi4caPcvnv37mHDhg3w9PREjx49YG5ujvXr18PU1FShczCQISIiUhFF/VlLI0aMwIgRIz6639zcHLt27SpQfxjIEBERqQolTPYtbp8ayTkyREREVGJxRIaIiEhFKPHBvsUGAxkiIiJVUQojGaaWiIiIqMTiiAwREZGKKPiapWI3IMNAhoiISFVIlBCFKKMNZWJqiYiIiEosjsgQERGpiFI415eBDBERkcoohZEMAxkiIiIVURon+3KODBEREZVYHJEhIiJSERLxfwrYRjHCQIaIiEhFlMIpMkwtERERUcnFERkiIiIVoZQH4hW8CaViIENERKQyilsYUnBMLREREVGJxREZIiIiFcHUEhEREZVYXLVEREREVIxwRIaIiEhFMLVEREREJVZp/KwlBjJERESqorhFIUrAOTJERERUYnFEhoiISEWUxlVLDGSIiIhURGmc7MvUEhEREZVYHJEhIiJSEaVx1RJHZIiIiFSFREnbF5DJZOjcuTOuXLkilsXExGDw4MFo1KgROnbsiAsXLijcLgMZIiIiKlTp6emYPHkywsPDxTJBEDB27FgYGhrCz88PXbt2xbhx4/DkyROF2mZqiYiISEV8jVVLDx48wJQpUyAIglz55cuXERMTg71790JHRwcWFha4dOkS/Pz8MH78+Hy3zxEZIiIiFSGRKGdTxD///AMnJyfs27dPrvzmzZuoX78+dHR0xDJ7e3uEhIQo1D5HZIiIiEhhSUlJcq+lUimkUmmuev3798/z+Li4OBgbG8uVGRgY4NmzZwr1g4EMERGRylDGuqV3WrRogeTkZPH1uHHjFEoJpaam5gp8pFIpZDKZQv1gIENERKQilPFAvBznz5+Xe53XaMynaGlpITExUa5MJpNBW1tboXYYyBAREZHC9PT0CnS8iYkJHjx4IFcWHx+fK930OZzsS0REREXOxsYG//77L9LS0sSy4OBg2NjYKNQOAxkiIiIV8TVWLX2Mo6MjKleujJkzZyI8PBxbtmzBrVu30KtXL4XaYSBDRESkIiRK+k8Z1NXVsWHDBsTFxaFHjx44cuQI1q9fD1NTU4Xa4RwZIiIiKhL37t2Te21ubo5du3YVqE0GMkRERCpCmauWigsGMkRERCqiFMYxnCNDREREJRdHZIiIiFRFKRySYSBDRESkIpT3AQXFB1NLREREVGJxRIaIiEhFcNUSERERlVilMI5hIENERKQySmEkwzkyREREVGJxRIaIiEhFlMZVSwxkiIiIVAQn+1KxJlUvhe/QYijnPvN+F60ymsyEFxXt/7/X2rznRYb3+stJBEEQvnYniIiIiL4EQ0AiIiIqsRjIEBERUYnFQIaIiIhKLAYyREREVGIxkCEiIqISi4EMERERlVgMZIiIiKjEYiBDREREJRYDGSIiIiqxGMhQiWBpaYkpU6bkKvf394erq2uB2r548SL69u0LGxsb2NvbY/jw4QgNDS1Qm0SFLSMjA2vXrkXr1q3RoEEDuLi4wMfHB0lJSV+7a0RFioEMlRjHjh3DpUuXlNpmaGgoxowZAzc3Nxw5cgR79uyBqakpBg4ciNjYWKWei0iZli9fjpMnT2LRokU4ceIEfHx8cPHiRUydOvVrd42oSDGQoRLDzMwMCxcuhEwmU1qbR48eRbNmzTBgwACYm5ujTp06WLBgAYyMjHD8+HGlnYdI2Q4dOoQJEyagSZMmqFKlCpo0aYL58+fj7NmzePHixdfuHlGRYSBDJcbEiRPx/PlzbNu27aN1nj17hgkTJsDR0RFOTk5YtGjRJwMfNTU13Lt3DwkJCWKZRCLB9u3b8d133wEA1q5dC3d3d7njXF1d4e/vDwDIzMzEypUr4ezsDHt7e3h4eODVq1cAgJSUFMydOxdOTk5wcnLCnDlzkJ6eDgB48+YNfvzxR9jZ2cHZ2RleXl5IS0sTz5HTprW1Ndzd3REeHg7gXUph9uzZcHJygq2tLUaNGoXnz58rciupFJBIJLh8+TKys7PFMltbWwQEBKBChQpy71EAuHLlCiwtLcXX0dHRGDZsGGxtbeHi4oJff/1V3Hfr1i3069cPNjY2aNeuHQICAsR9165dQ48ePWBtbQ03Nzf8+eef4r4nT55g6NChsLW1RZMmTeDl5YWMjAwAQFhYmJjCbd68OdatW1co94VUDwMZKjFMTEzg4eGBTZs2ISYmJtd+mUyGQYMGITU1FTt37oSvry+CgoKwdOnSj7bZq1cvvHz5Eq1atcLo0aOxc+dOPHr0CGZmZihfvny++rV69WocOnQIixcvxr59+5CQkIB58+YBAGbPno3g4GBs2LAB27dvR3BwMHx9fQEAnp6eePv2Lfbs2YMNGzbg9u3bWLhwIQDg1KlT2LdvH3x9fXHs2DEYGhpi5syZAIDdu3fj6tWr2L59Ow4ePIjk5GQsXrxYgTtJpcHAgQOxc+dOuLq6Yt68efjzzz+RlpaGWrVqQVNT85PHpqenY+jQodDV1cX+/fsxd+5crFq1CmfPnkVCQgKGDh2KevXq4dChQxg5ciSmT5+OsLAwxMXFYeTIkejRoweOHj2K4cOHY8aMGbh27RoAwMvLCzo6Ojh8+DDWr1+PP//8E/v37wcATJs2DfXq1cOxY8fg7e2NrVu34ty5c4V+n0gFCEQlQJ06dYTLly8LmZmZgpubmzBy5EhBEATBz89PaNWqlSAIghAYGCjY2NgIiYmJ4nHnzp0T6tevLyQlJX207fDwcGHKlCmCvb29UKdOHaFOnTqCh4eHkJKSIgiCIKxZs0b4/vvv5Y5p1aqV4OfnJ2RnZwuOjo6Cn5+fXHtr1qwREhMThXr16gmXL18W9129elX49ddfhejoaKFu3brCmzdvxH1hYWFi2Y4dO4RmzZoJjx8/FgRBEBISEoSrV68KgiAIXl5egpubm/Dq1StBEAQhNjZWCA0NVfieUsn3+++/C3369BHq1q0r1KlTR7C1tRUOHjwoCMJ/79Ecly9fFurUqSMIwrvvlUaNGglv374V9x88eFAICgoSfvnlF8HV1VXIysoS923fvl24ceOGsGrVKmHcuHFyffDx8RHL3NzchBkzZggymUwQBEH4999/hZiYGEEQBMHOzk7w9fUV271+/brw4sULZd8SUkEaXzuQIlKEuro65s+fj/79+yMwMFBuX0REBKpXr45y5cqJZXZ2dsjMzMSjR4+wYsUKBAcHi/tu3LgBAKhVqxaWL1+OzMxM3LhxAwEBAdi/fz+MjIwwe/bsT/bn1atXSExMhJWVlVhWq1YtjB8/Hrdu3UJWVpbcPgcHBzg4OODs2bPIzs5GixYt5NrLzs5GdHQ0OnXqhF27dqF169Zo1KgR2rRpg169egEA+vTpg4CAADg7O8PR0RFt2rRBjx49FLyTVBp06dIFXbp0watXr3DhwgXs2rULnp6ecimkvERFRaFGjRrQ09MTy3r27AkAWLBgAerXrw81tf8G7IcMGQIA2L59O86ePQtbW1txX0ZGBmrUqAEAGD58OGbNmoVTp06hRYsW6NixI+rXrw8AGDlyJFauXIl9+/bBxcUFXbt2hZGRkXJuBKk0BjJU4tjZ2aFnz57w9vbG8OHDxXItLa1cdbOyssT/9/b2lpuDAgBLlixB165dUbduXWhoaKBx48Zo3Lgx9PT0cPbsWQDv5iJ8KDMzEwCgofHxb6FPDe9nZWVBX18ffn5+ufaZmJhAW1sbf/zxBy5evIizZ89i27Zt2L9/Pw4fPozatWvjzJkzCAoKQlBQEFauXIljx45h9+7defaVSp+wsDAcPnwYM2bMAABUqFABbm5uaNeuHdq2bYvLly/nOibnewH49Pv2U/syMzPh5uaGUaNG5XlMly5d0KRJEwQGBiIoKAgeHh744YcfMGnSJIwYMQIdOnRAYGAgzpw5g0GDBsHLywu9e/dW6NqJPsQ5MlQiTZ06FSkpKXITf2vUqIGHDx8iMTFRLAsJCYGGhgaqVasGExMTmJubixsAXLhwIc9gomzZsqhYsSKAdwFJcnKyuC85ORkvX74U61WoUAFhYWHi/rt376JFixaoUqUK1NXV5fYFBgaie/fuqFGjBt6+fQuJRCL2Jy0tDUuXLoVMJkNQUBAOHDgAFxcXLFiwAL///jsePnyI+/fv4/Dhwzh79iw6dOiAJUuWYOvWrQgODpabsEylW1ZWFnbs2IE7d+7IlUulUmhra6NixYq53rfvzyurXr06oqOjkZqaKpYtWbIEixYtQvXq1XHv3j0IgiDumzhxIrZu3YoaNWogOjpa7vvo9OnTOHr0KABg1apVSEhIQL9+/bB582ZMnDgRJ0+eRHp6OhYtWgSpVIohQ4Zg586d+O677+QmChN9KQYyVCJVqFABU6dOxePHj8WyZs2aoWrVqpg2bRru3buHy5cvw8vLC507d0bZsmXzbGfMmDHYtWsXli9fjnv37iEyMhIHDx7E1q1bMXjwYABAw4YNERYWhj/++ANRUVGYO3eu3LC7u7s7Vq9ejcuXLyM8PBze3t5o1KgR9PX10a1bN3h7e+PWrVu4ffs2Vq1ahW+++QYWFhZo3rw5pk6dilu3buHff//FzJkzkZKSgrJlyyI7OxtLly7FqVOnEBsbC39/f5QpUwbVq1fH27dv4e3tjUuXLiEmJgZHjx5FpUqVUKFChUK951R8WFlZwcXFBWPGjMHRo0cRGxuLkJAQzJs3DzKZDG3btkXDhg1x8OBB3L9/H1euXMH27dvF452dnWFoaIi5c+ciIiICp0+fxt69e+Hs7Aw3NzckJiZi6dKlePjwIfz9/XH69Gk0a9YM/fv3R2hoKFatWoWHDx/i6NGjWLlyJUxNTQEAkZGRWLhwIcLCwhAeHo5z586hfv360NLSwvXr1+Hl5YXIyEjcvn0b165dE9NORAXytSfpEOVHzmTf92VnZwt9+vQRJ/sKgiA8evRI+OGHHwRra2uhSZMmwuLFi4W0tLRPth0YGCj0799fsLOzExo2bCj06tVLOHXqlNx5lixZIjg4OAiOjo7Cxo0bhe+//16cSCmTyQQfHx/ByclJsLe3F6ZMmSJOOH779q0wY8YMwc7OTnBychIWLFggpKenC4LwbgLvpEmTBFtbW6Fx48bC5MmThZcvX4rn3bZtm9CqVSuhQYMGQpcuXYSLFy8KgiAIWVlZwtKlS4VmzZoJDRo0EPr27Sv8+++/Bbi7VBKlpKQIK1euFNq2bSs0aNBAcHR0FCZPnixOEI+JiRG+//57wcrKSnBzcxMCAgLEyb6CIAgPHjwQBg4cKDRs2FBo1aqV8Ntvv4n7rl+/LvTq1UuwsrIS2rdvL/z555/ivosXLwrdu3cXrKysBFdXV2Hnzp3ivvj4eGH8+PGCg4OD0KhRI2HixIlCQkKCIAiC8PDhQ2Ho0KHi+33OnDlCampqYd8mUgESQXhv/JCIiIioBGFqiYiIiEosBjJERERUYjGQISIiohKLgQwRERGVWAxkiIiIqMRiIENEREQlFgMZIiIiKrEYyBCpIFdXV1haWoqblZUV2rdvj59//lmp53F3d8fatWsBADNmzBA/G+hTZDIZ9u/f/8Xn9Pf3h6urq8L7PrR27Vq4u7t/cT8sLS1x5cqVLz6eiPKHHxpJpKJmzZqFjh07Anj3YYCXL1+Gp6cnypcvj27duin9fJ6envmqFxAQgE2bNuG7775Teh+IqPThiAyRitLX14eRkRGMjIxQuXJldO/eHU2aNMHJkycL7Xz6+vqfrceHjRORIhjIEJFIQ0MDmpqaAN6lhby8vNC6dWu4uLggKSkJT58+xahRo2BjYwNXV1esW7cOWVlZ4vGnTp1Cu3bt0KhRIyxcuFBu34eppd9//x3t27eHjY0N+vbtizt37uDKlSuYOXMmHj9+DEtLS8TGxkIQBKxfvx7Ozs5wcHDAqFGj8OTJE7Gd58+fY/jw4WjUqBG6d++OR48e5ft6T58+jW7duqFhw4ZwcHDA5MmT5T4xOiMjA56enrCxsUGbNm1w/Phxcd/n+kVERYOBDBEhIyMDJ0+exMWLF9G6dWux3N/fH8uWLcO6deugq6uLcePGwcDAAIcOHYKPjw+OHj2KTZs2AQAePHiAiRMnol+/fvDz80NmZiaCg4PzPN9ff/0FT09PDBo0CEeOHEGDBg0wcuRI2NraYtasWahUqRIuXLiAypUrY9euXTh69ChWrFiBffv2wcDAAEOHDkVGRgYAYMKECcjOzsaBAwfwww8/4JdffsnXNT969AgTJkxA//798ccff8DX1xd///233PycGzduiPehX79+mDp1KqKjowHgs/0ioqLBOTJEKmrevHnw8vICAKSlpUFbWxuDBg1Cly5dxDouLi6ws7MDAFy6dAlPnjzBgQMHoKamhpo1a2L69OmYOXMmxo4dCz8/Pzg4OGDw4MEAgDlz5uDs2bN5nnvfvn3o3Lkz+vXrBwCYNm0aNDU18fr1a+jr60NdXR1GRkYAgK1bt2LevHlwcnICACxcuBDOzs7466+/ULVqVdy4cQNnz56FqakpateujdDQUJw4ceKz15+dnY3Zs2eLc3GqVKmCpk2bIjw8XKxjbGyM+fPnQ1NTExYWFggKCsKBAwcwderUT/YrvxOKiajgGMgQqSgPDw+0bdsWAKClpQUjIyOoq6vL1TEzMxP/HRERgcTERNjb24tl2dnZSEtLw6tXrxAREYF69eqJ+zQ1NeVevy8qKgp9+/YVX0ulUkyfPj1XveTkZDx79gyTJk2Cmtp/A8hpaWl4+PAh0tPTUb58eZiamor7GjZsmK9Apnr16pBKpdi4cSPCw8MRHh6OBw8eoGvXrmKdevXqiak2ALCyskJERMRn+0VERYeBDJGKMjAwgLm5+SfraGlpif/OzMxEzZo1sWHDhlz1cibxfjhR9/0g4H0aGvn70ZMzx2b16tWoUaOG3L5y5crh0qVL+T7nh8LCwtCvXz+4urqKI0kfpqXeD1KAd4GbpqbmZ/tFREWHc2SIKF9q1KiBJ0+eoGLFijA3N4e5uTliY2OxZs0aSCQS1K5dG7dv3xbrZ2dnIywsLM+2zM3N5fZlZWXB1dUVwcHBkEgkYnnZsmVhYGCAuLg48ZyVK1fGsmXLEBUVhTp16uD169fivBUAuHv3br6u5/fff0fjxo2xYsUK9O/fH9bW1oiOjpYLjN5PMwHArVu3ULNmzc/2i4iKDgMZIsoXZ2dnmJmZ4ccff8S9e/dw7do1zJkzB2XKlIG6ujq+++47hIaGYuPGjYiMjMSSJUs+uorH3d0dR44cwaFDhxAdHQ0fHx8IggArKyuUKVMGr1+/xsOHD5GZmYnBgwfD19cXZ86cwcOHDzF79mxcv34dNWvWhIWFBZo0aYJZs2YhLCwMgYGB2LVrV76up3z58rh37x5u3bqFqKgo/O9//8Pt27chk8nEOk+ePIGXlxciIiKwfv163LlzR5zX86l+EVHRYWqJiPJFXV0dGzduhJeXF7777jvo6Oigffv24twWc3NzbNy4ET4+Pti4cSPatGmDli1b5tlW48aNMW/ePKxfvx5xcXFo0KABNm3aBG1tbXzzzTcwNzeHm5sbfvvtNwwbNgzJycmYO3cukpKS0KBBA2zbtk1M4axatQpz5sxB3759YWpqCnd3d/j7+3/2etzd3XHnzh0MHjwYWlpaaNy4McaOHYuAgACxTsuWLZGYmIju3bvDzMwMGzduhImJCQB8tl9EVDQkAp8+RURERCUUU0tERERUYjGQISIiohKLgQwRERGVWAxkiIiIqMRiIENEREQlFgMZIiIiKrEYyBAREVGJxUCGiIiISiwGMkRERFRiMZAhIiKiEouBDBEREZVYDGSIiIioxPo/lNeJXcPvZFsAAAAASUVORK5CYII="
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "id": "21fcfe5a660af49",
   "metadata": {},
   "source": [
    "## Model Probabilities Predictions"
   ]
  },
  {
   "cell_type": "code",
   "id": "c6eacecf2034725c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T20:07:36.299243Z",
     "start_time": "2025-01-16T20:07:36.126044Z"
    }
   },
   "source": [
    "pd.options.display.float_format = '{:.4f}'.format  # Ensure non-scientific number formatting\n",
    "gb_train_probabilities = best_gb_model.predict_proba(X_train_scaled)[:, 1]\n",
    "gb_train_predictions = best_gb_model.predict(X_train_scaled)\n",
    "\n",
    "gb_test_probabilities = best_gb_model.predict_proba(X_test_scaled)[:, 1]\n",
    "gb_test_predictions = best_gb_model.predict(X_test_scaled)\n",
    "\n",
    "# LightGBM\n",
    "lgb_train_probabilities = best_lgb_model.predict_proba(X_train_scaled)[:, 1]\n",
    "lgb_train_predictions = best_lgb_model.predict(X_train_scaled)\n",
    "\n",
    "lgb_test_probabilities = best_lgb_model.predict_proba(X_test_scaled)[:, 1]\n",
    "lgb_test_predictions = best_lgb_model.predict(X_test_scaled)\n",
    "\n",
    "# Neural Network\n",
    "nn_train_probabilities = nn_model.predict(X_train_scaled).ravel()\n",
    "nn_train_predictions = (nn_train_probabilities > 0.5).astype(int)\n",
    "\n",
    "nn_test_probabilities = nn_model.predict(X_test_scaled).ravel()\n",
    "nn_test_predictions = (nn_test_probabilities > 0.5).astype(int)\n",
    "\n",
    "# Logistic Regression\n",
    "logistic_train_probabilities = logistic_model.predict_proba(X_train_scaled)[:, 1]\n",
    "logistic_train_predictions = logistic_model.predict(X_train_scaled)\n",
    "\n",
    "logistic_test_probabilities = logistic_model.predict_proba(X_test_scaled)[:, 1]\n",
    "logistic_test_predictions = logistic_model.predict(X_test_scaled)\n",
    "\n",
    "# TabPFN\n",
    "tabpfn_test_probabilities = loaded_predictions_tabpfn[\"Probability\"]\n",
    "tabpfn_test_predictions = loaded_predictions_tabpfn[\"Predicted\"]\n",
    "\n",
    "# Weighted Ensemble for training data\n",
    "ensemble_train_probabilities = (\n",
    "        0.3 * gb_train_probabilities +\n",
    "        0.3 * lgb_train_probabilities +\n",
    "        0.4 * nn_train_probabilities\n",
    ")\n",
    "ensemble_train_predictions = (ensemble_train_probabilities > 0.5).astype(int)\n",
    "\n",
    "# Weighted Ensemble for test data\n",
    "ensemble_test_probabilities = (\n",
    "        0.3 * gb_test_probabilities +\n",
    "        0.3 * lgb_test_probabilities +\n",
    "        0.4 * nn_test_probabilities\n",
    ")\n",
    "ensemble_test_predictions = (ensemble_test_probabilities > 0.5).astype(int)\n",
    "\n",
    "training_results = pd.DataFrame({\n",
    "    \"Observation Index\": range(len(X_train_scaled)),\n",
    "    \"ID\": ID_train_resampled,\n",
    "    \"True Class (Train)\": y_train_resampled.values,\n",
    "    \"GB Prediction (Train)\": gb_train_predictions,\n",
    "    \"GB Probability (Success, Train)\": gb_train_probabilities,\n",
    "    \"LGB Prediction (Train)\": lgb_train_predictions,\n",
    "    \"LGB Probability (Success, Train)\": lgb_train_probabilities,\n",
    "    \"NN Prediction (Train)\": nn_train_predictions,\n",
    "    \"NN Probability (Success, Train)\": nn_train_probabilities,\n",
    "    \"Logistic Prediction (Train)\": logistic_train_predictions,\n",
    "    \"Logistic Probability (Success, Train)\": logistic_train_probabilities,\n",
    "    \"Ensemble Prediction (Train)\": ensemble_train_predictions,\n",
    "    \"Ensemble Probability (Success, Train)\": ensemble_train_probabilities,\n",
    "})\n",
    "\n",
    "# Create test results DataFrame\n",
    "test_results = pd.DataFrame({\n",
    "    \"Observation Index\": range(len(X_test_scaled)),\n",
    "    \"ID\": ID_test.values,\n",
    "    \"True Class (Test)\": y_test.values,\n",
    "    \"GB Prediction (Test)\": gb_test_predictions,\n",
    "    \"GB Probability (Success, Test)\": gb_test_probabilities,\n",
    "    \"LGB Prediction (Test)\": lgb_test_predictions,\n",
    "    \"LGB Probability (Success, Test)\": lgb_test_probabilities,\n",
    "    \"NN Prediction (Test)\": nn_test_predictions,\n",
    "    \"NN Probability (Success, Test)\": nn_test_probabilities,\n",
    "    \"Logistic Prediction (Test)\": logistic_test_predictions,\n",
    "    \"Logistic Probability (Success, Test)\": logistic_test_probabilities,\n",
    "    \"Ensemble Prediction (Test)\": ensemble_test_predictions,\n",
    "    \"Ensemble Probability (Success, Test)\": ensemble_test_probabilities,\n",
    "    \"TabPFN Prediction (Test)\": tabpfn_test_predictions,\n",
    "    \"TabPFN Probability (Success, Test)\": tabpfn_test_probabilities\n",
    "})"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m63/63\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 357us/step\n",
      "\u001B[1m10/10\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 719us/step\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "id": "6ccb72060175a8a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T20:05:33.518804Z",
     "start_time": "2025-01-16T20:05:33.512338Z"
    }
   },
   "source": [
    "test_results"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     Observation Index    ID  True Class (Test)  GB Prediction (Test)  \\\n",
       "0                    0   732                  0                     0   \n",
       "1                    1    37                  1                     0   \n",
       "2                    2   408                  0                     0   \n",
       "3                    3   784                  0                     0   \n",
       "4                    4   135                  0                     0   \n",
       "..                 ...   ...                ...                   ...   \n",
       "297                297   188                  0                     0   \n",
       "298                298   882                  0                     0   \n",
       "299                299  1318                  0                     0   \n",
       "300                300   335                  0                     0   \n",
       "301                301  1045                  1                     0   \n",
       "\n",
       "     GB Probability (Success, Test)  LGB Prediction (Test)  \\\n",
       "0                            0.0468                      0   \n",
       "1                            0.0389                      0   \n",
       "2                            0.0426                      0   \n",
       "3                            0.0240                      0   \n",
       "4                            0.0166                      0   \n",
       "..                              ...                    ...   \n",
       "297                          0.0457                      0   \n",
       "298                          0.0073                      0   \n",
       "299                          0.0090                      0   \n",
       "300                          0.0113                      0   \n",
       "301                          0.0150                      0   \n",
       "\n",
       "     LGB Probability (Success, Test)  NN Prediction (Test)  \\\n",
       "0                             0.1532                     1   \n",
       "1                             0.0643                     0   \n",
       "2                             0.0301                     1   \n",
       "3                             0.0969                     1   \n",
       "4                             0.0375                     0   \n",
       "..                               ...                   ...   \n",
       "297                           0.0433                     0   \n",
       "298                           0.0281                     1   \n",
       "299                           0.0101                     0   \n",
       "300                           0.0528                     0   \n",
       "301                           0.0290                     0   \n",
       "\n",
       "     NN Probability (Success, Test)  Logistic Prediction (Test)  \\\n",
       "0                            0.5561                           0   \n",
       "1                            0.4632                           1   \n",
       "2                            0.6250                           0   \n",
       "3                            0.7133                           0   \n",
       "4                            0.4053                           0   \n",
       "..                              ...                         ...   \n",
       "297                          0.3247                           0   \n",
       "298                          0.5199                           0   \n",
       "299                          0.3994                           0   \n",
       "300                          0.2448                           0   \n",
       "301                          0.3756                           0   \n",
       "\n",
       "     Logistic Probability (Success, Test)  Ensemble Prediction (Test)  \\\n",
       "0                                  0.4354                           0   \n",
       "1                                  0.5796                           0   \n",
       "2                                  0.1860                           0   \n",
       "3                                  0.2957                           0   \n",
       "4                                  0.0418                           0   \n",
       "..                                    ...                         ...   \n",
       "297                                0.1560                           0   \n",
       "298                                0.1721                           0   \n",
       "299                                0.2247                           0   \n",
       "300                                0.1818                           0   \n",
       "301                                0.2302                           0   \n",
       "\n",
       "     Ensemble Probability (Success, Test)  TabPFN Prediction (Test)  \\\n",
       "0                                  0.2825                         0   \n",
       "1                                  0.2162                         0   \n",
       "2                                  0.2718                         0   \n",
       "3                                  0.3216                         0   \n",
       "4                                  0.1784                         0   \n",
       "..                                    ...                       ...   \n",
       "297                                0.1566                         0   \n",
       "298                                0.2186                         0   \n",
       "299                                0.1655                         0   \n",
       "300                                0.1172                         0   \n",
       "301                                0.1635                         0   \n",
       "\n",
       "     TabPFN Probability (Success, Train)  \n",
       "0                                 0.1125  \n",
       "1                                 0.0365  \n",
       "2                                 0.0339  \n",
       "3                                 0.0602  \n",
       "4                                 0.0165  \n",
       "..                                   ...  \n",
       "297                               0.0470  \n",
       "298                               0.0170  \n",
       "299                               0.0274  \n",
       "300                               0.0247  \n",
       "301                               0.0283  \n",
       "\n",
       "[302 rows x 15 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Observation Index</th>\n",
       "      <th>ID</th>\n",
       "      <th>True Class (Test)</th>\n",
       "      <th>GB Prediction (Test)</th>\n",
       "      <th>GB Probability (Success, Test)</th>\n",
       "      <th>LGB Prediction (Test)</th>\n",
       "      <th>LGB Probability (Success, Test)</th>\n",
       "      <th>NN Prediction (Test)</th>\n",
       "      <th>NN Probability (Success, Test)</th>\n",
       "      <th>Logistic Prediction (Test)</th>\n",
       "      <th>Logistic Probability (Success, Test)</th>\n",
       "      <th>Ensemble Prediction (Test)</th>\n",
       "      <th>Ensemble Probability (Success, Test)</th>\n",
       "      <th>TabPFN Prediction (Test)</th>\n",
       "      <th>TabPFN Probability (Success, Train)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>732</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0468</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1532</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5561</td>\n",
       "      <td>0</td>\n",
       "      <td>0.4354</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2825</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0389</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0643</td>\n",
       "      <td>0</td>\n",
       "      <td>0.4632</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5796</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2162</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>408</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0426</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0301</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2718</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>784</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0240</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0969</td>\n",
       "      <td>1</td>\n",
       "      <td>0.7133</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2957</td>\n",
       "      <td>0</td>\n",
       "      <td>0.3216</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>135</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0166</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0375</td>\n",
       "      <td>0</td>\n",
       "      <td>0.4053</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0418</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1784</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>297</td>\n",
       "      <td>188</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0457</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0433</td>\n",
       "      <td>0</td>\n",
       "      <td>0.3247</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1560</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1566</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>298</td>\n",
       "      <td>882</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0073</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0281</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5199</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1721</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2186</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>299</td>\n",
       "      <td>1318</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0090</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0101</td>\n",
       "      <td>0</td>\n",
       "      <td>0.3994</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2247</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1655</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>300</td>\n",
       "      <td>335</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0113</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0528</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2448</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1818</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1172</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>301</td>\n",
       "      <td>1045</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0150</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0290</td>\n",
       "      <td>0</td>\n",
       "      <td>0.3756</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2302</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1635</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0283</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>302 rows  15 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Visualizations"
   ],
   "id": "2951ca6881aa42e5"
  },
  {
   "cell_type": "code",
   "id": "f6f23d504b67e04c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T19:42:45.415772Z",
     "start_time": "2025-01-16T19:42:44.412586Z"
    }
   },
   "source": [
    "def probability_distribution_plot(df, model_name):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(df[df[\"True Class (Test)\"] == 0][f\"{model_name} Probability (Success, Test)\"], color=\"red\", label=\"No-Success\", kde=True)\n",
    "    sns.histplot(df[df[\"True Class (Test)\"] == 1][f\"{model_name} Probability (Success, Test)\"], color=\"green\", label=\"Success\", kde=True)\n",
    "    plt.title(f\"{model_name} Probability Distribution\")\n",
    "    plt.xlabel(\"Probability\")\n",
    "    plt.ylabel(\"Density\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def calibration_curve_plot(model, X, y, model_name, is_nn=False, is_ensemble=False):\n",
    "    if is_nn:\n",
    "        probs = model.predict(X).ravel()\n",
    "    elif is_ensemble:\n",
    "        probs = model\n",
    "    else:\n",
    "        probs = model.predict_proba(X)[:, 1]\n",
    "    prob_true, prob_pred = calibration_curve(y, probs, n_bins=10)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(prob_pred, prob_true, \"s-\", label=f\"{model_name} Calibration\")\n",
    "    plt.plot([0, 1], [0, 1], \"k--\", label=\"Perfect Calibration\")\n",
    "    plt.title(f\"Calibration Curve: {model_name}\")\n",
    "    plt.xlabel(\"Mean Predicted Probability\")\n",
    "    plt.ylabel(\"Fraction of Positives\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Violin Plot\n",
    "def violin_plot(df, model_name):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.violinplot(x=\"True Class (Test)\", y=f\"{model_name} Probability (Success, Test)\", data=df, palette=\"muted\")\n",
    "    plt.title(f\"{model_name} Violin Plot\")\n",
    "    plt.xlabel(\"True Class\")\n",
    "    plt.ylabel(\"Probability\")\n",
    "    plt.show()\n",
    "\n",
    "# Gradient Boosting\n",
    "probability_distribution_plot(test_results, \"GB\")\n",
    "calibration_curve_plot(best_gb_model, X_test_scaled, y_test, \"Gradient Boosting\")\n",
    "violin_plot(test_results, \"GB\")\n",
    "\n",
    "# LightGBM\n",
    "probability_distribution_plot(test_results, \"LGB\")\n",
    "calibration_curve_plot(best_lgb_model, X_test_scaled, y_test, \"LightGBM\")\n",
    "violin_plot(test_results, \"LGB\")\n",
    "\n",
    "# Neural Network\n",
    "probability_distribution_plot(test_results, \"NN\")\n",
    "calibration_curve_plot(nn_model, X_test_scaled, y_test, \"Neural Network\", is_nn=True)\n",
    "violin_plot(test_results, \"NN\")\n",
    "\n",
    "# Weighted Ensemble\n",
    "probability_distribution_plot(test_results, \"Ensemble\")\n",
    "calibration_curve_plot(ensemble_test_probabilities, X_test_scaled, y_test, \"Weighted Ensemble\", is_ensemble=True)\n",
    "violin_plot(test_results, \"Ensemble\")\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0cAAAIhCAYAAACFYMFwAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAACsbElEQVR4nOzdd3gU5d7G8e/M1vRACL1J70UQVPRVEEQQG6LYD4piAbtHRcWuqCg2wF5RsaHYPR712AERBAXpSK8BAmlbZ94/UiSCkIQks0nuz3Xlyu7M7Myd3clufnmeeR7Dtm0bERERERGRGs50OoCIiIiIiEgsUHEkIiIiIiKCiiMRERERERFAxZGIiIiIiAig4khERERERARQcSQiIiIiIgKoOBIREREREQFUHImIiIiIiAAqjkREpBqrCvOcx0LGWMggIhILVByJiFSw888/nw4dOvD777/vc32/fv24+eaby7z9361fv562bdsW+2rXrh3du3dn6NChvPvuuwf3AxWYPXs2bdu2Zfbs2Qe9r/PPP5/zzz9/v9s8+eSTtG3btuj+ns9D4c/83nvvAbB7925uvPFGfvnll4PKdfPNN+/1PHbr1o2TTjqJSZMmEQgESv1z7Gnu3LmMGjXqgNv9/Wcv7XH+SSgU4v777+ejjz4qWnbzzTfTr1+/g963iEhV5HY6gIhITRCNRhk7dizvvfceXq+33Lffl8svv5xjjz0WyG8ZyMnJ4Z133uHWW28lEolw1llnlWm/TjnjjDM4+uij97mubt26vPXWWzRt2hSAxYsX88EHH3D66acf9HHT09OZNGkSAJZlkZWVxS+//MIzzzzDDz/8wCuvvILP5wPgjjvuKNW+33nnHVauXHnA7fb3sx+MrVu38sorrzB+/PiiZVdccQUXXHBBuR9LRKQqUHEkIlIJkpKSWL58OZMnT+baa68t9+33pWnTpnTr1q3YsiOPPJIlS5bw8ssvV7niqH79+tSvX3+f67xe714/a3nZ176POeYYunbtyujRo3nxxRe5/PLLAWjVqlWFZNjfz17eCgtMEZGaSN3qREQqQfv27Tn11FN5/vnnWbhwYblvX1KmadK+fXs2btwI/NUd7aWXXuKEE06ga9euTJ8+HYDff/+dkSNH0rt3bw499FAuu+wyli9fvtc+V6xYwTnnnEPnzp0ZMGAAU6dOLbZ+x44d3HXXXfTt25dOnTrRq1cvRo8ezfr16/fa1+TJkznyyCPp3r07V1xxBevWrSta9/euZXvas1vd7Nmzi1o+LrjgAs4//3xef/112rZty59//lnscR988AHt27dn06ZNpXgW8/Xv359u3brx5ptvFi37e3e3H3/8kTPPPJPu3btz2GGHcfnllxe1FN188828//77bNiwoSj7P70e//Sz7+/52lf3uD2fp/Xr13PccccBMHbs2KJt//64aDTK66+/zkknnUSXLl049thjefjhhwkGg8WONWLECKZPn87AgQPp1KkTp5xyCt99912pn1cRESepOBIRqSS33HILtWrVYuzYsYRCoXLfvqT+/PPPvVoHnnzySS655BIeeugh+vTpw6xZszj77LMBuP/++7n33nvZtGkTZ5111l7dwMaPH0+3bt146qmnOProo7n33nt55ZVXgPzufJdeeik//vgjN9xwAy+88AJjxoxh5syZe3VBmzt3Lp988gm333479957L0uWLOGCCy4gOzu7VD9fx44duf322wG4/fbbueOOOzjppJPw+Xx88MEHxbadMWMGRxxxBA0aNCjVMQr16dOHzZs3s2HDhr3WrVu3jiuuuIJOnTrx1FNPcd999/Hnn38yatQoLMviiiuu4JhjjiE9PZ233nqrqAsk7P167MvBPl9169Yt6i54+eWXF93+u9tvv53x48fTv39/nnrqKc4991xee+01rrjiimIDOSxcuJAXXniBq666ismTJ+NyubjyyivZtWtXifKIiMQCdasTEakkKSkp3H333Vx++eUl6i5X2u3/zrIsIpFI0e0tW7YwdepUlixZwp133lls20GDBhW7PufKK6+kWbNmPPvss7hcLgCOOuooBgwYwBNPPMHjjz9etO2ZZ57JjTfeWLTNli1beOaZZzj//PPZtm0bcXFx3HTTTfTs2ROA3r17s3btWt56661iGVwuFy+++GJR97EWLVpw6qmnMmPGDM4777wS/9yJiYlF3dtatWpVdHvAgAF8+OGHXH311RiGwebNm5k1axYTJkwo8b7/rk6dOgBkZGTQqFGjYut+++03AoEAl156KfXq1QPyu8d99dVX5Obm0rRpU2rXrl2s215ubi6w9+uxLwf7fHm9Xtq3bw/kd6Xr0KHDXtusWLGCd999l+uvv75o4Ig+ffpQt25dbrzxRr777juOOeYYALKysnjvvfeKCu/4+HjOO+88Zs2axcCBAw+YR0QkFqjlSESkEvXr14+TTz6Z559/nkWLFpX79nu69dZb6dixIx07dqRz587079+f9957j8svv5zhw4cX27bwj2TI/wP9999/Z9CgQUWFEUBycjJ9+/bl559/LvbYwYMHF7s/YMAAtm/fzqpVq6hXrx6vvvoqPXr0YP369fz4449MnTqVefPm7dUaduihhxa7rqZ9+/Y0adKEOXPmlOrn/ifDhg1jw4YNRSPYzZgxg4SEBAYMGFDmfRa2nBiGsde6rl274vP5GDZsGPfddx/ff/897dq149prryUxMXG/+93z9fgnFf18AUWv9Yknnlhs+YknnojL5So2UmHt2rWLtUgWZsvLyyu3PCIiFU0tRyIiley2225j5syZjB07tuj6nvLcvtCYMWOKumqZpklSUhKNGzfGNPf+v1h8fHzR7aysLGzbLmoV2VOdOnXIysraa9me0tLSAIq6U3344YdMnDiRTZs2kZqaSvv27fH7/fvc99+lpaWxe/fuA/ykJXP44YfTuHFjZsyYwWGHHcaMGTMYPHhw0UhzZbFlyxaAopahPTVu3JjXXnuNZ599lnfffZdXX32V5ORkzjnnHK655pp9FlSF9nw9/klFP1/w12uYnp5ebLnb7aZWrVrFzoW4uLhi2xT+fJZllVseEZGKppYjEZFKlpKSwp133snSpUuZMmVKuW9fqFGjRnTu3JnOnTvTsWNHmjZtus/C6O+SkpIwDIOMjIy91m3bto3U1NRiy/5+TUnh49LS0vjll1+46aabOP744/nuu++YPXs2L7/88j5HltvXtSnbtm2jdu3aB8xcEoZhcNppp/Hll1+ycOFC/vzzz4Me6vunn36iWbNm+yyOALp06cKkSZOKfu4+ffrw9NNP8/nnnx/UceHAz5dhGESj0WLrC7vtlVRKSkrRfvcUDofZuXMntWrVKtX+RERinYojEREH9O/fnyFDhvDss8+yY8eOct/+YMTHx9OpUyc+++yzYn9cZ2Vl8c0339CjR49i23/zzTfF7n/yySc0aNCAZs2a8euvv2JZFldeeWVRARGNRvnpp5+A4q0Kc+fOLdYSsWDBAjZs2MDhhx9e6p9hz+6Aexo6dCi7d+/mwQcfpGXLlnTt2rXU+y70zTff8PvvvxcNXPF3L7/8Mn379iUUCuH1ejniiCO45557AIpGCyxJsfpPDvR8JSQksHPnzmKjys2dO7fYPv7peSrUq1cvIP813dMnn3xCNBrd61wQEanq1K1ORMQh48aNY9asWftsoSmP7Q/G9ddfz8iRIxk1ahTnnHMO4XCYZ599llAoxOjRo4ttO3XqVBISEujQoQOffPIJ33//PQ899BCGYdClSxcA7r77bk4//XR27drF66+/zpIlS4D8lozC628sy2LUqFFcdtll7Ny5k0ceeYQ2bdpw8sknlzp/UlISkF/ApKSk0K5dOwAaNmzIkUceyQ8//MANN9xQon2FQiHmz58P5F9jtHv3bn755RdeffVVevfu/Y+DHxx++OE8/PDDjB49mvPOOw+Xy8Wbb76J1+ulb9++QP51XBkZGXz77bclus5oTwd6vvr27cvUqVO59dZbGTZsGMuWLeOll14qVhAVPk8zZ87cZ7HYqlUrTjvtNJ544gny8vI47LDDWLx4MZMmTaJ3794VMjGtiIiT1HIkIuKQ1NTUvUaNK8/tD8YRRxzBSy+9RCAQ4LrrrmPcuHHUq1ePt99+mzZt2hTb9t577+Xzzz9n1KhRzJs3j4kTJ3LKKacA+SPT3X777fz6669ccsklPPDAAzRs2LBo2Og9WzL69+9Pz549+fe//83dd99Nr169eOWVV8p0TVDr1q0ZMmQIr7/++l5F0LHHHovL5SrKeCDbtm1j+PDhDB8+nLPOOotrrrmGH3/8kauuuornn38ej8ezz8e1a9eOp59+muzsbK677jrGjBlDZmYmL774Ii1atADyW7IaNWrE6NGjmTFjRql+xgM9X3369OGmm25i7ty5XHLJJXz66adMmjSpWHGUmJjIhRdeyJdffskll1xCOBze6zj33Xcfo0eP5qOPPmLUqFG8/vrrXHDBBTz33HMH1fIlIhKLDHvPSQpERESquYsvvhifz8fkyZOdjiIiIjFG3epERKRGmDx5Mn/++Sc//PADb7zxhtNxREQkBqk4EhGRGuHrr79m7dq13HjjjRx66KFOxxERkRikbnUiIiIiIiJoQAYRERERERFAxZGIiIiIiAig4khERERERARQcSQiIiIiIgKoOBIREREREQFqwFDe27dn4fR4fIYBaWlJMZFFqgadM1JaOmektHTOSGnpnJHSiqVzpjDLgVT74si2cfzFKBRLWaRq0DkjpaVzRkpL54yUls4ZKa2qdM6oW52IiIiIiAgqjkRERERERAAVRyIiIiIiIkANuOZIRERERGou27axrCiWZTkdpcYxDAgEAoTDoQq/5sg0TUzThWEYB7UfFUciIiIiUi1FImF27dpBOBxwOkqNtWOHWWmFqdfrJzm5Nm63p8z7UHEkIiIiItWObdts374Z0zRJSamDy+U+6FYFKT2XyyAardhmI9u2iUYjZGdnsn37ZurWbVzm11rFkYiIiIhUO5FIGNu2SElJx+v1Ox2nxnK7TSKRymg58uFyudixYwuRSBiPx1umvWhABhERERGptgxDf+7WFOXxWutsERERERERQd3qRERERKSGMU0D06y8648sy8ayKni4NikXKo5EREREpMYwTYPaqfEYrsrrQGVHLXZk5pa4QDrqqJ707z+QO++8r9jyTz/9iBdffJZ33/2ozFnmzJnFiy8+y7JlS3G73XTq1JVLLrmcdu3al3mf1YmKIxERERGpMUzTwHCZBN54C3vr1go/nlG3Lv5zhmOaRqlaj7788j+cdNKp9OhxWLllWbJkMTfffD2jR1/DrbfeRSgUZPr0t7nqqst45ZVpNGjQsNyOVVWpOBIRERGRGsfeuhVrw8YKP05Z26caNGjIxIkP8vLL0/B4yj5vz57++9/P6NXrcIYOPaNo2Q03jGXu3F/48ssvOP/8EeVynKpMAzKIiIiIiMSYSy65nG3btvHGG6/+4zZbt25h3LibGTSoHyeeeByPPTaBUCj0j9sbhsmKFSvYuXPHHssMHntsMqecchoAL7zwDGPGjCr2uGHDTuLTT/O78kUiEZ55ZjKnnDKQgQOP4bbbbmLXrkwA8vLyeOih+xg8+DgGDz6OBx+8j2AwCEBWVhb33DOO448/hlNOOYFHH32IYPCvyXkL99mvXx/GjBnFqlUri4734IP3cuKJxzFgwNHcdNO1bNtWcS1+Ko5ERERERGJMnTrpjBw5ildffZGNGzfstT4cDnPVVZcTCOQxadKz3H33A/z00w9MmfLEP+5zyJBTyMzcwemnn8TNN1/Hu+++yYYN66lfvwHJySklyvX880/z2WcfM3bsHTz99Evs3LmDCRPuB+CBB+7ht98W8MADj/Doo5P5/ff5PPPMlIJ1d5Odnc1TT73A+PEPs3jxH0yc+BAA3377Pz788D3uvvtBpk59i7S0NMaPvwuA6dPf4tdf5zFx4mSef34qubm5PPHExFI9l6Wh4khEREREJAYNG3YWjRs35bHHHt5r3ezZP5GRsZVx4+6hZctW9OhxGNdddxPvv/8Oubm5+9xf8+aH8Oyzr3Dssf2YP38ejz32MMOHn8q4cTcTCAT2+Zg92bbNRx+9z6hRV3D44UdyyCEtuOGGsRxySEt2797NN998xXXX3UiXLt1o27Yd//73LdSv34ANG9bz/fffFmXt0KETN910G5999jHZ2dls3rwRt9tDvXr1adSoMddccyNjxlwHwKZNm/D5fDRo0IBmzZpz6613ct55Iw7qed0fXXMkIiIiIhKDXC4XN9xwM1dccTHfffdNsXWrV/9JkyZNSU5OLlrWuXMXotEoGzas4+mnJ/Pbb78Wrfvvf78H4JBDWnD77fcQiURYuPA3vvzyCz766H3S0upwzTU37DdPZmYmu3btom3bv0a2O+SQFowceSmLFy8iGo0WG/Wua9fu9OjRg2+//RbLsjjttEHF9mdZFuvXr6N//4FMn/42Z555Mh07duboo49lyJBTADj55NP48sv/cPLJA+nevQf/9399GTx4SOmeyFJQcSQiIiIiEqM6d+7KiSeezOOPP8w551xQtNzr9e21bTRqFX2/+ebbiq73KTRp0mMMHDiY1q3b4Ha76dbtULp1O5SEhAR+/DG/eDKMved/ikajALjd/1w67G9dNBolMTGR55+fute69PR0fD4/b7wxnZ9/nsVPP33PtGlT+eij93nppTdo0aIl7777ET/99AM//fQ9zzwzif/+93MmT35un1kPlrrViYhUA4WTGbpcJm63s1+VObGiiEhNcPnlVxII5PHmm68VLWvatBnr1q1l9+5dRcsWLfoNl8tFo0aNSU+vS+PGTYq+IH+Oo8KBFfaUmJhEamoqAB6Pp1i3vNzc3KIBHJKS8rdbsWJZ0frly5dy2mmDadCgES6Xi+XLlxet+/77b7jggnNo2rQZ2dnZGIZRlCcYDDJ58uOEQmF++ukHPvpoBkceeRQ33DCWl19+g3Xr1rJy5Qo+++xjfvzxO/r1689tt93Fww8/yW+/zS82qER5UsuRiEgVZ5oGtVLiAahVK8HhNKWf7FBExAlG3bqV0kpg1K170PtISUnl8suv5IEH7qV+/QYAHHZYbxo2bMQ999zOZZddya5dmTz66AQGDDiBpKSkfe7nX/+6mDvvvAWv18vxxw/C43Hz228LeOONV7n11jsAaNeuA88//zRff/0lrVq15sUXn8U0XUX7GDbsLJ5//mnS0+uSmlqLxx9/hI4dO5OYmMgJJ5zI449P4IYbxmKaJs88M4U+ffrQvPkh9O59JHfddRvXXvtvTNPFgw/eS3JyMklJSViWxeTJj1G7dhpt2rTlyy//g9/vp0mTpixevJCnnnqJlJRUGjZsxH//+xl169YjJSX1oJ/XfVFxJCJSxRVOaMj06eSt3wgO1iRlnexQRKSyWJaNHbXwnzO80o5pR62Dfk888cRT+OSTD9m2bRuQfz3SAw9M5NFHH2LUqH8RH5/A8cefwKhRo/9xH/369cfr9TBt2mvMmPEu4XCEli1bMXbs7Rx11DEA9OzZi+HDz+Ghh+7D5TIZPvxcMjK2Fe3jvPNGkJWVxe2330wkEuHII4/mmmv+DcDVV1/PY489zLXXjsbj8dCv3wAuvTQ/z7hxd/Poow9x9dVX4HK56N37CK69Nv9xRx31f4wceRlPPjmRHTu207Rpc8aPf4Tk5GSGDj2TrVu3cs89t5OVtZu2bdvzwAOP4HK5qAiGbdvV+tMrIyMLp39Cw4A6dZJiIotUDTpnpDTcbjO/xeiZZ8hdudrRc8Zs1JC4a65k584cIhHLuSByQHqfkdKqaudMOBxi+/ZNpKU1wOPxFltnmkaldgG2LLvG/sPI7TYr7fNgf6954fl7IGo5EhEREZEapSYXK7J/GpBBREREREQEFUciIiIiIiKAiiMRERERERFAxZGIiIiIiAig4khERERERARQcSQiIiIiIgKoOBIREREREQE0z5GIiIiI1DCaBFb+iYojEREREakxTNMgtVY8LrPyOlBFLYvMnbklLpAikQivvPICn3/+KRkZW6lVqzZ9+x7HyJGXEh+fUMFpazYVRyIiIiJSY5imgcs0eePXt9iavbXCj1c3sS7ndB+OaRolLo6eeuoJ5syZzU033UqjRo3ZsGE9jz/+MOvWreOhhx6t4MQ1m4ojEREREalxtmZvZcPujU7H2KdPP/2YsWNvp2fPXgA0aNCQG264hdGjLyYjI4M6deo4nLD60oAMIiIiIiIxxDQN5s2bg2VZRcs6derM1Klvk5qayrBhJ/Hppx8VrZs37xeOOqpn0f3169dx3XVXMmDA0QwdeiLvvPNm0brFixdx+eUjOe64Ppx11lC+/PI/ResWLPiVkSPPp1+/PlxwwXC++earonWbN2/m2mtHM2DA0QwZMoBHH32ISCQCwPLly7jssos47rg+nHrqIF566bkKeV4qQ8wUR6NGjeLmm28uuv/HH39wxhln0LVrV04//XQWLlzoYDoRERERkcpxxhln8+67bzFs2Ek8/PB4vvnmK4LBIIcc0gK3e/8dv4LBINdeO4b4+DieeeZlrrvuJp59djI//vg9O3fu4NprR9O6dRteeul1LrjgQu67706WL1/G9u0Z3HjjNQwePIRXX32Tc8/9F/fddxcLFvwKwGOPPURcXDwvvfQG48c/zDfffMWHH74PwL333kHr1m2ZOvVtbr55HK+//gozZ/5Q4c9TRYiJbnWffPIJ3377LaeddhoAubm5jBo1ipNOOokHHniAadOmcemll/Lf//6X+Ph4h9OKiIiIiFScESMupmHDRrz//jt8+OH7zJgxnfj4BK6++npOPPHk/T52zpxZZGbu5JZb7iA+PoEWLVpyzTX/xjRNvvzyC5KSUoruN23anN27dxEMBnnvvXfo2bMXp58+HIDGjZuwbNlS3n77Dbp27c6mTZto27Yd9es3oHHjJkyY8DhJSckAbN68kaOPPob69RvQsGEjHntsCg0aNKzw56kiOF4cZWZm8tBDD9G5c+eiZZ9++ik+n48bb7wRwzC49dZb+e677/j8888ZOnSog2lFRERERCre8ccP4vjjB7FrVyazZ89i+vS3eOCBe2jZsvV+H7d27RqaNGlabFS7woLqkUcepE2bNph7jNR31lnnAfDmm1P58cfvGTDg6KJ1kUiEJk2aAnDuuRdw//138d13/6N37yM57rjjadOmHQDnn38hzzwzmQ8+eI8jjzyKgQMHk5ZWNa+Lcrw4evDBBznllFPYuvWv0UIWLFhAjx49MIz88ecNw+DQQw9l/vz5Ko5EREREpNpasWI5n332MVdeeS0AKSmpHH/8CfTtexzDh5/KvHlziv5GLhSNRotu76/b3f7WRaNRjj9+EBdccNE+H3P88YPo0eMwvv/+G3766QfGjbuJc8/9F6NGXcF5542gX78BfPfd//jxx++5+urLufHGWznppFNL98PHAEeLo5kzZ/LLL7/w0UcfceeddxYt37ZtG61atSq2bVpaGsuXLy/1MYzKm9/rgBliIYtUDTpn5GA4et7scWydv7FN7zNSWlXtnKkqOf8uGo3y1luvM3DgoKKWGQCPx4Pf7yc1tRZut5vc3JyidRs3bii63bhxUzZsWEcgEMDv9wMwadJjRCJhmjRpxsyZP2DbdlGBdfvtY2nXrj1NmjRj4cLfaNy4SdG+pk17jXA4xAUXXMQzz0ymX78BnHrqME49dRhTp77M559/zL/+NZKnnnqSc8+9gLPOOo+zzjqPCRPu55tvvnasODKMvV//kp4PjhVHwWCQO+64g9tvv73ohSuUl5eH1+sttszr9RIKhUp9nLS0pIPKWZ5iKYtUDTpnpLTi433OBvDnv3fXqqVJCqsKvc9IaVWVcyYQCLBjh4nLZeB2/9WNzOXKv10vuS6GWfEVVN3E9GLHPZCOHTvQp89RjB17A1dccSWdO3dlx44MPvnkY0KhEMcd15+5c3/mk08+5LDDepGZmclbb70OgNttcuSRR5KWVoeHH76fESNGsnbtWj74YDr33vsAXbp044UXnubpp5/glFNO57ff5vPDD98yYsRFJCUlMX36Wzz//FMMHnwSixcv4tlnJ3PrrXfgdpusW7eGxx57iBtuuBmXy8Xs2T/Stm07EhLi+P33+Tz22BauuOJKcnNzWbDgV4455tii533P578iWZaBaZrUqpWwV31RUo4VR5MmTaJTp04cffTRe63z+Xx7FUKhUKhMP+T27VnYJZtvq8IYRv4bSSxkkapB54yUhstlFhUjublBR7MYgRBxwM6dOUSj1gG3F+fofUZKq6qdM+FwCMuyiEZtIpG/3o8syyZqWZzddXilZYlaFuFwtMSTwN511wO88soLPPfcM2zduhm/P45evQ5n0qTn8PniuPjiy7nvvjsZMeJcmjZtzsiRl3HHHWMLfk6T8eMfYeLEB7nggnNIS0tj9Oir6d27DwAPPfQYjz/+CG+//SYNGzbijjvupUWL/OuYHnhgIk899SSvv/4qderUZcyYa+jf/wQiEYvrr7+ZRx55gMsvv4RoNMqRR/bhqqtuIBKxuOuu8Uyc+CAXXng+LpeLfv36c8EFI4lELNxus9jzX5GiURvLsti5MwePJ1xsXeH5eyCGbTtzevfr14+MjAxcLhdAUTHk9XoZMmQI4XCYBx54oGj7m266CZ/Px913312q42RkOP8LbBhQp05STGSRqkHnjJSG211QHD3zDLkrVzt6zpiNGhJ3zZXs3JlTaR+GUjZ6n5HSqmrnTDgcYvv2TaSlNcDjKd4jyTQNzEpoNSpkWXaJC6PqpjKLo/295oXn74E41nI0derUoomjAB5++GEAbrjhBubMmcNzzz1X1B/Stm3mzZvHZZdd5lRcEREREakmanKxIvvnWHHUqFGjYvcTEvK7hDRr1oy0tDQeeeQR7rvvPs466yzefPNN8vLyGDRokBNRRURERESkBqicq6NKKTExkWeeeYa5c+cydOhQFixYwLPPPqsJYEVEREREpMI4Ps9RoT2vLwLo0qUL77//vkNpRERERESkponJliMRERERkfLg0Nhj4oDyeK1VHImIiIhItfPXiMjOTnEglafwtXa5yt45Lma61YmISPVR0skOaxqNkCVSeUzTRVxcItnZOwHwen0YRuUN3y35LMsgGq3Y9z3btgmFgmRn7yQuLhHTLPtnkIojEREpN0ZSIlgWyclxTkcBwLYsjIP4kCxvdtRiR2auCiSRSpKcXBugqECSymeaJpZVOfMcxcUlFr3mZaXiSEREyo8/DkyT0LS3iG7Z6mgUs21bfIOOj4ksAEbduvjPGY5pGiqORCqJYRikpKSRlFSLaDRy4AdIuTIMqFUrgZ07cyp84mCXy31QLUaFVByJiEi5s7Zuw9qw0dEMRnp6zGQBXeQr4iTTNDFNr9MxahzDAL/fj8cTrvDiqLzovVpERERERAQVRyIiIiIiIoCKIxEREREREUDFkYiIiIiICKDiSEREREREBFBxJCIiscS2qTJDGomISLWjobxFRMQZoRDuZUtwrVyJkZ2FkZ2NkZMNbjdWel2sevWw6tYn0qo1+P1OpxURkRpAxZGIiFQqc8tm3PN/xb1kMUY4tPcGoRCuDetxbVgPgNfnI9y9B+Eeh0FcXCWnFRGRmkTFkYiIVI5QCO/33+L+dS5GwSIrNZVIx85YdepgJyZhJyRAMIhr6xbMrVtw/fkn5o7teGf9hGfuL4R79CR8RB9wuRz9UUREpHpScSQiIhXOXLsa338+w9y1C4BI2/aEu3XHatwkfwr1v4mk14WOneFYG9fypXhm/YRr61a8s37CtXYNwZNPxU5MquwfQ0REqjkVRyIiUnFsG88P3+GdPRMAKymZ4MBBWM0PKdnjDYNom3ZEW7fFtXQJvi8+x7VxA/5XXyZ40ilYTZpWYHgREalpNFqdiIhUDMvC+8VnRYVRuGt38i4cWfLCaE+GQbRde/LO/xdWnXTM3Bz8b0/DvfC3cg4tIiI1mYojEREpf+Ewvo9m4Pn9N2zDIHj8IEIDBoLXd1C7tWvVJu+c84m064Bh23j/8xmuZUvKKbSIiNR0Ko5ERKR85eXhefYp3MuXYbtcBE8+lUiXruW3f6+X4IknEe7SFcO28X3yEebqP8tv/yIiUmOpOBIRkfJjWXD++biWL8f2egmcfibR1m3L/ziGQaj/QCJt2mFEo/g/eA9z44byP46IiNQoKo5ERKTcuN+aBtOnY7vcBE4bhtW0WcUdzDQJDh5CpPkhGOEw/vfewcjMrLjjiYhItafiSEREyoV7wa+4P/kYgPDZ51TOSHJuN8GTTyNavwFGIIDvkw8hGq3444qISLWk4khERA6aa/WfeL/8Iv/OXXdh9Tys8g7u9RI86RRsnw/Xpo14fvy+8o4tIiLVioojERE5KEZ2Fr5PPsSwbaJHHQ3jxlV6BjslleDAQQB4f56FSwM0iIhIGag4EhGRsrMsfJ9+jJGXR7RuXcIjLwHDcCRKtE07wl27A+D99GPYlelIDhERqbpUHImISJl55szGtXYNtttDcMgp4PE4mid0bL+iSWI9LzzvaBYREal6VByJiEiZmHtc3xM6rj927TSHEwEeD4ETT8Y2TVzz5sKHHzqdSEREqhAVRyIiUnqhIL6PP8SwLCJt2xHp1MXpREXs9HTCPXvl37nqKgiFnA0kIiJVhoojEREpNe8P32HuysRKSiY44ATHrjP6J+HDj8ROS4M1a3D/9z9OxxERkSpCxZGIiJSKuWkj7nlzAQgNHAR+v8OJ9sHrJXz+vwBwff0VxvbtDgcSEZGqQMWRiIiUXDSK94vPMIBIh45Emx/idKJ/ZPXoCSeeiBGN4vvqC7BtpyOJiEiMU3EkIiIl5vllDq5t27D9foLHHud0nP0zDHjiCWyPB9faNbhWLHc6kYiIxDgVRyIiUiJG5k48M38AIHTscRAf73CiEmjRguixfYH866SwLIcDiYhILFNxJCIiB2bbeL/8L0YkQrRpMyIdOzmdqMQifY/D9vkwt2fgWvKH03FERCSGqTgSEZEDcv25CvfqVdimSbD/wJgbnW6/4uMJ9zocAO+PP0A06nAgERGJVSqORERk/6JRvN98DUD40J7YtWs7HKj0wt17YCUkYO7KxP37AqfjiIhIjFJxJCIi++Ve8Cvmju3YcfGEjzjS6Thl4/USPjw/u2fmTxAOOxxIRERikYojERH5Z3l5eH8qGIThqKPBF4NzGpVQpEs3rOQUzJxsPL/OdTqOiIjEIBVHIiLyj7wzf8AIBLDqpBPp3NXpOAfH5SJ85FEAeOb8rNYjERHZi6PF0Zo1axg5ciTdu3fn2GOP5fnnny9ad++999K2bdtiX6+99pqDaUVEahZjx3bcv84DINj3ODCr/v/TIh06YiUnY+Tl4l70u9NxREQkxridOrBlWYwaNYrOnTvz/vvvs2bNGq677jrq1avHSSedxMqVK7n++us57bTTih6TmJjoVFwRkRrH+8P3GLZNpEVLrGbNnY5TPkyTcM9e+L7+Es+cn4l06VYtij4RESkfjn0iZGRk0L59e+68806aN2/OMcccwxFHHMHcufn9wFeuXEmHDh1IT08v+oqLi3MqrohIjWJu2Yx72RJsIHT0MU7HKVeRTl2w4+Iwd2XiWrbU6TgiIhJDHCuO6taty2OPPUZiYiK2bTN37lzmzJlDr169yM7OZsuWLTRv3typeCIiNZrn++8AiLbviJ1e1+E05czrJdy9BwCeObPAth0OJCIiscKxbnV76tevHxs3bqRv374MHDiQhQsXYhgGTz/9NN999x2pqalceOGFxbrYlVQszFNYmCEWskjVoHNGDsbBnjfmurVFE76G+xxVuv3tsa3j5+9+skQO7YFnzmxcW7bgWrsGqzL+GRdLzw16n5HS0zkjpRVL50xJM8REcfTEE0+QkZHBnXfeyfjx4+nYsSOGYdCiRQvOO+885syZw7hx40hMTGTAgAGl2ndaWlIFpS69WMoiVYPOGSmt+Hjfwe3AtuHH7wEwuncnrlH90j3e5wHA7/fAwWY5WPvLEu+D7t3h55/xz/0ZOrSt+Dx+LwC1aiVU/LFKQe8zUlo6Z6S0qtI5ExPFUefOnQEIBoPccMMNzJs3j759+5KamgpAu3btWL16NdOmTSt1cbR9e5bjPSYMI/+kiIUsUjXonJHScLnMoj+4c3ODB7Uvc+UK/OvWYbvdBA47HLuU+zODYfxAIBDGOsgsB+tAWYxuh+KfMwdj1Sry/lyDXa+UhWApGYEQccDOnTlEo1aFHqtEefQ+I6Wkc0ZKK5bOmcIsB+JYcZSRkcH8+fPp379/0bJWrVoRDofJzs6mdu3axbZv0aIFs2bNKvVxbDt2upPHUhapGnTOSFmU+ZyxbTw/5Lcahbv3wEpMgtLua4/tHT93D5DFTk4l2rY97iV/4J43j9AJgys0jhFLz80e9D4jpaVzRkqrKp0zjg3IsH79esaMGcOWLVuKli1cuJDatWszdepURowYUWz7JUuW0KJFi0pOKSJSc7hWrsC1dQu2x0v4sN5Ox6kU4e6HAuBe8gfk5TmcRkREnOZYcdS5c2c6duzILbfcwooVK/j222+ZMGECl112GX379mXOnDm88MILrF27ljfeeIMZM2Zw0UUXORVXRKR6s208M38ECgqG+HiHA1UOq2EjonXrYUQiuBf+5nQcERFxmGPFkcvlYsqUKcTFxTF8+HBuvfVWzj//fC644AK6dOnC448/zgcffMCQIUOYOnUqjzzyCN27d3cqrohIteZatRLXls3Ybg/hnr2cjlN5DINIQeuRZ/48sJy/FkhERJzj6IAM9erVY9KkSftc179//2LXI4mISAWpoa1GhSLtOuD99n+Yu3bh+nMV0ZatnI4kIiIOcazlSEREYoNr9Z+4Nm/CdrtrVqtRIY+HSKcuALh/nedwGBERcZKKIxGRmmyPVqNI1+6QEFtz8FSWcLfu2IB79SqMnTudjiMiIg5RcSQiUoOZa9fg2rghv9WohoxQty92ai2ih7QECq49EhGRGknFkYhIDeb9OX/+uEjnLtiJiQ6ncVbhwAzuRb9DJOJwGhERcYKKIxGRGsrcvBnXmtXYhlEzrzX6m2jzQ7CSkjECAVwrljkdR0REHKDiSESkhvLMyW81irbrgJ2S6myYWGCaRDp2AsDzu+Y8EhGpiVQciYjUQEbmTlzLlgIQPkytRoUKR60z16zG2JXpbBgREal0Ko5ERGogz5yfMWybSPMWWHXrOR0nZtipqUSbNsMA3At/dzqOiIhUMhVHIiI1TU5O/qADQLhXzR2h7p+EOxfMebTwd7Ash9OIiEhlUnEkIlLDeH6dixGJEK3fAKtJU6fjxJxoqzbYPh9m1m5ca9c4HUdERCqRiiMRkZokFMTz61wAwr0OB8NwOFAM8niItO8IgPv3BQ6HERGRyqTiSESkBnH/tgAjGMSqVZtoq9ZOx4lZkYKuda4VyyEvz+E0IiJSWVQciYjUFNEonl/mAAUj1Jn6CPgnVr36ROvWw4hGcS9e5HQcERGpJPpkFBGpIdyLF2FmZ2ElJBDp0MnpODGvcM4j9+I/HE4iIiKVRcWRiEhNYNt45vwMQOTQw8DtdjhQ7Iu064BtGLg2bcTYscPpOCIiUglUHImI1ACuVSswt2dge32Eu3VzOk7VkJBAtNkhAOpaJyJSQ6g4EhGpATw/zwYg3LUb+PzOhqlCIh0KRq37YxHYtsNpRESkoqk4EhGp5swN63FtWI/tchHp0dPpOFVKtFVrbI8Hc1cm5qaNTscREZEKpuJIRKSa8/xScK1R+47YiUkOp6livF4irdsA4F600OEwIiJS0VQciYhUY0bmTlzLlwEQ7nmYw2mqpmjByH7upYshGnU4jYiIVCQVRyIi1Zhn7i8YQKR5C+w66U7HqZKiTZthJSRgBAK4/lzldBwREalAKo5ERKqrQAD3wt8AtRodFNMk2q4DAO4/1LVORKQ6U3EkIlJNeX6bjxEOY9VJx2rW3Ok4VVrhqHWulSsgGHQ4jYiIVBQVRyIi1VE0inveXADCPQ4Dw3A4UNVm1a2HVas2RjSKa+Vyp+OIiEgFUXEkIlINuZYtwczOwopPINK+g9Nxqj7DINK2HQDuJYsdDiMiIhVFxZGISHVj238N3939UHC7HQ5UPUQKrjtyrf4TAgGH04iISEVQcSQiUs2Y69fh2rIF2+0m3LW703GqDbtOHaw66RiWhXv5UqfjiIhIBVBxJCJSzXjmzgEg0rETxMc7nKZ6ibRrD4Br6RKHk4iISEVQcSQiUo0YO3fgWpE/YED4UA3fXd4ibQuKozWrITfX2TAiIlLuVByJiFQj7sJJX1u0xE5LczpOtWPXqkW0Xn0M28a9TF3rRESqGxVHIiLVRU4O7oW/A5r0tSJFC1qP3Es1ap2ISHWj4khEpLr4/nuMcJhoel2sJs2cTlNtFQ7pba5bi5Gd7XAaEREpTyqORESqg1AI/vc/ACI9NelrRbJTUog2bIRB/nxSIiJSfag4EhGpDt5+G3btwkpILJqPRypOpE1bANzLlzmcREREypOKIxGRqs62YeJEoGDSV5fL4UDVX7R1GyB/TimNWiciUn2oOBIRqeLcP34Pv/4KHg+Rbpr0tTLYKal/jVpXMHS6iIhUfSqORESqON+USfk3jjgC4uKcDVODFLYeuTSkt4hItaHiSESkCnOtXI7380/z7xx3nLNhapjC645ca1dDIOBsGBERKRcqjkREqrC4Z6bk3zjpJKhXz9kwNYxdOw0rrQ6GZeFatcLpOCIiUg5UHImIVFHGju3433oj/8711zsbpoaKFHSt06h1IiLVg4ojEZEqKu6VFzHy8oh07Qb/939Ox6mRooVd6/5clT/XlIiIVGmOFkdr1qxh5MiRdO/enWOPPZbnn3++aN26desYMWIE3bp1Y/Dgwfzwww8OJhURiTHBIP4Xns2/ecWVmvTVIVZ6XayUFIxIBNfqVU7HERGRg+RYcWRZFqNGjaJWrVq8//773HXXXTz11FN89NFH2LbN6NGjqVOnDtOnT+eUU05hzJgxbNy40am4IiIxxff+u7i2biHaoCGhU05zOk7NZRhEWhdMCLtMXetERKo6t1MHzsjIoH379tx5550kJibSvHlzjjjiCObOnUudOnVYt24db775JvHx8bRs2ZKZM2cyffp0rrzySqcii4jEBtsm/unJAOSNvBS8XocD1WzRNm3hl5/zB2WIRMDt2EeriIgcJMfewevWrctjjz0GgG3bzJs3jzlz5nDHHXewYMECOnToQHx8fNH2PXr0YP78+aU+Tiz0NCnMEAtZpGrQOSP74/n+W9x/LMSOjyf4rxF7dQFw9LzZ49iOn7+VlMVu2BArMREzOxvXujVYLVo6mqek9D4jpaVzRkorls6ZkmaIiX9v9evXj40bN9K3b18GDhzI/fffT926dYttk5aWxubNm0u977S0pPKKedBiKYtUDTpnZJ9eeBoA46KLSGvVtNiq+HifE4n+4vMA4Pd7oCZlad8e5szBv3I5dOqw7238+S18tWolVGyWUtL7jJSWzhkprap0zsREcfTEE0+QkZHBnXfeyfjx48nLy8P7t24iXq+XUBlGAtq+PQvbLq+kZWMY+SdFLGSRqkHnjPwT17Kl1Pr0U2zDYOf5I7EysnC5zKI/uHNzg47mM4Nh/EAgEMaqQVnMQ1rinzMHe+lS8rLzwNz7kl4jECIO2Lkzh2jUqtA8JaH3GSktnTNSWrF0zhRmOZCYKI46d+4MQDAY5IYbbuD0008nLy+v2DahUAi/31/qfds2jr8YhWIpi1QNOmfk7/wF1xqFBg0hekhL2Mf54eg5s8exHT93KzFLtHFT7Lg4jLw8jHVrsZo232sbI5aemz3ofUZKS+eMlFZVOmccG60uIyODL7/8stiyVq1aEQ6HSU9PJyMjY6/t/97VTkSkJjEyMvC/PQ2A3MvGOJxGijFNIi1bAxq1TkSkKnOsOFq/fj1jxoxhy5YtRcsWLlxI7dq16dGjB4sWLSIQCBStmzt3Ll27dnUiqohITIh7+XmMYJBw90OJ9D7c6TjyN0UTwi5fVnX+RSoiIsU4Vhx17tyZjh07csstt7BixQq+/fZbJkyYwGWXXUavXr1o0KABY8eOZfny5Tz77LP89ttvDBs2zKm4IiLOCgSIe/E5APIuGxMbQ/9IMdGmzbC9PsycbMxNmpdPRKQqcqw4crlcTJkyhbi4OIYPH86tt97K+eefzwUXXFC0btu2bQwdOpQPP/yQyZMn07BhQ6fiiog4yv/eO5gZ24g2akxwyClOx5F9cbuJtswfxtu9bKnDYUREpCwcHZChXr16TJo0aZ/rmjVrxmuvvVbJiUREYpBtE/d0/ntl3sWXgcfjcCD5J5HWbXAv/gPX8qVwTF+18ImIVDGOtRyJiEjJeL75GveSxVgJiQTOu8DpOLIf0eYtsN1uzF27MLduOfADREQkpqg4EhGJcfFPPQlA4NzzsVNSnQ0j++f1Ej2kBVAwMIOIiFQpKo5ERGKYa/EfeL/5Gts0ybvkcqfjSAlEW7UBwL1iucNJRESktFQciYjEsLhnCiZ9PfFkrGbNnQ0jJRJp0RLbMDAztmFk7nQ6joiIlIKKIxGRGGVs2YL/3bcAyL1stMNppMTi4rCaNAXApdYjEZEqRcWRiEiMinvhGYxQiHCPw4gc1tvpOFIKkVatAXCv0HVHIiJViYojEZEYZGRnEffS8wDkXnmtw2mktKIt84sjc8MGyM11OI2IiJSUiiMRkRjkf+0VzF2ZRFq2InTCYKfjSCnZKSlE69bDsG3cK1c4HUdEREpIxZGISKwJh4l7ZgoAeVdcBabeqquiaEHXOpe61omIVBn6xBURiTG+GdNxbViPlV6XwBlnOR1HyijSOn9Ib9ea1RAKORtGRERKRMWRiEgssW3iJz0OQN4ll4Hf73AgKSu7TjpWSgpGJIJrzZ9OxxERkRJQcSQiEkM8//sK9+JF2PEJ5I0Y6XQcORiG8VfXuuUa0ltEpCpQcSQiEkPiJxe0Gp0/Aju1lsNp5GBFWuV3rXOvWgHRqMNpRETkQFQciYjECPf8eXi//xbb7Sbv0iucjiPlwGrUGDsuDiMQwPxzldNxRETkAFQciYjEiLjJTwAQPPV0rMZNHE4j5cI0ibRoBYDr998cDiMiIgei4khEJAaYq//E99EMAHJHX+1sGClX0dYFE8L+/jvYtsNpRERkf1QciYjEgPinJ2FYFqF+/Yl27OR0HClH0WaHYLvdmDt3wIIFTscREZH9UHEkIuIwY/t2/NNeAyB3zDXOhpHy5/EQbX5I/u0ZMxyNIiIi+6fiSETEYXEvPouRl0e4a3fCfY52Oo5UgMIhvfngA2eDiIjIfqk4EhFxUm4ucS88A0DemKvBMBwOJBUh0qIVtmHA/PmYa9c4HUdERP6BiiMREQfFTX0Jc8cOos2aEzzxZKfjSEWJj8dq0RIAz6cfOxxGRET+iYojERGnBINFw3fnXnUduN0OB5KKFO3cGVBxJCISy1QciYg4xP/m67g2byLasBGBM892Oo5UMKtTFwDcP/2IsWO7w2lERGRfVByJiDghHCb+yUcByBt9Ffh8DgeSimanpUHXrhiWhfeLz52OIyIi+6DiSETEAb7pb+NauwarTjp55/7L6ThSWU45BQDf5586HERERPZFxZGISGWLRol/YiIAuZdfCfHxDgeSSnPqqQB4//cl5OY6m0VERPai4khEpJL5Pv4A94rlWKmpBC4c6XQcqUzduhFt3AQjLw/vd984nUZERP5GxZGISGWyLOInTgAgb9QV2IlJDgeSSmUYhAcPAcD7mUatExGJNSqOREQqkfeLz3EvXoSVmETexZc6HUccED4xvzjy/edTiEQcTiMiIntScSQiUllsm/hHHwIgcNEl2Km1HA4kTogc0QcrNRVzxw48c2Y7HUdERPag4khEpJJ4vv0fnl/nYcfFkXvpaKfjiFPcbkIDTgDA+9knDocREZE9qTgSEakk8Y8WXGt0wYXY6ekOpxEnBQcVdK377GOwbYfTiIhIIRVHIiKVwDPzR7wzf8T2esm74iqn44jDQn2Pw/b7ca1ZjWvxH07HERGRAiqOREQqQWGrUeCs87AaNHQ4jTguIYHQMX2BgtYjERGJCSqOREQqmPvXuXi/+Rrb5SL3ymucjiMxIjSocEhvXXckIhIrVByJiFSw+EcfBiA4bDhWs+bOhpGYETx+ELZp4vltPuaG9U7HERERVByJiFQo16KF+D7/BNswyL36eqfjSAyx69QhclhvALyfq/VIRCQWqDgSEalA8Y8XtBqdfBrRVq0dTiOxpmjUuk9VHImIxAIVRyIiFcS1Yjm+D94HIPeaGxxOI7EoeMJgADw/fY+RudPhNCIiouJIRKSCxD8xEcO2CZ4wmGjHTk7HkRhktWhJpH0HjGgU73//43QcEZEaz9HiaMuWLVx11VX06tWLo48+mvHjxxMMBgG49957adu2bbGv1157zcm4IiIlZq5dg++dNwG1Gsn+BQedCIBPo9aJiDjO7dSBbdvmqquuIjk5mddff51du3Zxyy23YJomN910EytXruT666/ntNNOK3pMYmKiU3FFREol/snHMKJRQsf0JXJoT6fjSAwLDRpCwsQJeL/+EgIB8PudjiQiUmM51nK0atUq5s+fz/jx42ndujU9e/bkqquu4uOP8yfDW7lyJR06dCA9Pb3oKy4uzqm4IiIlZm7ehH/aVAByr7vR4TQS6yJduhFt2AgjNwfv9984HUdEpEZzrDhKT0/n+eefp06dOsWWZ2dnk52dzZYtW2jevLkz4UREDkLc5CcwQiFChx9J+Ig+TseRWGcYhAoGZtCEsCIiznKsW11ycjJHH3100X3Lsnjttdc4/PDDWblyJYZh8PTTT/Pdd9+RmprKhRdeWKyLXUkZRnmmLpvCDLGQRaoGnTNVl5GRQdyrLwKQd+0NjryGjp43exzb8fM3lrLAfvOEBg8h7sXn8P3nU3Ksx8Dlqvg4ep+RUtI5I6UVS+dMSTM4Vhz93YQJE/jjjz949913WbRoEYZh0KJFC8477zzmzJnDuHHjSExMZMCAAaXab1paUgUlLr1YyiJVg86ZKmjieMjLg549STnj1Er/RIiP91Xq8fbi8wDg93tAWYrzewGoVSth73UnD4KUFMxt26izYhH0qbwWR73PSGnpnJHSqkrnTEwURxMmTOCVV17h0UcfpU2bNrRu3Zq+ffuSmpoKQLt27Vi9ejXTpk0rdXG0fXsWtl0BoUvBMPJPiljIIlWDzpmqycjcSa0nJ2ECu6+6ntD27Eo5rstlFv3BnZsbrJRj/hMzGMYPBAJhLGUpxgiEiAN27swhGrX2Wp84YCD+d98md9rb5LbtUvF59D4jpaRzRkorls6ZwiwH4nhxdM899zBt2jQmTJjAwIEDATAMo6gwKtSiRQtmzZpV6v3bNo6/GIViKYtUDTpnqpa455/FzM4i0r4jweMHgUOvnaPnzB7HdvzcjaUsgHGAPMFBQ/C/+zbeTz8m5/Z7Kq3VUe8zUlo6Z6S0qtI54+g8R5MmTeLNN99k4sSJnHjiiUXLH3/8cUaMGFFs2yVLltCiRYtKTigiUjJGdhZxz04BIPea68HUHNtSOuG+x2H7fLj/XIVr2VKn44iI1EiOfXqvXLmSKVOmcMkll9CjRw+2bdtW9NW3b1/mzJnDCy+8wNq1a3njjTeYMWMGF110kVNxRUT2y//yi5g7dxJp0ZLgyaUfPEbETkwidPQxAPg++9jhNCIiNZNj3eq++uorotEoTz31FE899VSxdUuXLuXxxx/niSee4PHHH6dRo0Y88sgjdO/e3aG0IiL7kZdH/FNPApB7zQ2VMtKYVE+hQUPwffkF3s8+zj+XRESkUjlWHI0aNYpRo0b94/r+/fvTv3//SkwkIlI2/jdexdy2lWiTpgRPP9PpOFKFBY8fRKJh4Pl1HuamjVgNGjodSUSkRlGneBGRgxEKEf/kYwDkjrkGPB5H40jVZterR6RnLwC8n3/qcBoRkZpHxZGIyEHwvz0N18YNROvVJ3D2eU7HkWogOGgIoOuOREScoOJIRKSsIhHin5gIQN7oq8DvdziQVAehwfmjt3p++A5jV6azYUREahgVRyIiZeSbMR3X6j+x0tLIO/9Cp+NINRFt0YpIm7YYkQjer/7rdBwRkRqlTMXRunXryjuHiEjVYlnEP/YwAHmXjoaEBIcDSXUSKuha5/3sE4eTiIjULGUqjk444QTOOOMMXn75ZbZs2VLemUREYp73k49wL1uKlZxC3kWXOB1HqpngoPyudd4vv4Bg0OE0IiI1R5mKo++//56hQ4fy9ddfc9xxx3HeeefxxhtvsGPHjvLOJyISe2z7r1ajiy/FTk5xOJBUN5FuhxKt3wAzJxvPj985HUdEpMYoU3FUu3Ztzj77bF599VW+/fZbTjzxRL777jv69+/PyJEjef/998nLyyvvrCIiMcH71Rd4fl+AHZ9A3qjLnY4j1ZFpEjphMAC+T9W1TkSkshz0gAzbtm1j27ZtbN68GcuySEhI4O233+bYY4/liy++KI+MIiKxw7aJf+QhAPIuvBi7dprDgaS6KhzS2/v5J2BZDqcREakZ3GV50OLFi/n888/5/PPP2bBhA0ceeSQXXngh/fv3J6HgouQpU6Ywbtw4jj/++HINLCLiJM8P3+GZOwfb5yP3sjFOx5FqLNznaKykZFxbt+Ce90vR5LAiIlJxylQcDR06lJ49ezJixAhOOOEEatWqtdc2PXr00Kh2IlLtFF5rFDjvX9j16jmcRqo1r5dQ/wH435+O77NPVByJiFSCMnWre+CBB3jxxRc5++yzixVGoVCIL7/8EoDevXszfvz48kkpIhID3D/Pxvv9t9geD7mjr3Y6jtQAfw3p/bHDSUREaoYyFUc333wzWVlZey1fvnw511133UGHEhGJRfGPTQAgMPwcrMZNHE4jNUHouAHYHg/uFctxLV/mdBwRkWqvxN3q3njjDe6++24Mw8C2bfr06bPP7Y488shyCyci8nemaWCaRqUf17VgPr4vv8A2TULXXIfbbWJZNpZlV3oWqTnspGTCRx+D9+sv8X72CXmt2zgdSUSkWitxcXTOOefQunVrLMviX//6F0888QQpKX/N7WEYBnFxcbRpozduEakYpmlQOzUew3XQA22W3pMTATDOPpuUHl0AsKMWOzJzVSBJhQoOGoL36y/xffYxeVdd63QcEZFqrVQDMhx22GEAfPXVVzRs2BDDqPz/3opIzWWaBobLJPDGW9hbt1bacY1Nm/C/9x4AgcZNsR97EqNuXfznDMc0DRVHUqFCAwfBv6/BM3cO5pbNWPXqOx1JRKTaKnFxNHbsWG699VYSExOZNGnSfrfVQAwiUpHsrVuxNmystOP5PvkIgEjrtkSjNmzYePCTxImUkFW/AeEePfHM/QXv558S+NdFTkcSEam29PkuIrIfxs6duJb8AUD48CMcTiM1VeGEsL5PPnQ4iYhI9VbilqM9W4PUMiQiNYXn55kYtk3kkJbqziSOCQ05Ge69E8/332Ls2I5dO83pSCIi1VKZWo5ycnJ4+OGHWbVqFZZlceONN9KtWzfOOeccNmzYUN4ZRUQcYezejXvRQkCtRuKsaItWhDt1wYhG8X32idNxRESqrTIVR3feeSfffvsthmHw0Ucf8cUXX3D//fdTp04d7rrrrvLOKCLiCM8vP2NYFtEmTbEaNXY6jtRwwVNOA8D3wXsOJxERqb7KVBx9++23TJgwgUMOOYT//Oc/9O3bl8GDB3PdddcxZ86c8s4oIlL5cnNx/zYfgFBvtRqJ84InnQqQ37Vu+3Znw4iIVFNlKo5s28bj8RAIBJg5cybHHHMMALt27SI+Pr5cA4qIOMEz7xeMSIRo/QZYzZo7HUcEq0VLwp27FnSt+9jpOCIi1VKZiqPDDz+ccePGMWbMGEzTpH///sycOZOxY8fSr1+/8s4oIlK5ggE8v84FINz7CNCcbhIjgiefCqhrnYhIRSlTcXT//ffToUMHvF4vkydPJjExkaVLl3LMMcdw6623lndGEZFK5Zn/K0YwiJVWh2ir1k7HESlS1LXuh+/UtU5EpAKUeCjvPSUlJXHbbbcVWzZixIjyyCMi4qxwGM8v+ddOhnofrlYjiSlWi5aEu3TD89t8fJ9+ROD8EU5HEhGpVspUHIXDYWbMmMHvv/9OJBLBtu1i6zUPkohUVe7fF2Dk5WIlpxBt18HpOCJ7CZ58an5x9MH7Ko5ERMpZmbrV3Xrrrdx3333s3Llzr8JIRKTKikbxzPkZgHCvw8Es01ukSIX6q2vdtxgZGc6GERGpZsrUcvTf//6XyZMn06dPn/LOIyLiGPcfizCzdmMlJBDp1NnpOCL7ZB3SgnDX7ngW/IrvoxkELrzY6UgiItVGmf4tmpSURL169co7i4iIcywLz8+zAAj37AXuMv3vSKRSBE8bBoD/vXccTiIiUr2UqTi6/PLLue+++1i5ciWRSKS8M4mIVDrX8mWYO3dg+/1EunZzOo7IfgVPHYptGHhmz8Rcv87pOCIi1UaZ/jX63HPPsXXrVoYMGbLP9YsXLz6oUCIilcq28cz+CYBw9x7g9TkcSGT/rIaNCB/RB+9PP+Cb8R55Y652OpKISLVQpuLogQceKO8cIiKOcf25CtfWrdgeD+FDezodR6REgqcNyy+O3ntHxZGISDkpU3HUq1cvALKzs1m7di2tWrUiFAqRmJhYruFERCqDZ/ZMACJdu0NcnMNpREomeNIpJI69Ac/C33AtW0q0TVunI4mIVHlluuYoFApx22230atXL4YNG8aWLVu4+eabGTlyJLt27SrvjCIiFcbcsB7XhvXYLhfhnoeV+vEul4nb7eyXy6Uhx2siu3Yaob7HAeDTwAwiIuWiTC1HDz30ECtWrOD999/nrLPOAuDKK69k7Nix3HvvvUyYMKFcQ4qIVJTCEeoiHTphJyaV+HFGUiJYFsnJamkS5wSHnoHvv//B9/675N50KxiG05FERKq0MhVHX3zxBZMnT6Zt27+a8Nu2bcs999zDRRddVG7hREQqkpGRgXvlCmwgfFiv0j3YHwemSWjaW0S3bK2QfCVltm2Lb9DxjmYQZwQHDsaOi8P95yrc8+cR6d7D6UgiIlVamYqjnJwc4vbRL9+yLKLR6EGHEhGpDJ45swGItm6DXTutTPuwtm7D2rCxPGOVmpGe7ujxxUGJiQRPGIz//en43ntHxZGIyEEqU0f1fv36MXHiRLKzs4uWrVu3jnvvvZdjjjmm3MKJiFQUI2s37sWLAAgf1tvhNCJlFxx6JgC+96eD5h4UETkoZSqObr/9dtxuN7179yYvL4/TTz+dAQMGkJyczLhx48o7o4hIufPM/QXDsog2boLVsJHTcUTKLNT3OKy0NFxbt+D99mun44iIVGll6laXmZnJaaedRseOHWnbti1r1qzh6KOPpkWLFqXaz5YtW7jvvvuYNWsWPp+PwYMHc9111+Hz+Vi3bh3jxo1j/vz5NGzYkFtuuYWjjjqqLHFFRIoLBHAvmA9AuNfhzmYROVheL4HThhH//DP43p5G6DhdfyYiUlalKo5mzpzJ+PHjWb58ObZtFy03DIOPPvqIm2++mZ49SzaBom3bXHXVVSQnJ/P666+za9cubrnlFkzT5MYbb2T06NG0adOG6dOn8+WXXzJmzBg+/fRTGjZsWLqfUETkbzwLfsUIh7DqpBM9pHT/1BGJRcHh5+QXR599QvbuXdjJKU5HEhGpkkrcre6HH37g4osvpl27dkydOpVZs2axaNEiZs+ezcsvv0yLFi248MIL+fXXX0u0v1WrVjF//nzGjx9P69at6dmzJ1dddRUff/wxs2bNYt26ddx99920bNmSSy+9lG7dujF9+vQy/6AiIgBEIrjn/gIUXGukoY+lGoh06UakbTuMQADfRx84HUdEpMoqcXE0efJkRowYwUMPPUTPnj1JTU3F5XKRkpJC7969eeihhzjrrLN46qmnSrS/9PR0nn/+eerUqVNseXZ2NgsWLKBDhw7Ex8cXLe/Rowfz588vaVwRkX1yL1qImZuDlZRMpF17p+OIlA/DIHDG2QD43p7mcBgRkaqrxN3qlixZwj333LPfbc4444wSz3OUnJzM0UcfXXTfsixee+01Dj/8cLZt20bdunWLbZ+WlsbmzZtLGrdILPxTuDBDLGSRqkHnzAEYZXxuLKto+O5Iz8Mw3K6DylB00+nX6W/HdzRPjD4vjmeBojwuV5nGQjqgyPCzsO+7E+/MH/FuWIvVrPkBHpHfPT4mnhupEvTZJKUVS+dMSTOUuDgKBAKkpOy/D3OtWrXYsWNHSXdZzIQJE/jjjz949913efnll/F6vcXWe71eQqFQqfebllbyGe8rWixlkapB58y+xfm9EO8r/QP/+AMyd4Lfj/fwXnu9z5SKzwOA3+8pW5byVJAFID5GssTS8xITWQDq1AbLIjl573kCy0WtNnDccfDll6R89B4caPRYywL0PiOlp3NGSqsqnTMlLo5s28Y09//fLsMwig3UUFITJkzglVde4dFHH6VNmzb4fD4yMzOLbRMKhfD7/aXe9/btWZQhUrkyjPyTIhaySNWgc2bfXC6TWrUSyAuEsHODpXuwbeP7/gdcQLj7oYQjNkRKuY89mMEwfiAQCGOVNks5K8wCkBsjWWLpeYmFLACm4cJvmoSmvUV069YKOYarfgO8gPXEkwSTUv7xX6Vm3br4zh4OxMbnpFQN+myS0oqlc6Ywy4GUarS6zz77jMTExH9cn5WVVZrdAXDPPfcwbdo0JkyYwMCBAwGoV68eK1asKLZdRkbGXl3tSsK2cfzFKBRLWaRq0DnzD8rwvJhr1+LavAnb7SbUrUdhj6KDylB00+nX6G/HdzRPjD4vjmeBojzW1m1Y6zdWyCGs2ul4PB7MjG3w8xysRo33mwX0PiOlp3NGSqsqnTMlLo4aNmzIiy++eMDtGjRoUOKDT5o0iTfffJOJEydywgknFC3v2rUrzz77LIFAoKi1aO7cufTo0aPE+xYR2ZPn54JrjTp1gYQEh9OIVBCvl0jbdngW/o779wWE/qk4EhGRfSpxcfT11+U76/bKlSuZMmUKo0aNokePHmzbtq1oXa9evWjQoAFjx47liiuu4H//+x+//fYb48ePL9cMIlIzGNu24V69CtswCPc8zOk4IhUq0rlrfnG0ZAmhvv3BFwPXW4mIVBGl6lZXnr766iui0ShPPfXUXsN/L126lClTpnDrrbcydOhQmjVrxuTJkzUBrIiUiWfuHACirdtgp9ZyOI1IxbIaNsKqnYa5YzvuJYuJdO3mdCQRkSrDseJo1KhRjBo16h/XN2vWjNdee60SE4lItZSTg3vxIgDCPXs5HEakEhgG4S5d8X3zNe7fF6g4EhEphYqZbEFEJEZ45s/DiEaJNmiI1bCR03FEKkWkQyds08S1eRPm1i1OxxERqTJUHIlI9RUO45k/L/+mWo2kJomPJ9qqDQDu339zOIyISNWh4khEqi334kUYeXlYyclEW7dxOo5IpYp06QqA+4+FEA47nEZEpGpQcSQi1ZNt4/klfyCG8KE94QCTWItUN9FmzbGSkzGCQVwrljkdR0SkStBfCyJSLbn+XIW5Yzu210ukc1en44hUPsPIn9cL8Py2wOEwIiJVg4ojEamWCofvjnTuqnlepMaKdO6CbRi41q3FyMhwOo6ISMxTcSQi1Y6xbSuuNavzJ309tKfTcUQcYyclE23VGqBocBIREflnKo5EpNrx/PIzANE27bBTUhxOI+KscLdDgYKBGUJBh9OIiMQ2FUciUq0Y2dm4F/8BQLjnYQ6nEXGe1bQZVu00jFAI96JFTscREYlpKo5EpFpxz5+HYVlEGzXGatDQ6TgizjMMwt26AwVd62zb4UAiIrFLxZGIVB97TvraQ61GIoUiHTthezyY2zMw1611Oo6ISMxScSQi1YZ70UKMQAArJaXoInQRAXx+Ih06ARqYQURkf1QciUj1YNtFw3eHexymSV9F/qZwYAbX8mWQmelsGBGRGKW/HkSkWnCtWom5cwe2z0ekU2en44jEHDs9nWjjJhi2jfvH752OIyISk1QciUi1UDh8d7hLN/Bq0leRfSm8Fs/904+Qk+NwGhGR2KPiSESqPHPLZlzr1mIbBpHuPZyOIxKzoi1bYaXWwsjNhZdecjqOiEjMUXEkIlWeu+Bao2jb9tjJyQ6nEYlhpvnXSI6PPgrRqLN5RERijIojEanSjKws3EsWA5r0VaQkIp06Y8fHw6pVMGOG03FERGKKiiMRqdKKJn1t3BirfgOn44jEPo+HSJ+j8m8/8oizWUREYoyKIxGpukIhPAt+BSDco5fDYUSqjshR/wdeL8yciXvObKfjiIjEDBVHIlJlFU36mlqLaMtWTscRqTqSk+HccwGIm/ykw2FERGKHiiMRqZqKTfraU5O+ipTW9dcD4P30I1zLljocRkQkNuivCRGpklwrl2Nm7sT2+4l01KSvIqXWsSOceiqGbRP/6ASn04iIxAQVRyJSJXl+KWg16tIt/9oJESm9224DwPf+u5irVjocRkTEeSqORKTKMdauxbV+HbZpatJXkYPRoweh/sdjWBbxT0x0Oo2IiONUHIlIleP+9n8ARNu1x05KcjiNSNWWe/2NAPjfnoa5do3DaUREnKXiSESqlnXrcM0vHL5bk76KHKxIz16E/q8vRiRC/JOPOR1HRMRRKo5EpGqZNCl/0tcmTbHq1Xc6jUi1kHvdvwHwT5uKuWmjw2lERJyj4khEqo7sbHjmGQDCPdVqJFJewkceRejwIzFCIY1cJyI1moojEakyfG9MhV27sNLTibbQpK8i5Sl37DgA/K+9opHrRKTGUnEkIlVDNIrv6SkARI45FgzD2Twi1Uz4iD4EjxuAEYmQ8NB9TscREXGEiiMRqRK8n3+Ka/WfULs20Z69nI4jUi3l3HIHAP733sX1+28OpxERqXwqjkSkSoh/elL+jcsuA5/P2TAi1VS0cxcCQ4cBkHD/XQ6nERGpfCqORCTmuef9gmf2TGyPB8aMcTqOSLWWc+Ot2G43vq/+i+enH5yOIyJSqVQciUjMi3tmMgChYWdCgwYOpxGp3qwWLQmc9y8AEu65A2zb4UQiIpVHxZGIxDRz/Tp8H84AIHi5Wo1EKkPu9TdhxyfgmTsH33vvOB1HRKTSqDgSkZgW99zTGNEooaOPJdqps9NxRGoEq159cq+5HoCEu2+HnByHE4mIVA4VRyISs4ys3fhfewWAvMtHO5xGpGbJvWwM0abNcW3aSPwTjzgdR0SkUqg4EpGY5X9jKmbWbiKt2xDqN8DpOCI1i99P9l358x3FT3kSc81qZ/OIiFQCFUciEpsiEeKeexqAvEtHg6m3K5HKFho8hNDRx2IEgyTeeZvTcUREKpz+2hCRmOT97GNca9dg1a5N4IyznI4jUjMZBtn3PoDtcuH75EM8333jdCIRkQoVE8VRKBRiyJAhzJ49u2jZvffeS9u2bYt9vfbaaw6mFJHKFP9U/qSveSMuhrg4h9OI1FzR9h3Iu/BiABL/fQ3k5TkbSESkArmdDhAMBrn++utZvnx5seUrV67k+uuv57TTTitalpiYWNnxRMQB7jmz8fzyM7bXS96FlzgdR6TGy735Nnwff4j7z1XEPzqB3FtudzqSiEiFcLTlaMWKFZx55pmsXbt2r3UrV66kQ4cOpKenF33F6b/HIjVC3DNTAAicfiZ2vXoOpxEROzmF7AfyR6yLn/QYrkULHU4kIlIxHC2Ofv75Z3r37s1bb71VbHl2djZbtmyhefPmzgQTEceYa9fg+/gDoGAgBhGJCaHBQwgOPgkjEiHp+ishGnU6kohIuXO0W90555yzz+UrV67EMAyefvppvvvuO1JTU7nwwguLdbErKcM42JQHrzBDLGSRqqEmnzPxz0zGsCxCx/bF6tiRfT4FRgw8N3scP5aygMN5YvR5cTwLxFaeMmbJefBhPN9/i2feXOJefJbAqMvLP5vErKry2WSaBkash6whYumcKWkGx6852pdVq1ZhGAYtWrTgvPPOY86cOYwbN47ExEQGDCjdXCdpaUkVlLL0YimLVA017pzZsQPemAqA95ax1Kmz758/zu+FeF9lJtubzwOA3++JmSwA8TGSJZael5jIArGVx+8tulmq95k6SfDQg3D55STefzeJw0+Hli0rIKDEslj/bLJsC9OIiTHHhPzXI9bPmT3FZHF06qmn0rdvX1JTUwFo164dq1evZtq0aaUujrZvz8K2KyBkKRhG/htJLGSRqqGmnjNxjz1BQk4OkY6dyezWGzKyiq13uUxq1UogLxDCzg06lDKfGQzjBwKBMFaMZAHIjZEssfS8xEKWWMtjBEIUXsVb6veZoWeT/NobeH/8nvDZ57Drw8/BHZN/Tkg5qwqfTYWfE9Pmv8XW7K1Ox6nx6ibW5exuw8nMzCESsRzNUnj+HkhMvpsZhlFUGBVq0aIFs2bNKvW+bJuY+QWOpSxSNdSocyYYxF8w6WvuFVdiY8A//eyx8LzscfxYygIO54nR58XxLBBTeYy/ZSlVHsMk64mnqHXskXjm/EzcE4+Se+2/yz2jxK6q8Nm0JWsrG3ZvdDqGFKgK50yhmGxzfPzxxxkxYkSxZUuWLKFFixbOBBKRCuef/jaurVuINmhI8NTTnY4jIvthNWlK9vgJAMRPGI97wa8OJxIRKR8xWRz17duXOXPm8MILL7B27VreeOMNZsyYwUUXXeR0NBGpCJZF3JQnAMgbdQV4PAd4gIg4LXjGWQRPOjV/9LorLtHksCJSLcRkcdSlSxcef/xxPvjgA4YMGcLUqVN55JFH6N69u9PRRKQCeL/6AveypVhJyQQuGOF0HBEpCcMga8KjROvVx718GYnjxjqdSETkoMXMNUdLly4tdr9///7079/foTQiUpnipjwJQOD8EdhJyQ6nEZGSsmunkfXk06QMP424V18kfMSRBE8/0+lYIiJlFpMtRyJSc7jnz8P74/fYbjd5l1zmdBwRKaXwsf2KBmRIuv5qXMuWHuARIiKxS8WRiDgq7qn8VqPgacOwGjV2OI2IlEXuv8cSOvoYjNwckkeeDzk5TkcSESkTFUci4hhz7Rp8H84AIPfyK50NIyJl53Kx+6kXiNath3vpEpJuvLbqjNsrIrIHFUci4pi4Z6dgRKOEjulLtFNnp+OIyEGw69Yl69mXsE0T/ztvEvfsFKcjiYiUmoojEXGEkbmTuNdeBSD3iqscTiMi5SF85FHk3HkvAAl33Irn6y8dTiQiUjoqjkTEEf5XXsTIzSHSoRPhY/s5HUdEyknepaPJO/s8DMsiedSFuFYudzqSiEiJqTgSkcoXDBL33NMA5F5xJRiGw4FEpNwYBtkPPUr4sN6Yu3eRfN5wjF2ZTqcSESkRFUciUun877yJa+sWog0aEjz1dKfjiEh58/nY9dLrRBs1xr1yBckXnQ+hkNOpREQOSMWRiFSuSIT4JyYCkHf5GPB6HQ4kIhXBrluXXa++iZWQiPf7b0m6+gqwLKdjiYjsl4ojEalUvo9m4Fr9J1bt2uSdN8LpOCJSgaKdu7D7xanYbjf+6W+TcN9dTkcSEdkvFUciUnlsm/jHHgEg75LLITHR4UAiUtHCfY8ja2L+ZM/xTz6K/4VnHE4kIvLPVByJSKXx/vdz3IsXYSUkkjdylNNxRKSSBM86l5yx4wBIvOVGfO+/63AiEZF9U3EkIpXDtol/9GEAAhdejJ1ay+FAIlKZcq+5gbwRIzFsm6TRo/D+93OnI4mI7EXFkYhUCs9PP+CZOwfb5yP30tFOxxGRymYYZD/wCIGhZ2BEIiSPvADPj987nUpEpBgVRyJSKeIfK2g1Oud87Hr1HE4jIo4wTbKefJrgCYMxAgGSzxuOe94vTqcSESmi4khEKpz717l4v/0ftstF7uirnY4jIk7yeNj97MuEjj4GMyeblDNPw/3rXKdTiYgAKo5EpBLEP54/r1Hw9DOxmjZzOI2IOM7vZ9cr0wj3PgJz9y5SzjhVBZKIxAQVRyJSoVxLl+D79CNswyD3quucjiMisSIxkcxp01UgiUhMUXEkIhUq/on8VqPQ4JOItmnrcBoRiSkqkEQkxqg4EpEKY65Zje+9dwDIvVqtRiKyDyqQRCSGqDgSkQoTP/lxjGiU0LH9iHQ71Ok4IhKrVCCJSIxQcSQiFcLcshn/tNeA/MkfRUT2SwWSiMQAFUciUiHiJj2GEQwSPqw34SP6OB1HRKqCvxdIw07BPWe206lEpAZRcSQi5c7cspm4V14EIOeGm8EwHE4kIrHMNA3cbjP/KzWZ7HfeJ3xkH8ys3aQOPw3/zz/9tb6Cv0xT71ciNZnb6QAiUv3EPfkoRiCQ32p0bD+n44hIDDNNg9qp8RiuPf5fWysBvvgPnHIKxldfkXTmUPjwQzjuuArPY0ctdmTmYll2hR9LRGKPiiMRKVfmls3EvfoSADn/HqtWIxHZL9M0MFwmgTfewt66tfjK4wfh3bAR15LF2IMGEbroYqx27Sssi1G3Lv5zhmOahoojkRpKxZGIlKu4Jyb+1Wp0TF+n44hIFWFv3Yq1YeNeywMnnIgvHMa9cgXe558lePKpRFu2rpAMutZARPQ+ICLlxty86a9WoxtvUauRiBw8t5vgyacRad0WIxrF98H7uJYtdTqViFRTKo5EpNzEPTExf4S6XocT/r9jnY4jItWFy0XwpFOItGuPYVn4PpqBa8lip1OJSDWk4khEyoW5fp2uNRKRimOaBAefRLhDJwzbxvfJh7gXLXQ6lYhUMyqORKRcxE98CCMUItTnaLUaiUjFME1Cg04k3Lkrhm3j/exj3L8vcDqViFQjKo5E5KC5Vi7HP+01AHLG3q5WIxGpOIZB6PgTCHc7FAPw/ecz3PPnOZ1KRKoJFUcictDiH7ofIxolePwJRHr1djqOiFR3hkHouAGEexwGgO/LL3DPneNwKBGpDjSUt4gcFNfC3/G/Px2AnJvHOZxGRErL5XL2/6RlPr5hEDq2H7bLhffnWfj+9xVGNEq41+HlG1BEahQVRyJyUBIeuAeAwGmnE+3U2eE0IlISRlIiWBaYJrVqJTgdJ19ZuuMaBuGjjwGXC+/MH/F+9w1Eo4SP6FPu8USkZlBxJCJl5v55Nr4vPsd2uci98Ran44hISfnjwDRh+nTy1m8E27koZtu2+AYdT5mvVDQMwn2Ozi+QfvgO74/f5xdIR/1fecYUkRpCxZGIlI1tk3jXbQAEzj6vwmasF5EKlJGBvWEjtoPFkZGeXi77CR9+JLbLhe/b/+Gd9ROYJuEjjyqXfYtIzaEBGUSkTLyffIRnzmzs+Hi1GolITIgc1pvgsf0A8P70A55ZPzmcSESqGhVHIlJ64TAJ994BQO5lY7DqN3A4kIhIvkjPXoSOPhYA7w/f4fl5lrOBRKRKUXEkIqXmf/Ul3KtWYtVJJ2/M1U7HEREpJtz7cEIF1xx5v/sG9y8/O5xIRKqKmCiOQqEQQ4YMYfbs2UXL1q1bx4gRI+jWrRuDBw/mhx9+cDChiBQysnaT8MgDAOT8eyx2YpLDiURE9hY+/EhCBaPW+b75Gve8XxxOJCJVgePFUTAY5LrrrmP58uVFy2zbZvTo0dSpU4fp06dzyimnMGbMGDZu3OhgUhEBiJv0GGZGBpGWrQic9y+n44iI/KPwkUcROvxIAHxff4l7/jyHE4lIrHN0tLoVK1Zw/fXXY/9tmJxZs2axbt063nzzTeLj42nZsiUzZ85k+vTpXHnllQ6lFRFz/Trin5oEQM5td4HH43AiEZH9KBzmOxrFO2c2vi+/ANMk0qWb08lEJEY52nL0888/07t3b956661iyxcsWECHDh2Ij48vWtajRw/mz59fyQlFZE8Jd4/DCAQIHXkUocFDnI4jInJghkH4/44l3OMwALxffI5r8SKHQ4lIrHK05eicc87Z5/Jt27ZRt27dYsvS0tLYvHlzqY9Rlgm3y1thhljIIlVDLJ4z7pk/4Z/xHrZpknPvAximg+GMGHhu9jh+LGWBis9jpKZAQsK+19Wulf89PR3TyZlFYywLVFCenBzszF1lCPO3u06ew5Xxu2QYhPv2g2gUz/x5+D79mJDHQ7R1m8rPUoXF4mfTPzFi4XNCisTC61HS48fkJLB5eXl4vd5iy7xeL6FQqNT7SkuLnYvFYymLVA0xc85Eo3DHWACMiy+mVt8+jsaJ83sh3udoBnz5XQr9fk/MZAGIr+gsKSlYo6/A9O7/OL5zhldsjlKIpSxQvnmsUBBz8hTYVcoCqTLPmQOpzN+lk4eAFcH47Td8H30A55wDLVr8td6f/7dHrVr7Lv4lX8x8Nu2H3+8lPuLwuS34C36nUlOrzu9UTBZHPp+PzMzMYstCoRB+v7/U+9q+PcvRmb8hv1JNS0uKiSxSNcTaOeOb+gpJv/6KlZzCzmtvxs7IciSHy2VSq1YCeYEQdm7QkQyFzGAYPxAIhLFiJAtAbgVnMWq5ifP6ePvtu9i2bc3eG6Sl4W7VisjvCyE3p0KzHFAsZamAPOnpzTjzzDvIM92l/n2ozHOmpFkq7XdpwAl48wK4ly/DfvNNgmechdWoEQBGIEQcsHNnDtGoVfFZqphY+2zal8LPiUAg5Pi5LRBw5zdsZGbmEIk4+ztVeP4eSEwWR/Xq1WPFihXFlmVkZOzV1a4kbJuY+QWOpSxSNcTCOWPs3kXC/XcBkHvDTVhpdXC8h1IMPC97PgexlAUqNo9RsO+t29awceOyvdab0fq46ycQ2bISK2t3xQUpgVjKUhF57D1ulPo1r8Rz5oAq+3fJMAmeeDLMmI579Z/4pr9NYPg5WHXrFZ3flZalioqFz6YDqQoZa5Kq9Ho4PpT3vnTt2pVFixYRCASKls2dO5euXbs6mEqkZoqfMD5/6O5Wrcm7aJTTcUREDp7bTfDk04g2aowRDOJ/5y2MHdudTiUiMSAmi6NevXrRoEEDxo4dy/Lly3n22Wf57bffGDZsmNPRRGoU1++/Effc0wBk3/sg/O1aQBGRKsvrJTB0GNF69TDycvG/86YKJBGJzeLI5XIxZcoUtm3bxtChQ/nwww+ZPHkyDRs2dDqaSM1hWSTdeC2GZRE4+TTC/fo7nUhEpHz5/AROH45VOw0zKwvvU5OhDCPjikj1ETPXHC1durTY/WbNmvHaa685lEZE/G9MxTN3DlZCIjn3jHc6johIxYiPJ3DGWfjffA0zIwMGDsT44FNISHY6mYg4IGaKIxGJHcb27STcczsAuTfdgtVArbYiEpssbCJYRLCIYhEpuG8ZNmBjF2xjF9y2sQvu89f9ZBvOPgHfpx/Dxt8IX3QimY88jh3nz3+cXfhoMDAwDKP492LL8rdhn+uKL9t7fyYuw4XLMDENE9N04TJcmEXLXUWPE5GKoeJIRPaScM/tmDt3EunQibyLL3M6johUsggWuYTJMyLkESaPCAEjQi5hQkaUIBFCRAkaUcJECRIlZEQJkf+11/29tosQwiJoRAivn4713ETCWZsIxwWJGNbfCp6C20Z+8RPdY13EKMehgROAiwrvLIAPjy2/fZezPQsms/C2uUdRZbj+to35V4FlujAo3N6FaRh7b2+6MDGKtsl/nBu36SYhzk8kZOM23LhMN56C5S4j/7vbdO1x25P/fY/Hu4vW7ftxHpcHt+HG6/Lidfnw7fFVuMzv8uMyXU6/DFJNqTgSkWI8P3xH3BtTAch66FFwuzFNA9N0/r+VLldMXiYp4gjLttgV2MW2WiZZUZMsO0g2gfzvdoBsO0g2wWL38wiTZ4fJ2/oFwZemkJu5gdykXPLsMLmECBR8j1CJ85EEgI0Ftyvg712zqK0mv5Vm3/cLWnBME8N0YWZlYdhgeL2QlIxp5G+1ZyvSnq1JNuy1fM/v8A/ritYXtGDZJXveo3aUqB0lTLj8n7AqwmW4igomn8tfVEDFe+JJjksiI2c7USuKx/TgMb35312egvuegu39+Nz53/0F992mW61zNZyKIxH5S24uSdeOASBvxEgivXpjmga1U+MxYqkw0QeXVHE2NmEsAkQIGhEC5H/lGdH823ssCxqR/BaXghaYwtaYyPYfufrBV8oWYHfBVwnEueOI98QT54kjzh2H353/B2X+H6UF3/9+/5+W/8N9T0ELg8t07dGisMftMiw3jYN4z/rPf+CkkyAcgsuGwZQpB3zfiVoWmTtzsayDm8zFsi0s28ovgKwoFhaWlV8MWbZd8D26j232eJwdxd7jdv7y/P1YWEStv29jFRz3r/39tY1NxI4U3I/gj3ezKyuHcDRMxI4QsfLX/XU7QtgKE7WjRKyCZXaEiBXNX25FCraN7nE7TMSKFm0btsKEoiGC0SDBaJBQwfc9i8eoHSU3kktuJPegnu+/Mw2zWLHkd/uJd8cT54knwZ1AvCeeeE8C8e4EEjzxxLnjcZv6c7o60aspIkUSHrwP15rVRBs1Jmdc/sSvpmlguEwCb7yFvXWro/nMtm3xDToelUYSawq7oeUa4b99jxTdz9u9hLyfvyEQzCLPEy64JubguTDxGW58hge/4cFX9OUuul/43WO48eLCm5SKv2FjvBs248oL4zZceA03HsOFFzeegtse9nGNiwWEDpQqWPCVtdcaC8gr+AIwaqfhbt2KyO8LsXNzDvLZODhGfDzuzp0JJIZpdeu5DL7rFYynn+bnnFX8dMmQf3xc3cS6nNN9OKZpHHRxVNgFzo27QlrSDoZhQJ06SWRkZDkyoWfEihCIBghFg38VT5EgQauggIoECdp5GN4o7y/6gK05WwlFw4StEGErTMQKE46GCVvhosIrGA0QjAYJRAJFrXd5kVzySlF0+Vw+4t0JJHoTSfImk+xNJsmbTJI3qei+z+WrwGdGypOKIxEBwD3vF+KemQxA9oRHsZOKj9Rkb92KtWHjvh5aaYz0dEePLzWLhU0uYbKMENmEyDZCRbdzjFCx4idkRA+8wyh7VAT530zbwIcLP278trv4d9z47fx1Ptx4bVd+YYMLr+3i0A59OX/4PTz/6vVsXLmgVD+b6Y3HndQQsjcS3plF4d+5kYKvymRG6+NukEBky0qsrBI2Z1UQIykJzyF1yMsLsb5nffJGD2TYpM/pNfULtnrCfDu0t6P5ajq36SbRTARP4j9v4zapVSuBDTs2s2F3yT+zbNsuKJoCBKJBgpH8oikvkpffQhXOITeSQ2648HZ+q5VlW0WF1s7gjn/cv9/lJ9Vfi1RfLVJ9qdTyFdz21yLeHa+ufDFExZGIQChE0rVj8uc0GjacUP+BTicSqVBBIuw2guwuKHqyKSh8jBBZBctyCGGX4u8V0zaIx0287SEez17fE1LSSWzVFs8fK/DlBPHjxlN0xUvpJbsT8Lv9+qOqAs0+oRtx2QFOfPkbhrz4P3IT/cw5vqvTsaQCGIZRMOCDl6QSPsa2bQLRALnhHHLCOWSHs8gKZbE7tJusgq/dod35LVPRAJtzNrE5Z9Ne+/G5fNSJSyfNn0ZaXJ2i2wmeRP1+O0DFkYgQP/Eh3Iv/wKpTh+x7HnA6jshBsbHJI0JWZCdZ2/5gZ+RPMl272GUE2U2QXUaQgFGy9hHDhgS8JNpekgq+J9peEvdR/Phw7bfQMb31cac2J2JuxSrpBT/iuG+GHU58Vh59p89m2KTPyUv0s/DItk7HkhhgGAZx7vxr8dLi6vzjdqFokMxgZv5XYGfB7Z1kBncWFU8bstezIXt9scf5XX7S4+tSN74e9eLrUy++PrX8tQ7uejo5IBVHIjWce+4c4h9/BIDs8Q9jp6U5nEjkwCxssgiy0wjs8ZXHTgJkGgHChpV/ucsfBQ/Yx7UbfttNsu0lER9JtofEwiKoaJmX+IK2HanZPh1xLPFZAXp/sYBzH/qQF+88g+XdmjsdS6oIr8tH3fh61I2vt9e6iBVhZ2AHGXkZbA9ksD0vg4y8DDKDOwlEA6zLWsu6rLVF23tMT0GxVI8GCQ1pmNiYZG+yWpjKkYojkZosJ4ek0aMwolECQ88geMpQpxOJFLGxySFMhpHLdiOPHUZeUSG0iwDRAwxokGD4SU1MIzknSkrYRbLtI8X2kYKPZNuHTx+BUlKGwfTRA/HnBOj641L+de90nrnvbNa11QTZcnDcppv0+Lqkx9cttjxiRdgR2M6W3C1szd3MlpwtbM3bStgK79HKNBeARE8iDRMb0yixEQ0TG1M3rq7mgToI+mQQqcES7x6He9VKog0akv3Aw07HkRrKwiaTANuNXDKMPLYbeWwvKIiC+xnowLQNUvFTy97zK45U208KPrz1GuLu0onIrJ+xAurGJgfHdplMu+Ek4nKDtPl1NRff8TZTHjyXLc00UIyUP7fp3qO1qQuQP8z7jsAOtuZuZnPOZjZmb2Br3hayw9ks27mEZTuXFD22UUJjmiY3o1lyc+rG11NXvFJQcSRSQ3m+/pK4l54HIOuJp7BTazmcSKo7G5ssQmw1cthm5BZ932Hk/WMrkGFDKn7S7DhqF3zVsv2k2n6S8anLm1SqqMfNK7cMZdRtb9Js6UYuGfcWkx86D5LVgiQVzzRM6sTVoU5cHTqkdQIgHA2zOXcTG7LXszF7AxuzNxCIBliTtZo1Wav5fsO3+Fw+miY1o2lyM5omNae2v7a64e2HiiORGsjIyCDp6isAyL34UsLH9HU4kVQ3ITvCFiOLbUYOW4sKoRwC/9AS5LZN0uy4Pb7iqUMctew43Og/nhI7QnFeXrjzDC6/+Q0arNnGqHFv8t6UG5yOJTWUx+WhSVJTmiQ1BfJH0MsIZLBu9xrWZK1mXdY6gtEgyzOXsTxzGQDJ3hRaprakZUprGic10SS2f6NnQ6SmsSySrrwU15bNRFq3Iee2u5xOVCJGagpGrfzWLSM9HdOJGQj3zBODWahTByMQopzmFt33sfYx11SQCJuNHDYb2WzJXsumn//HjtAO8Ozj8Tak2fHUteOpayeQbsdTx44nBV+Zh7QWqWx5SXE8d89wRt/4GnU2ZXLaDVPgmCtwufxOR6sULlfs/sMilrNVBsMwSI9LJz0unUPr9cSyLbbkbM5vSdq9mo3ZG9gd2sWvW+fx69Z5eEwPzZMPoUVqS1qktCRhP3NI1RQqjkRqmLinJuH76r/Yfj+7n3sF4uOdjnRARmoK/n9fi+nNn2Hcd85whxP9JZaycPrpxFXwIXYHdzNr9Td8HVzMUtcKNpvZ7DACf20QLvgCEmwPde0E6trxpBd8T7Pj1RIk1UJW7USevfcsRv97KukrN2IPGULyF19AcoLT0SpcrVqx/zOq21g+0zBpkNiQBokNObzBkYSiIdZmrWFV5gpW7lpJTji7WKtSw4RGtK3djja12pLkTT7A3qsnFUciNYh77hwS7rsTgOx7HiDaoaOzgUrISEjA9Pp45z+PsyPNT+T3hdi5Oc5mqp2Gu3WrmMrCH38QycqivBqOLNtiYzST1eGt/BnZxurINrZGd/21wR6DISXbXurbiTSIb0Dj1t1IX7qJuOxgOSURiU076qfy3D3DGTP2LXw//cTGgX149+5/YXmq559XhgF+v5dAIITDDeb/qG16Wwa1Ox41RO+b1+WlVWprWqW2xrZttuZuYeWuFazMXMGW3M1szNnAxpwN/G/dVzRMaESbWm1pU7sdyTWoUKqev70ishdjVybJl16EEYkQOPk0Ahdc6HSkUtu6Yz3bfAlEtqzEynJ29DEzWh93g9jKwtaVhHdmlrk4yiXMRiOLDWYWG4wsNhlZhAxrr+2aJDehdthDSlYkvyCyE4kv6ENnxtXHXbsVEWMHFiqOpPrb3Lwu3zxxIwNHP0jDHxfwf3c+xxs3nIRdDbt3GQbER3zk5gZjtjhKT9DogSVlGAb1EupTL6E+RzY8iqxQFst3LmXpziX5AzwUFErfrP+aBgkNaZ/WkXa12hPvif0eJwdDxZFITWBZJF11Ba61a4g2bU72xCfyP+WkxrKx2UEe68zdrDN2s8HMYuee3eMKeG2ThnYSDe0kGllJDGh/IpecO4HJL41hw675lR9cJAZldGkD779P9MTBdPt+McE4D9PHDMI29T4rVUeSN4lD6/Xk0Ho9yQ5lsWznUpbtXML67PVsytnIppyNfLPuKw5JaUHHtE60SGlVLQdzqH4/kYjsJf7xR/B99jG218vu517CTk5xOpJUMhubbUYu64zdrDV2sc7cTY4R3mu72nYcjawkGtn5X3Xs+GLDZSe7Y/9aAxFHHH88n9/xLwbd8RK9v/gNyzR5b/RA/SNKqqTEvxVKS3cu4Y/ti9iSu5mVmfnd8PwuP21rt6djWicaJDSsNtd5qTgSqeY8X/+X+AfuBSD7wYlEuvdwOJFUBgubLUYO64xdrC1oHQoYkWLbuGyDhnYSTexkGln5rUPx+xpiTkRKZEXf7ryZuY2zJn7EEZ/Px3KZzLhsgAokqdISvUn0qHcYPeodRkbeNv7Yvog/ti8iO5zFgm2/smDbr9T2p9E1vRsd0joR567ooYEqloojkWrMXP0nyZeNxLBt8s6/kMC5FzgdSSqIbdtst7JZbm7iTzOTtcYugn+bU8hjmzSyk2lqJdPETqahnaSR40TK2a99O2JaFmc+9gl9PpmH5TL58JLjVCBJtVAnLp3/a3wsRzX6P9ZlrWXR9t9ZtnMZOwLb+d+6r/hu/Te0rdWOLundaJTY2Om4ZaLiSKS6ys4mZcS5mJmZhHv0JPv+h5xOJOUsmxCrzUzW5Kznz1lfkBXKKvau7rNdNC4qhlKobyfgUjEkUuHmHtcZM2px5hOfcfSHv2CZBh+P7KcCSaoN0zBpltycZsnNOa5JgMU7/mDBtvlsy9vKHzsW8ceORaT56/B/Tf6Py6OXOh23VFQciVRH0SjJV1yM+4+FWHXS2f3CVPD5nE4lBylElLXGLlabmaw2drHNzC1cAYALkyZWEs2tVJrbKdSzE4tdLyQilWfO8V0xLZthkz7nmBlzsFwmn444VgWSVDs+t59udQ+la3p3Nuds4reM+SzesZjtgQzeX/4ej858lFEdxzgds8RUHInEsLLO9B139zh8n3+K7fOR/dqbmE2blLm9oKbPNu6kwhHlVpo7WWlmss7YRdSw99yA+nYCh8Q1olXb3jRZsQ0yy2+eIxE5OLNP6IZpWQyd8gV9p8/Gcpl8fv7/qUCSaskwjKIJZ49tchyLty9iW2ArA1sNdDpaqag4EolBpmmAZZVtFvLnn4dJjwNgvPQSyQP7lU8ofZhXijBR1hi7WGnuZJWZSebfhtdOsX0cYqXS3EqlmZ1CPB7M+Pq4a7UAYwd7jz8nIk6aOfhQzKjFqc98yXFvzwRQgSTVns/lo1vdQ2mc0pBu9buxc6ezk6WXhoojkRhkGAaYJsFpb2Ft2Vrix5nLl+F9egoGEB54ApEtGfDYkweVxWzbFt+g49U5qwLtJMBKcwcrCwZSiOwx8appGzS1k2lp1aKlVYvaxGHo1RCpUn48qSdm1OLk57/muLdn4ssL8eEl/TUPkkgMUnEkEsOsrVuxNmws0bbm1i1433wDw7KItOtAqFNXKOFj98dI12zj5c3GZpORzXJzB8uMHWQUXjtUINn2FhVDzexUvLgcSioi5eX7U3sR8bo59akvOOqjufhzgrxz9WAsdV0WiSkqjkSqASMzE9/0tzFCQaKNmxA8YbC6bMSYCBZrjF0sN7ez3NxB9h4TsBo2NClsHbJrU8dW65BIdTRz8KEE47yc+egn9Px6Ib68EK/feDJRj/4cE4kV+m0UqepycvC/+xZmTg7R9HQCp54Obv1qx4I8wqw0d7Lc3MEqYyehPbrLeW2TFlYtWtu1aWnVJk5vxyI1wry+nQjGeTnvgQ/oPHMZF939Li/fOpSw3+t0NBFBxZFI1RYM4n/vHczMnVjJKQRPPxP8fqdT1Wg5hFhqbmepuZ01xi7sPRqAEm0vra3atLZq08xO0QSsIjXUosPb8MKdZzDi3um0+XU1o257ixfuPINAot6/RZym4kikqgqF8L/3Dq4tm7Hj4ggMG46dmOR0qhopi2BRQbTW2M2ePeLSrXha2/kFUQM7Ud3lRASAFd2a8+y9ZzHyjrdpvmQDl499g+fuHk52WUYpFZFyo+JIpCoKh/HPmI5rw3psny+/MKpd2+lUNcquaA7L1s/kj9DPrPdmFlvXwEqknZVGWyuNWsQ5E1BEYt7ado14+oFzuWTcWzT8cyujb5zKC3eeSUYjvZ+LOEXFkUhVE4ng++B9XGvXYHu9BIYNx6pX3+lUNcIuAiw2M1hibmfT7mzY/de6xlYSbQsKohTUNUZESmbTIXWZ8uC5XHL7W9TZlMmYf0/lpXGns6Z9Y6ejidRIKo5EqpJIBN+H7+NevQrb7SEw9AysBg2dTlWtZRNiiZnBH2YGG8ysouUG0DSlGW1zEmiTG08SPudCikiVltGoNk8+fAEX3v0uTZdv4tJbpjHt+pP4/ah2TkcTqXFUHIlUFeEwvg/ew736T2y3m8Bpp2M1buJ0qmopjwhLzQwWmxnFB1WwoZmdQjsrjXZp7Unt1pPIrJ+x9mxCEpEqzyzl5KxGwfamYZT6sYVy0xJ59sFzOOfBD+gwewUXPDCDz/51DF8PO1xTM4hUIhVHIlVBKIT//XdxrVtb0GI0DKtpM6dTVSshoiw3d/CHuY1VRiaWYReta2gl0sFKp52VVtRCZJrqOidS3RheL9g2Pp+nVI/zefP/nPJ63cTFHcSQ3HFe3rnvbE6Y/B96v/8zg175lvprtvH2lYOIlDKTiJSNiiORWBcI5BdGG9bnX2N0+plYjdQXvTxEsFhp7GSxuY3l5k4ie8xDVNeKp72VTgerDqm6hkikZnB7wDCI/r4QKyenxA+LdPBDB4gsX0F4zcKDjvFhr9ps9nZnyLsL6P7NH6Rt2Mkrtw1ld5pGJBWpaCqORGLZrl3433od17ZtRaPS6Rqjg2Njs8bYxUJzG8vM7QSNaNG6WrafDlYdOkTTqUO8gylFxEl2Ti52VtaBNyyUF8j/Hsgr3eP24+du9dh5+PmccftbNF2+iauveZnXbjqVPzupO7VIRVJxJBKrVqzA98RjmNu3Y8UnEBx2Jlbdek6nqrIyyGWhaysLzW1kGaGi5Um2l/ZWHTpY6dS3EzQPkYjEjNXdmjPpsRFccPd0GqzZxqW3vMHHI/vxw8k9dR2SSAVRcSQSg1wL5sPwofmFUWoqgWFnYaemOh2ryskhxB9mBgvNrWw2/+oi47NdtLfq0NFKp4mdrIJIRGLWjga1ePKR8xn25Occ+u0fnPLcVzRbspF3rzyB/2/vzuOjqu/9j7/OOTOTmWxkZwk7JCEQloCKAioQQDYF1CJq1VZtbW+t/upVuVoXWtRa7G3duqit1HpVUEFckVVUQBaRLawhBEgIkIWELDOZzJlzfn8ERsIiCSRzJuHzfDzmkZlzzpzznplvZs7nLN/jDZdeMoVoalIcCRFiHEs+J+rnP4XqaozkZDzXXQ8RcsX0hvId71ghWy1mr1IW6GlONRV6mLFk+BPpacZhQ7U2qBBCNJDP6eCdB69lf69krvvnMgZ8vYPkPYd5a/okDvaU69wJ0ZRCujhasmQJ9957b71h11xzDS+++KJFiYRoXs7XXyPy0YdQDAOysvCOGgOlZVbHCnkmJvlKBdlqETtPOY+ovRFJhpFEbyOBcKS3JyFEC6UorL52EAd7tuXWWR+ReKiMex/8D5/+dIQcZidEEwrp4mjPnj2MGDGCmTNnBoaFhckuZNEK+f1E/O5xwv/xMgDeW28nbPY/4W//AKQ4OptS3GRrxWxTizmmeAPDo80wMoxEMvyJxEvHCkKIVmR/ekf+8uJPmfrCZ2SsyWHSa8tI2bSP9+8bR2VspNXxhGjxQro4ys3NJTU1lcTERKujCNFslPIyon9xF47lSwGo+u2T+B54kDC77OU4E7fPzWb/frbaDnBIrQoMDzM1ehnxZBhJch6REKJV80S5eOO31zPk0++Y+K/l9F6fywO/+hfz7h1L9pA0q+MJ0aKFfHE0ZMiQC5pHKOxlPpEhFLKI0KLt2E707Tej7cvDdLmofOFv1E65AdtJV1i3vN2ctHyrstTg49Pt7/NK4Qdsy8nDMA1QQTGpO4/ISCTFiMOOFrxQyhnvWkP5wYfBWuxpA0PpfbE8CzRrnkbPz6I2c0ah9Dk1QZamfg2qerY5Kqy57hL29e3MTf/7MR32FnHHMx/w7ai+fPzzUdRENv312Uzz+4tjW/7bdBZKCPxmidMpivWfR0OXH7LFkWma5OXlsXLlSl555RX8fj9jx47lvvvuw+Fo+NWn40PogmmhlEWEgPffh5/8BKqroWtXlA8+IHrAgHqTuJwOsLo3ouNXZXc67UHNYmKyinzeZDPvVuyk/D13YFx7NZp+/kQySCJScYAGwayLANDqOnSw2VSwW/xVqn3fuYS9mbPYbHVvtE3TzrysEHxfQiILNHmec34WDcgCzd9mzimUPqfzzHJBn8XZhLvANAkL++GjCI717si//v4zhr+xgmHvrOKSpVtJ25jHZ78ex46r0ptujdQ0A/MKt/p36QeceL+cYfaQznmxcDrr1tljYlpOx1Ih8GtxZoWFhXg8HhwOB88//zwFBQU89dRT1NTU8NhjjzV4PqWllZy0ocMSilJXGIVCFhEC/H7C//AU4S/8LwC1Vw2n8tXZmPHxUFJ38UCbTSUmJgJPTS2m2/tDc2t2qteHE6ip8WEEIUuuUsY7tmzm2LaRp5bXDTShY3RH0m0dSOl+CbFb8zBrKgDwoTd7pjNR/AY2QNcNTJ81GU7NAuBr5iy6XtfZhe73n3FZofi+hEKW5shzrs+iIVmg+dtMQ7OEwud0vlku5LM4axZFxaYo+LdmY1RX/+C0PuDzvlHsiBrMlHlbSSyp4qYZ77G9dxIfX9ebijYXthdJiYjA1jeDmppanE4Hbot/l36I1+sDoMbrC+mcF4saW911BcvLq9F1w9IsJ9bHzyVki6Pk5GTWrl1LmzZtUBSF9PR0DMPgoYce4pFHHkHTGraZ2DQJmYIklLIIayjlZUT98m7Cli0BwP3LX1P9+O/AZoOT2oZ5lvuWCEKWo3iYb9vJO/Zs1moHA8MjTQeT9FRujRnOmPuf5x9z/4fiiAh08rD6bVFOfl+siwHUzwLBy3Om5YTq+2J1FmjePI2dn1Vt5kxC6XNqiixN9RpOZDGr3ZiVlQ16zr54Gy/d2Z/hq/K5+puD9N5eRPc9pSwa0YV1A9thXuBepJD6bTqLlpDxYtSS1oFDtjgCiDnlopc9evTA6/Vy7Ngx4uLirAklxHnSsrcSfddt2PL21p1f9OeX8N4w1epYlqnFzyItl7ft2SzScqk93v22aiqM9HflFj2DCXoKEThQbR3Q1GAfNyeEEC2PblNZenUXtqYnMGVhLp0PVjJp0V4GbCvmg3E9KUqUHjyF+CEhWxx9/fXXPPjgg6xYsQKXywXAjh07iImJkcJItCymiXP2P4l88lEUrxd/p85U/Pst9L79rU4WdCYma9WDvGPfxge2HRxVagLj+vqTuEXP4Ed6b9qZ0h2tEEJciCNJEbxyW18Gf3eYa1bso0tBJff+axOrBnfgiyEdqQ0L2VVAISwVsv8ZmZmZhIWF8dhjj/GrX/2K/Px8Zs2axd133211NCEaTDlWTtRvfk3YJx8C4B0zlsoX/l53ftFFZI9ylDn2bcw9+TwioJ0RyTS9D9P0PmQYSdYFFEKIVshUFdZc0p4dqXFctyiX9Jwyrv7mIJlbilg8ogsb+yZd8KF2QrQ2IVscRUZG8q9//YtnnnmGG264gYiICKZNmybFkWgxbN+uI/qeO9HyD2Da7VQ//js89/zK+r4sg6QEN/PsO5hj28Z6rTAwPMK0M0lP42Y9g6v8ndFQf2AuQgghLtSx6DDevDGdtD1lTFiaR0JZDTd+sofLNxzmk9HdONAx2uqIQoSMkC2OAFJSUpg9e7bVMYRoHMPA9dcXifjD71F0HX+XrlS89m/0AQOtTtbsatD5TNvDHHs2i7W96EpdzzSqqZDl78bNep/AeURCCCGCSFHYlRLHnu4xXLH+ECNX5tPxUBW/+M9WNvVJ4PMRXamIlq6vhQjp4kiIlkYpKiL6vl/gWL4UgJrJ11P1pxcwo9tYnKz5GJisUvOZY89mgW0Xx5Tvu04d4G/LzXoGN+rptJXziIQQwnJ+TWXl5clsykhk9Jf7GbS5iAHbSui9+ygrL+vA15cn45XzkcRFTFq/EE3E8dknRP33r1FLSzGdTqqenkXNj+9otYfR7VRKAucR5asVgeEdjWim6X24ydeHdDPBwoRCCCHOpirSwQcTUlg7qD0TF+fRtaCCkasKGPzdYZYP68S6zHb4bXLYs7j4SHEkxHGqqqCq51HIVFQQ/uh0wt5+EwC9TwbVr/wLo3ef8/4H07TQ/EHKV44xz7aT923b2aQdCQyPNsOYoqdxk96HYf7OqLTOglAIIVqbwnaRvHpbBn12lTJmxX4Sj9Zw7ZI8hq4rZMnVndnSJ9HqiEIElRRHQlBXGMXFhKM0tij5+mu4/XbYt69uD9HDD2P73e9oE9Y0x20rIVBkHNbL+WTdy7xV+WfWROQFhttMlTH+7tyk92G83hMXdgtTCiGEOG+KwrZeCexIjWfQ5iNkfX2AuGNebvoohyvXFLJoQi/2D24hV/AU4gJJcSQEdcWRoqnUvD0Xs6jo3E/QfdgWfobti+UopokRF4fvltsw2iXD31+94DxarzQcY8dc8HzO11E8fGTbzfu27Xy17wDGvrofRcWEoUYnfuTrzSQ9jQTkYoJCCNFaGKrC+sx2bMpIZMj6Q1z1TQEdiqr56ewN7N9QzOe3DOPQ4BSrYwrRrKQ4EuIkZlERxsHCH5xGLTqCY+GnaMV1RZQvox+1I7IgLAzO8dyGUpKCfxhDJV4+teXwvm0HS7W8QE9zAIOTB3NDaTKTS9vTwYwKejYhhBDB47NrfDmkI+sGtGX4NwVcvuEwXbYc4J4tb5OX2ZWF04aS16eT1TGFaBZSHAnRUH4/9jWrsa/9BsUwMF0uvGPG4U9JtTrZefPgY5GWy/v2HXyu5VKj6IFxff1J3KinMzX1enr95D5qXngZv3nQwrRCCCGCyRNuZ2FWN1aPSGXEzmoGfvId3Tbu47827mP3gK4svnUY+9M7Wh1TiCYlxZEQDaAeOYzj80/RiosB0FNS8Y66BiIiLE7WeJV4WWTby4faLhbbcqlWfIFxKUYcN+jp3OhLp9fxnuY0e5JVUYUQQoSAijZOPrv/apZOuYzR89aQ+dlGUjftI3XTPnYN7MaiW68kP62D1TGFaBJSHAnxQ3T9+71Fplm3tyhrDP60Xi2qi+6jePjMlsNHtt0s0/LwKv7AuE5GNDfq6dyop9PPaBsSnUAIIYQIPccSo/n0NxNZMuUyRs5ZzaVLt5L2XR5p3+Wx45IeLL51GAUp7a2OKcQFkeJIiLNQDx8ibOGnqKUlAOipvfBmjW4xe4uOKFV8rOXwoW0XX2n78Svf9zTU04hlkp7GJD2NTKOdFERCCCEarDypDfN+PY7lP7qCUXNXM2jZVtK/zSX921y2X9qDpdOGyp4k0WJJcSTEqWprsX+zCvu3647vLQrHO+r43qIQZmKyWznKQtsePrXlsEYtwDyp5snwJzJJT+M6fxq9jQQpiIQQQlyQsnYxvHf/+Loiac4qBq7YRu/1ufRen0tO/y4su2kIuX07t6gjLYSQ4kiIk6g7dxD2ztuoFccA0Hv1xjtyFISHZpfVPvys0vJZqO1hoW0Pe9XyeuMv8bevK4j0NHqYsdaEFEII0aqVdohl7gMTWXbTEEa+9w0Dv9hGyub9pGzez75eySy76Qp2XtJDiiTRIkhxJASgFBfBvY8R9vbbABhR0dSOGoO/R0+Lk52uFA9LbLl8pu1hqS2PCsUbGOcwNa7yd2asvycT9RQ6mtEWJhVCCHExKUmO493/N4HFtwxjxLy1XLp4M113HuSu373Pwe5JLJ86hK1XpGI29oLrosEURQmZGtRsodcNluJIXNxMk7A5bxE54zEoO4qpKOgDB1E79CpwOKxOB4CByVb1CEu1PBbZclmjHsQ46fyhBCOcsf4ejNN7MtLflSjCLEwrhBDiYlee1IYPfjmGpTcN4aoF67jis40k7y3itmcXUNQxjuU/uoKNV/fGsGlWR21VFEXB5bSjqKFRHZmG2SL3FkpxJC5a2p4cIh/+DY6VX9UN6N8f78hR+DW7tcGAQ3oZX216g4XVb7AsfDslqrve+D7+RMb5ezJe78kgoz0ashVOCCFEaKmMi+TTO0fyxY1XMOzjbxn60bckFRxl2l8+ZczbK/lyymWsH9UPn9P6393WQFFAURX0rdmY1dXWZomIwNY3w9IM50uKI3Hxqa4m4i/P4fr7Syg+H6bLhWf6bwl/9GHMv/4DDhYGPZIHH6u1ApZpeSzT8tiWVwx5x0eqEGk6uMrfmSx/N8bqPehixgQ9oxBCCHE+3NEuFt96JV9OuYwrPtvIVQvWEXfkGFP+sYQxb69k9fhMVk0cRHVMy+gNNtSZ1dWYlZVWx2ixpDgSFw/TxPHJR0Q+8QjawQIAvFmjqXrmOdSUnoTbg7flqhY/36mH+Fo7wNfaAVZrBdQoemC8gsKgDoPIKo1nRFk8lxnJOJDDD4QQQrRc3vAwVtx4OSuvHcRlS7Zw1YL1xB8uZ/Sc1Qyft5YNWX35avKlFHeMtzqquIhJcSQuCtqeHCIfeRDHl18A4O/Umaqn/kjt2PGgKM1+UJoXnW/VQ6zUDrBSy2eNVoDnpGIIoIMRxSh/N0b6u5HVezwdb/s5NS+8jL/0YDOnE0IIIYJHD7OzeuIgvhmXScY3uxk+fy2ddx/i8s83cdmiTWwfnMJXky8lr0+nFnnOimjZpDgSrduph9CFheH+1f2473ugWbvn9uALFENfawdYpxXW2zMEEG+6GOrvxDB/Z0boXellxgeuPaRp0sucEEKI1s3UVLYO68XWoWl021bA1R+spc/aPWSsySFjTQ6F3ZJYee0gNl7dGz1MzksSwSHFkWid/H6cc98m/A8z0Y4cBsA7agxVT/0Ro3uPJl2Uick+5RjrtYOsUwtZpx1ki1qErhj1pks0whnm78wwoxNX+jvTy0hAlQuxCiGEuNgpCnkZncjL6ERifilXfbiegV9k0yGviKkvLmTC7C9YN6Y/qycMpDypjdVpRSsnxZFodewrlhM54zFs27MB8HfuStVTz1J7zbgm2T1fRS0btEOsVwsDBVHxKb3JAbQ1IrjS37muIPJ3Iu2kPUNCCCGEOF1xp3jm3TuWz+64msuWbGHIp98Rd+QYI+at5eoP1rH9sp6snjiIPf26YIZIl9WidZHiSLQa2o7tRP7uMRzLlwJgtInB/cDDeO78GYSd37V/KvGyVS1is3aEzeoRNqmH2a6W1LvOEIDdVBlgtONSfwcuNTow2J9MJzNaiiEhhBDiPHiiXHx5/WC+mnQp6d/mMvTjDaRu2hc45O5o2zasH9WP9aP6cixRDkUXTUeKI9HiqUcOEz7rGZxv/QfFMDDtdjx3/gz3Aw9jxsY1eD6l7lLW6LvYZN/OJvUwm7Uj7FGOYp6hvuloRHOpvwOXGR24zJ9Mf6MtTvl3EkIIIZqUqalsH5zC9sEpJB0oYein35G5YhtxR45xzVtfM/qdlezK7Mb6Mf3YflmK1XFFKyBrc0GkaefuE01RFNRm3k1sGCamaZ57wiC4kNerHD6E44W/4Hj9nyg1NQD4rptMzYyZmD164DjL8zy6h12lO9lZuoMdpTvYWbqdbSXZ5Ffm101wyk6mDkYU/Y0k+hvt6O9vyyCjPR3MqPPKLIQQQrRUJ36vFUWxpBO5kq6JfPira/jsrpFkrN7FpYs302PLAdI37CV9w16q2oRTMPFquLcr6kl5m5tpEjLrVeLCSXEUBKqqgGEQG3vui5sZpoGqNHfH0i3coUMwaxb84x9wvChiyBCYNQv70KGc6M+mRq9hV8kuthVvY1vRtrq/xdvIPZqLyZm/xHrG9SSzXSaZ7TIZkNiX9PfXkFjlD87rEkIIIUKQ4nCAaRJ2vMc4l+tsmx+Dw3Ta2TlhIDsnDCTu4FEyP9vIgEWbiCqtotdbC+GthfykfRxbrupF9og+HEpt36xdgpuGiafGJwVSKyHFURAoigKqiveduRhHis4+XWIizlun8e5HsyguPdA8YVwubD17UuP1YRrW/hMrqoIzzI6+Zw94POecPvxYNZkLN5LxRTY2X13Bkt+zLZ9OzmBLVyfFG/6HojXHKNLLKfaVc9RfddYiKEJ10t4eF7h1iO1Cl54Dse/MxSyqJtGsZdyVE/FE5mEcK2zS1y2EEEK0KDY7KAr+rdlo3hp8unUbDZX4BGwpPfBvzcaoruYI8HnfSBb3HkLK7hKu3ueny3e5RB06ytC5qxk6dzWlceFs7deObRltKewQ3aSFkhIRga1vBopStwdJtHxSHAWRUVSEcfDsK9on9hcV5e+ksHB3s2RQoqKwt4vA46nFsLg4UlUFl8uBb182ZmXlWaeLKHPTdUc+UYdKWRVt8uYI2Jyssa2dRpGjCENZBiVnfq7T1Egww0kww0k86W84dpSaE1+OflTclHvL0Y/kYlRWNP2LFUIIIVo4s9oNNW7w6WfZ9Nj8lPCIQJaT1x38wM6OLsLGj6LLddNZ+OD1dFy+nrQ9ZcQfdTN8xV6Gr9hLRYSdnB6x7O4eS063GGpcsios6pMWISznM/0cw0OF4qVcqeGYUkM5Xjy+aiqo4WhbE9qe+iz/8RvYTJVY00kcrrq/pos400ms6SICu/QYJ4QQQlxMXC72XtKDr2MqcdT66ZVzlL47SkjJKye62segLUUM2lKEX4H85Ch296grlA61jcBowPnhonWT4kg0q1q/l8rayrqbr5KqU+/7KqjRazhj7wna93cjayHWcOKyh9MGJ3HHi6BY00U0DimAhBBCCHGaWofGlj6JbOmTiKYbdM2vIDW3jLTcMpJKPXQtqKRrQSVjvjxArU3lYPtI9neM4kByFPnJ0VRH2M+9ENGqSHEkGs00TXxGLdW+6pNuVYG/Vb6qQAFU6/c2aJ4OQ6VDtULaET89j0K3MuhUAbXxseSnJVOZ2KbuuEPpG0EIIYQQ58FvU8ntFkNutxgWjupGTHkNqXvLSM0tp2t+BeE1Ot3yK+iW//3h9SWxTvI7RFGU4KIoIZyihHDKYp0YcgHaVkuKIxHgN/xU63XFjjtQ7JxS/Bwfrxu+Bs83TAsj0h5FlKPuVnc/mg4elSHZJYz4aCs9CtyBfT+lMU7WDmzHd4OTcIfLFhshhBBCNL3yGCfrBrZn3cD2KKZJQqmHzgcr6VxQQZeCSpJKPSSU1ZBQVlPvebqmUBLnoijBRWm7GKqKNEqjXZTHRlIRH0lVmwjMECieFMPErhvYfQY23cCu+7HrBjafgc3//Vljyik9SZiKgs+m4rOrJ/3V8NlV/JrSrD3/hQIpjloxH37c+HArPtzouBUfHr2Emr2VVNRU4fZV4/F5cOtuPLobbwP38pxgVx1E2CPq32yRRDgiiLJHB4ohh3b8wkGmSUJhGRnf7Kbfqo10yjn8fVabyvbUODb0SyK3WwxmK//HE0IIIUToMBWF4oRwihPC2dC/7kRnp0enc2El7Y9Uk1TiJqnETWKJB4du0K7YTbtiN+wohS9y683Lr6lUxkZQGRtBjSuMWpcdrysMr9OON9xBrdNBrdOOqSiYCoF1HlNVMI9vKrb5dGw+P/Zave5+7cmP/dhqdew+HZtXx177/c1Rq2Nz12Cv9WNrho63/KpCtcuGO9xOdbid6nAbblfd/aoIO8eiwzgW7eCYGobe5EsPDimOWggTE+9JxY4HH25FP6n4+f6x5/hjn2KcPiM/kL/rrMtRFZUIeyQRtu8LnvBTi5/jwxzaua9zEOb20m1bLmkbcum1YS8Jh8oD4wxV4UDfzmzsHsmWbpF4ndIchRBCCBEaalw2dveIZXeP2MAwxTRpc8x7vFjyEF/tJ0ZzEVFUQXRpJZHl1Wh+g5iSSmJKzt4Tb7D5NAX9xJ4gm1a3BwgwA9uiv3+sGiY23cBxYo+Tz492vM7SDJPoah/R1ec+gmjDxjJWPnJnM7ya5iVroyFks7+ApV89xdKqNRRrxfX3+ODDrzR+C4BmKoRjJ9y048JOhM1FZPvOOJQwnFo44TYXLls4Lls44fZwnJqz7rpM58M0iSmuoOOew3TdXkD3bfkk5x5BPWnLhW5T2ZvRma1D09g+JA1/h1h8a9b+YFfeVlISE7Gi3xolNvb75Vt84YRQyKIkJlqyXCGEEOJkpqJQHuOkPMbJ7p7HL5Fy+eDAJVJUv0FUWRXRpVVEllcTVuMjzFMbuDlqaglz1+Lw1hUXinF8X5FpopjfH+Km2zV8Dhu6XUN32NDtttMe6w4Nn92GL8yGL8yO32lHjXLh2bEdX63neCGkotvVCz4iR/Ub2HWDMK+fCI+PCLdOhNtHuNtHxPFbZLWPNhVeYipqiXT7sNe0zH1HUhyFkB+7Z5P7RXHdA+3M09hNNVDsfP/XFnjsCgy3EY4dB1q9ntwUVxT2noMv7DpHpklUeTUJB8tILCilbX4pbQ+U0DH3MBEVp1/M9WjbNuzO7MbOQd3Z078L3vC6w+xUVcF1fgmaXWRkHIZp4Lx1mqU5wm65ydLlnywkstjkK0sIIUToMjSVYwnRHEuIDvqyA9ePLNmPWXmGo4cugKGpeDUVb5iNiuiwc06vhUegDb2C2HNOGXpkTSMIHO+8Bau/wpG9DbOqCvx+FL8fTtxMAxSFZ1IMlqYm4iyvok21QbxXIbZGJdqnEu3XiNJtaIqGblPQbWrdTTu+VcCmottMfDYd3eZH1+pOmvOf+KvW3TeqQdtXTK3bC34D5fhWDtUwUQwDzW/Ubd1w123hCK/0EHnMXXcrqyamuIKYkgrsvjN3G+fXVA53SSA/tQO5GZ3I69OJY4nB/4K4UC5nJKqi8u5HsyjK3xn05Svx8dh69kTfmo3prg768utliYvHlmJtltSUwYwZcw9oZ9lqIIQQQoiQYWjq2bbzhzwpjpqbaeL67f9Aedk5G8nUIzB1ZTBCrbrgORiqQnlCFEWdEijqGE9Rp3gOdm/L4a6J6I7W06yKSw9QWLg76MtVjXbY2kWgH8nFqKw49xOaM4u/Hbb21mZJTOhsyXKFEEIIcXFpPWuxoUpRqHrnPaK3b6Z29TeYlVWgaZiaVrcVXNVAVeuONY2LI2zSRD75+HnKjh5EMUHzm2j+E10wnvTXbx4/Sc7Adny8Ta8/nWaYqH4Tm99EM4y6+yaoqoahKBiqgqGqGJqCqaoYqoLfplHrtON1OfC6HLijXFS1Cae6TThVMeGUJ0RTlhTNsfgoDFtL3SYghBBCCCHE6aQ4CgL/4MthbBZ+7SWMgsKzTqcmd4CJE9m3fz6FhTVnne5CnHrioBBCCCGEEKKOFR1xCSGEEEIIIUTICeniyOv18uijj3LJJZcwbNgwXn/9dasjCSGEEEIIIVqpkD6sbtasWWRnZ/PGG29QWFjI9OnT6dChA2PHjrU6mhBCCCGEEKKVCdniyO1289577/Haa6/Rp08f+vTpQ05ODm+99ZYUR0IIIYQQQogmF7KH1e3cuRNd18nMzAwMGzRoEJs3b8YwmvbCVkIIIYQQQggRsnuOiouLiY2NxeFwBIYlJCTg9XopLy8nLi6uQfM53ku2pRTleJYOHcBhP/t08QkAdOjWD0ebhr2+RnM6sUV3ptalY1r8xiiKgsNuQ++pQ03z9M7X4Cxt2qBFJ+PvqWPW1JCQnApAh059sNud1uVJ0TE9ofXeWCHwebTvgSvMtDTLCaHwvpyahe4GutvdrMs61/9GKL4voZClOfJcyPdUMNtMQ7OEwud0vlma4zcjJN+XFB2tthbdb91G6nO9L0H//Zb1qjM7/r5E29scz1a3Tm6lE+vj55zOtPqTPIsFCxbwwgsv8MUXXwSG5efnM2rUKL788kvatWtnYTohhBBCCCFEaxOyh9WFhYVRW1tbb9iJx05n8LfkCyGEEEIIIVq3kC2O2rZtS1lZGbquB4YVFxfjdDqJjo62MJkQQgghhBCiNQrZ4ig9PR2bzcamTZsCwzZs2EDfvn1RrT5oUQghhBBCCNHqhGyV4XK5mDx5MjNmzGDLli0sXbqU119/ndtvv93qaEIIIYQQQohWKGQ7ZADweDzMmDGDxYsXExkZyV133cVPfvITq2MJIYQQQgghWqGQLo6EEEIIIYQQIlhC9rA6IYQQQgghhAgmKY6EEEIIIYQQAimOhBBCCCGEEAKQ4qjJeL1eHn30US655BKGDRvG66+/ftZpt2/fzo9+9CP69+/PDTfcQHZ2dhCTilDRmDazYsUKJk2aRGZmJtdeey3Lli0LYlIRKhrTZk4oKCggMzOTtWvXBiGhCDWNaTO7du3i5ptvpl+/flx77bWsWbMmiElFqGhMm1myZAnjxo0jMzOTm2++mW3btgUxqQgltbW1TJw48Qd/a1rK+q8UR01k1qxZZGdn88Ybb/Dkk0/y8ssv8/nnn582ndvt5uc//zmXXHIJ8+fPJzMzk3vuuQe3221BamGlhraZnTt3cu+993LDDTewYMECpk2bxv3338/OnTstSC2s1NA2c7IZM2bI98tFrKFtprKykjvvvJOePXvy8ccfM3r0aO69915KS0stSC2s1NA2k5OTw3//939zzz338OGHH5Kens4999yDx+OxILWwktfr5YEHHiAnJ+es07So9V9TXLDq6mqzb9++5po1awLD/vrXv5o//vGPT5v2vffeM0eOHGkahmGapmkahmGOHj3anDdvXtDyCus1ps0899xz5l133VVv2J133mn++c9/bvacInQ0ps2c8OGHH5rTpk0zU1NT6z1PXBwa02beeOMNc9SoUaau64Fh119/vblixYqgZBWhoTFtZvbs2eaUKVMCjysrK83U1FRzy5YtQckqQkNOTo553XXXmddee+0P/ta0pPVf2XPUBHbu3Imu62RmZgaGDRo0iM2bN2MYRr1pN2/ezKBBg1AUBQBFURg4cCCbNm0KZmRhsca0mSlTpvDggw+eNo/KyspmzylCR2PaDEBZWRnPPfccv//974MZU4SQxrSZdevWkZWVhaZpgWHz5s3j6quvDlpeYb3GtJmYmBj27NnDhg0bMAyD+fPnExkZSefOnYMdW1ho3bp1DB48mLlz5/7gdC1p/ddmdYDWoLi4mNjYWBwOR2BYQkICXq+X8vJy4uLi6k3bs2fPes+Pj4//wV2RovVpTJvp0aNHvefm5OTwzTffMG3atKDlFdZrTJsBePbZZ5kyZQopKSnBjipCRGPaTH5+Pv369ePxxx9n+fLlJCcnM336dAYNGmRFdGGRxrSZ8ePHs3z5cm655RY0TUNVVV555RXatGljRXRhkVtuuaVB07Wk9V/Zc9QEPB5PvS8SIPC4tra2QdOeOp1o3RrTZk529OhRfv3rXzNw4ECysrKaNaMILY1pM6tXr2bDhg3813/9V9DyidDTmDbjdrt59dVXSUxM5LXXXuPSSy/lrrvu4tChQ0HLK6zXmDZTVlZGcXExTzzxBO+++y6TJk3ikUcekfPUxBm1pPVfKY6aQFhY2Gkf7onHTqezQdOeOp1o3RrTZk4oKSnhjjvuwDRNXnzxRVRV/n0vJg1tMzU1NTzxxBM8+eST8r1ykWvM94ymaaSnp3PffffRu3dvHnroIbp27cqHH34YtLzCeo1pM3/6059ITU3l1ltvJSMjg5kzZ+JyuZg3b17Q8oqWoyWt/8raVRNo27YtZWVl6LoeGFZcXIzT6SQ6Ovq0aUtKSuoNKykpISkpKShZRWhoTJsBOHLkCLfeeiu1tbX85z//Oe0QKtH6NbTNbNmyhfz8fO677z4yMzMD5w787Gc/44knngh6bmGdxnzPJCYm0r1793rDunbtKnuOLjKNaTPbtm2jV69egceqqtKrVy8KCwuDlle0HC1p/VeKoyaQnp6OzWard1LZhg0b6Nu372lb9/v378/GjRsxTRMA0zT57rvv6N+/fzAjC4s1ps243W7uvvtuVFXl//7v/2jbtm2Q04pQ0NA2069fPxYvXsyCBQsCN4CnnnqK+++/P8iphZUa8z0zYMAAdu3aVW/Y3r17SU5ODkZUESIa02aSkpLIzc2tNywvL4+OHTsGI6poYVrS+q8UR03A5XIxefJkZsyYwZYtW1i6dCmvv/46t99+O1C31aWmpgaAsWPHUlFRwdNPP82ePXt4+umn8Xg8jBs3zsqXIIKsMW3mlVde4cCBA/zxj38MjCsuLpbe6i4yDW0zTqeTLl261LtB3Va7+Ph4K1+CCLLGfM9MmzaNXbt28dJLL7F//35eeOEF8vPzmTRpkpUvQQRZY9rM1KlTeffdd1mwYAH79+/nT3/6E4WFhUyZMsXKlyBCSItd/7W0I/FWxO12mw8//LA5YMAAc9iwYebs2bMD41JTU+v1475582Zz8uTJZt++fc0bb7zR3LZtmwWJhdUa2mauueYaMzU19bTb9OnTLUourNKY75mTyXWOLl6NaTPffvutOWXKFDMjI8OcNGmSuW7dOgsSC6s1ps28++675tixY80BAwaYN998s5mdnW1BYhEqTv2taanrv4ppHt+/JYQQQgghhBAXMTmsTgghhBBCCCGQ4kgIIYQQQgghACmOhBBCCCGEEAKQ4kgIIYQQQgghACmOhBBCCCGEEAKQ4kgIIYQQQgghACmOhBBCCCGEEAKQ4kgIIYQQQgghACmOhBBCBMHIkSNJS0sL3Pr06cPYsWP597//fV7zmz9/PiNHjrygPPPnzz/juIKCAtLS0igoKAAgLS2NtWvXnva8qqoqFixYcN4ZhBBChB6b1QGEEEJcHB599FHGjx8PgK7rrFmzht/+9rfExMQwefJka8OdpH379qxcuZK4uLjTxr3//vuEh4cD8O9//5u1a9eGVHYhhBAXRvYcCSGECIqoqCgSExNJTEykffv2TJkyhSuuuILFixdbHa0eTdNITExE07TTxsXFxeF0OgEwTTPY0YQQQjQzKY6EEEJYxmazYbfbue2225g5cyZZWVkMHz6cqqoqDh8+zP33389ll13G4MGDeeqpp6itra33/D//+c8MHDiQK6+8kjfffDMwvLa2lj/84Q9ceeWV9OnTh5EjRzJ37tx6z83JyWHy5Mn07duXu+66i8LCQuD0w+pOduKwuvnz5/Pyyy+zbt060tLS+Oijjxg8eDC6rgemXbRoEcOHD5ciSgghWhApjoQQQgSdz+dj8eLFrFq1iqysLKDuPKLnnnuOl19+GYfDwR133IHH4+HNN9/k+eefZ8WKFcyaNSswj4MHD7Jr1y7mzp3LAw88wB//+MfAuUGvvvoqK1as4KWXXuLzzz9n8uTJzJw5k5KSksDz33nnHe6++27mzZuHrutMnz69wfnHjx/PnXfeSWZmJitXriQrK4uamhrWrFkTmGbhwoWMGzcORVEu9O0SQggRJHLOkRBCiKB48sknmTlzJgA1NTU4nU7uuOMOrrvuOt577z2GDx/OwIEDAVi2bBlHjhzh3XffpU2bNgA88cQT/PKXv+Q3v/kNAGFhYTz77LPExsaSkpLCunXrmDNnDoMHD6ZXr15cfvnlDBgwAIBf/OIX/PWvf2Xfvn0kJCQAcPPNNzNx4kQAnn76abKyssjNzSUsLOycr8XpdBIeHo7dbicxMRGAESNG8PnnnzNs2DA8Hg9ffvllvb1ZQgghQp8UR0IIIYLivvvuY8yYMUBdYXPqeT3JycmB+7m5uXTt2jVQGAEMHDgQXdc5cOAAAJ06dSI2NjYwvnfv3rz33nsAjBo1ilWrVvHss8+yd+9etm/fDoDf7w9M369fv8D9jh07EhMTw969e0lPTz+v1zdx4kQee+wxZsyYwYoVK0hKSiIjI+O85iWEEMIaclidEEKIoIiPj6dLly506dKFdu3andbhwcl7bM609+ZEYXPir6rW/wkzDAO73Q7AX/7yFx566CFsNhuTJ08+7Xwj4LTln/z883HVVVfh9/tZv349ixYtYty4cec9LyGEENaQ4kgIIUTI6datG/v27aO8vDwwbNOmTdhsNjp37gxAfn4+Ho8nMH7Lli10794dgDlz5vD444/z4IMPMn78+MB0J3eOsHv37sD9ffv2UVFRQbdu3Rqc8dRziRwOB6NHj2bJkiWsWrWKCRMmNPwFCyGECAlSHAkhhAg5Q4cOpVOnTjz88MPs2rWLNWvWMHPmTCZOnEh0dDQAXq+X6dOnk5OTw5w5c1i0aBF33HEHADExMXzxxRfk5+fz7bff8vDDDwPU6+1u9uzZLF68mJ07d/LII48wYsQIunTp0uCMLpeLoqKier3aTZw4kffff5927dqRkpLSFG+FEEKIIJLiSAghRMjRNI2//e1vAEydOpUHHniArKwsfv/73wemSU9Pp23btkydOpVXX32VZ555JnCOzzPPPMOOHTuYMGECjzzyCGPHjqVfv37s2LEj8Pyf/vSnPP/880ydOpX4+HieeeaZRmUcPXo0hmEwYcIESktLARg8eDARERGBi90KIYRoWRRTLsAghBBCNImqqiqGDh3KJ598QqdOnayOI4QQopGktzohhBDiApmmyaJFi1i8eDGZmZlSGAkhRAsle46EEEKIJpCVlYWmafz973+nR48eVscRQghxHqQ4EkIIIYQQQgikQwYhhBBCCCGEAKQ4EkIIIYQQQghAiiMhhBBCCCGEAKQ4EkIIIYQQQghAiiMhhBBCCCGEAKQ4EkIIIYQQQghAiiMhhBBCCCGEAKQ4EkIIIYQQQggA/j//XQnKoP3WxgAAAABJRU5ErkJggg=="
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m10/10\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 955us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "id": "8e87dfe7",
   "metadata": {},
   "source": [
    "## Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0517d43fb1c2ac6",
   "metadata": {},
   "source": [
    "### SHAP-values"
   ]
  },
  {
   "cell_type": "code",
   "id": "021d54c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T19:42:46.102246Z",
     "start_time": "2025-01-16T19:42:45.432505Z"
    }
   },
   "source": [
    "def shap_analysis_top_features_save(model, X_train, X_test, feature_names, top_n=7, output_dir=\"./\", label_size=10):\n",
    "    import os\n",
    "    try:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        explainer = shap.TreeExplainer(model)\n",
    "        shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "        mean_abs_shap_values = pd.DataFrame({\n",
    "            'Feature': feature_names,\n",
    "            'MeanAbsSHAP': abs(shap_values).mean(axis=0)\n",
    "        }).sort_values(by='MeanAbsSHAP', ascending=False)\n",
    "\n",
    "        top_features = mean_abs_shap_values.head(top_n)['Feature'].values\n",
    "        X_test_top = pd.DataFrame(X_test, columns=feature_names)[top_features]\n",
    "\n",
    "        feature_indices = [list(feature_names).index(feature) for feature in top_features]\n",
    "        shap_values_top = shap_values[:, feature_indices]\n",
    "\n",
    "        summary_plot_path = os.path.join(output_dir, f\"{model.__class__.__name__}_SHAP_Summary_Top{top_n}.png\")\n",
    "        shap.summary_plot(shap_values_top, X_test_top, feature_names=top_features, show=False)\n",
    "        plt.gca().tick_params(labelsize=label_size)\n",
    "\n",
    "        plt.savefig(summary_plot_path, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"Saved SHAP Summary Plot to {summary_plot_path}\")\n",
    "\n",
    "        bar_plot_path = os.path.join(output_dir, f\"{model.__class__.__name__}_SHAP_Bar_Top{top_n}.png\")\n",
    "        shap.summary_plot(shap_values_top, X_test_top, feature_names=top_features, plot_type=\"bar\", show=False)\n",
    "        plt.gca().tick_params(labelsize=label_size)\n",
    "\n",
    "        plt.savefig(bar_plot_path, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"Saved SHAP Bar Plot to {bar_plot_path}\")\n",
    "\n",
    "        return shap_values_top, top_features\n",
    "    except Exception as e:\n",
    "        print(f\"Error in SHAP analysis: {e}\")\n",
    "        return None, None\n",
    "\n",
    "output_directory = \"/Users/janlinzner/Library/Mobile Documents/com~apple~CloudDocs/Documents/McGill/1_Lectures/Decision Analytics (MGSC 662)/2_Coding/Exercises/ADA_Coding/Code/Plots\"\n",
    "\n",
    "print(\"Analyzing Gradient Boosting Model...\")\n",
    "gb_shap_values_top, gb_top_features = shap_analysis_top_features_save(\n",
    "    best_gb_model, X_train_scaled, X_test_scaled, X.columns, top_n=7, output_dir=output_directory, label_size=10\n",
    ")\n",
    "\n",
    "print(\"Analyzing LightGBM Model...\")\n",
    "lgb_shap_values_top, lgb_top_features = shap_analysis_top_features_save(\n",
    "    best_lgb_model, X_train_scaled, X_test_scaled, X.columns, top_n=7, output_dir=output_directory, label_size=10\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing Gradient Boosting Model...\n",
      "Saved SHAP Summary Plot to /Users/janlinzner/Library/Mobile Documents/com~apple~CloudDocs/Documents/McGill/1_Lectures/Decision Analytics (MGSC 662)/2_Coding/Exercises/ADA_Coding/Code/Plots/GradientBoostingClassifier_SHAP_Summary_Top7.png\n",
      "Saved SHAP Bar Plot to /Users/janlinzner/Library/Mobile Documents/com~apple~CloudDocs/Documents/McGill/1_Lectures/Decision Analytics (MGSC 662)/2_Coding/Exercises/ADA_Coding/Code/Plots/GradientBoostingClassifier_SHAP_Bar_Top7.png\n",
      "Analyzing LightGBM Model...\n",
      "Saved SHAP Summary Plot to /Users/janlinzner/Library/Mobile Documents/com~apple~CloudDocs/Documents/McGill/1_Lectures/Decision Analytics (MGSC 662)/2_Coding/Exercises/ADA_Coding/Code/Plots/LGBMClassifier_SHAP_Summary_Top7.png\n",
      "Saved SHAP Bar Plot to /Users/janlinzner/Library/Mobile Documents/com~apple~CloudDocs/Documents/McGill/1_Lectures/Decision Analytics (MGSC 662)/2_Coding/Exercises/ADA_Coding/Code/Plots/LGBMClassifier_SHAP_Bar_Top7.png\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "id": "ceaf28a919227f3e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T18:09:09.201737Z",
     "start_time": "2025-01-15T18:09:09.197982Z"
    }
   },
   "source": [
    "## Dashboard Preparation (Data to JSON)"
   ]
  },
  {
   "cell_type": "code",
   "id": "cf6e9fa392e2f450",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T20:07:42.063245Z",
     "start_time": "2025-01-16T20:07:42.050664Z"
    }
   },
   "source": [
    "def prepare_dashboard_data(test_results, feature_names,\n",
    "                           gb_shap_values, lgb_shap_values,\n",
    "                           ensemble_test_predictions, ensemble_test_probabilities):\n",
    "    from sklearn.metrics import confusion_matrix, classification_report\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import json\n",
    "\n",
    "    def prepare_confusion_matrix_section(true_class, predicted_class):\n",
    "        tn, fp, fn, tp = confusion_matrix(true_class, predicted_class).ravel()\n",
    "        return {\n",
    "            \"truePositive\": int(tp),\n",
    "            \"trueNegative\": int(tn),\n",
    "            \"falsePositive\": int(fp),\n",
    "            \"falseNegative\": int(fn)\n",
    "        }\n",
    "\n",
    "    def transform_results(results, prediction_column, probability_column):\n",
    "        transformed = []\n",
    "        for _, row in results.iterrows():\n",
    "            transformed.append({\n",
    "                \"id\": row[\"ID\"],\n",
    "                \"actualLabel\": \"Positive\" if row[\"True Class (Test)\"] == 1 else \"Negative\",\n",
    "                \"predictedLabel\": \"Positive\" if row[prediction_column] == 1 else \"Negative\",\n",
    "                \"confidence\": f\"{row[probability_column] * 100:.2f}%\",\n",
    "                \"timestamp\": pd.Timestamp.now().isoformat()\n",
    "            })\n",
    "        return transformed\n",
    "\n",
    "    def transform_results_ensemble(true_class, predictions, probabilities, observation_ids):\n",
    "        transformed = []\n",
    "        for obs_id, actual, pred, prob in zip(observation_ids, true_class, predictions, probabilities):\n",
    "            transformed.append({\n",
    "                \"id\": obs_id,\n",
    "                \"actualLabel\": \"Positive\" if actual == 1 else \"Negative\",\n",
    "                \"predictedLabel\": \"Positive\" if pred == 1 else \"Negative\",\n",
    "                \"confidence\": f\"{prob * 100:.2f}%\",\n",
    "                \"timestamp\": pd.Timestamp.now().isoformat()\n",
    "            })\n",
    "        return transformed\n",
    "\n",
    "    def format_shap_values(shap_values, feature_names):\n",
    "        shap_formatted = [\n",
    "            {\"feature\": feature, \"value\": float(shap_value)}\n",
    "            for feature, shap_value in zip(feature_names, np.abs(shap_values).mean(axis=0))\n",
    "        ]\n",
    "        return shap_formatted\n",
    "\n",
    "    gb_shap_data = format_shap_values(gb_shap_values, feature_names)\n",
    "    lgb_shap_data = format_shap_values(lgb_shap_values, feature_names)\n",
    "\n",
    "    gb_data = {\n",
    "        \"name\": \"Gradient Boosting\",\n",
    "        \"stats\": {\n",
    "            \"accuracy\": (test_results['True Class (Test)'] == test_results['GB Prediction (Test)']).mean(),\n",
    "            \"precision\": classification_report(\n",
    "                test_results['True Class (Test)'],\n",
    "                test_results['GB Prediction (Test)'],\n",
    "                output_dict=True\n",
    "            )['macro avg']['precision'],\n",
    "            \"recall\": classification_report(\n",
    "                test_results['True Class (Test)'],\n",
    "                test_results['GB Prediction (Test)'],\n",
    "                output_dict=True\n",
    "            )['macro avg']['recall'],\n",
    "            \"f1Score\": classification_report(\n",
    "                test_results['True Class (Test)'],\n",
    "                test_results['GB Prediction (Test)'],\n",
    "                output_dict=True\n",
    "            )['macro avg']['f1-score']\n",
    "        },\n",
    "        \"confusionMatrix\": prepare_confusion_matrix_section(\n",
    "            test_results['True Class (Test)'],\n",
    "            test_results['GB Prediction (Test)']\n",
    "        ),\n",
    "        \"results\": transform_results(\n",
    "            test_results, \"GB Prediction (Test)\", \"GB Probability (Success, Test)\"\n",
    "        ),\n",
    "        \"shapValues\": gb_shap_data,\n",
    "        \"paperUrl\": \"https://arxiv.org/abs/1603.02754\"\n",
    "    }\n",
    "\n",
    "    lgb_data = {\n",
    "        \"name\": \"LightGBM\",\n",
    "        \"stats\": {\n",
    "            \"accuracy\": (test_results['True Class (Test)'] == test_results['LGB Prediction (Test)']).mean(),\n",
    "            \"precision\": classification_report(\n",
    "                test_results['True Class (Test)'],\n",
    "                test_results['LGB Prediction (Test)'],\n",
    "                output_dict=True\n",
    "            )['macro avg']['precision'],\n",
    "            \"recall\": classification_report(\n",
    "                test_results['True Class (Test)'],\n",
    "                test_results['LGB Prediction (Test)'],\n",
    "                output_dict=True\n",
    "            )['macro avg']['recall'],\n",
    "            \"f1Score\": classification_report(\n",
    "                test_results['True Class (Test)'],\n",
    "                test_results['LGB Prediction (Test)'],\n",
    "                output_dict=True\n",
    "            )['macro avg']['f1-score']\n",
    "        },\n",
    "        \"confusionMatrix\": prepare_confusion_matrix_section(\n",
    "            test_results['True Class (Test)'],\n",
    "            test_results['LGB Prediction (Test)']\n",
    "        ),\n",
    "        \"results\": transform_results(\n",
    "            test_results, \"LGB Prediction (Test)\", \"LGB Probability (Success, Test)\"\n",
    "        ),\n",
    "        \"shapValues\": lgb_shap_data,\n",
    "        \"paperUrl\": \"https://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree\"\n",
    "    }\n",
    "\n",
    "    logistic_data = {\n",
    "        \"name\": \"Logistic Regression\",\n",
    "        \"stats\": {\n",
    "            \"accuracy\": (test_results['True Class (Test)'] == test_results['Logistic Prediction (Test)']).mean(),\n",
    "            \"precision\": classification_report(\n",
    "                test_results['True Class (Test)'],\n",
    "                test_results['Logistic Prediction (Test)'],\n",
    "                output_dict=True\n",
    "            )['macro avg']['precision'],\n",
    "            \"recall\": classification_report(\n",
    "                test_results['True Class (Test)'],\n",
    "                test_results['Logistic Prediction (Test)'],\n",
    "                output_dict=True\n",
    "            )['macro avg']['recall'],\n",
    "            \"f1Score\": classification_report(\n",
    "                test_results['True Class (Test)'],\n",
    "                test_results['Logistic Prediction (Test)'],\n",
    "                output_dict=True\n",
    "            )['macro avg']['f1-score']\n",
    "        },\n",
    "        \"confusionMatrix\": prepare_confusion_matrix_section(\n",
    "            test_results['True Class (Test)'],\n",
    "            test_results['Logistic Prediction (Test)']\n",
    "        ),\n",
    "        \"results\": transform_results(\n",
    "            test_results, \"Logistic Prediction (Test)\", \"Logistic Probability (Success, Test)\"\n",
    "        ),\n",
    "        \"shapValues\": [],\n",
    "        \"paperUrl\": \"https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\"\n",
    "    }\n",
    "\n",
    "    nn_data = {\n",
    "        \"name\": \"Neural Network\",\n",
    "        \"stats\": {\n",
    "            \"accuracy\": (test_results['True Class (Test)'] == test_results['NN Prediction (Test)']).mean(),\n",
    "            \"precision\": classification_report(\n",
    "                test_results['True Class (Test)'],\n",
    "                test_results['NN Prediction (Test)'],\n",
    "                output_dict=True\n",
    "            )['macro avg']['precision'],\n",
    "            \"recall\": classification_report(\n",
    "                test_results['True Class (Test)'],\n",
    "                test_results['NN Prediction (Test)'],\n",
    "                output_dict=True\n",
    "            )['macro avg']['recall'],\n",
    "            \"f1Score\": classification_report(\n",
    "                test_results['True Class (Test)'],\n",
    "                test_results['NN Prediction (Test)'],\n",
    "                output_dict=True\n",
    "            )['macro avg']['f1-score']\n",
    "        },\n",
    "        \"confusionMatrix\": prepare_confusion_matrix_section(\n",
    "            test_results['True Class (Test)'],\n",
    "            test_results['NN Prediction (Test)']\n",
    "        ),\n",
    "        \"results\": transform_results(\n",
    "            test_results, \"NN Prediction (Test)\", \"NN Probability (Success, Test)\"\n",
    "        ),\n",
    "        \"shapValues\": [],\n",
    "        \"paperUrl\": \"https://arxiv.org/abs/1512.03385\"\n",
    "    }\n",
    "    \n",
    "    tabpfn_data = {\n",
    "        \"name\": \"TabPFN\",\n",
    "        \"stats\": {\n",
    "            \"accuracy\": (test_results['True Class (Test)'] == test_results['TabPFN Prediction (Test)']).mean(),\n",
    "            \"precision\": classification_report(\n",
    "                test_results['True Class (Test)'],\n",
    "                test_results['TabPFN Prediction (Test)'],\n",
    "                output_dict=True\n",
    "            )['macro avg']['precision'],\n",
    "            \"recall\": classification_report(\n",
    "                test_results['True Class (Test)'],\n",
    "                test_results['TabPFN Prediction (Test)'],\n",
    "                output_dict=True\n",
    "            )['macro avg']['recall'],\n",
    "            \"f1Score\": classification_report(\n",
    "                test_results['True Class (Test)'],\n",
    "                test_results['TabPFN Prediction (Test)'],\n",
    "                output_dict=True\n",
    "            )['macro avg']['f1-score']\n",
    "        },\n",
    "        \"confusionMatrix\": prepare_confusion_matrix_section(\n",
    "            test_results['True Class (Test)'],\n",
    "            test_results['TabPFN Prediction (Test)']\n",
    "        ),\n",
    "        \"results\": transform_results(\n",
    "            test_results, \"TabPFN Prediction (Test)\", \"TabPFN Probability (Success, Test)\"\n",
    "        ),\n",
    "        \"shapValues\": [],\n",
    "        \"paperUrl\": \"https://arxiv.org/abs/1512.03385\"\n",
    "    }\n",
    "\n",
    "    true_classes = test_results['True Class (Test)']\n",
    "    ensemble_data = {\n",
    "        \"name\": \"Ensemble Model\",\n",
    "        \"stats\": {\n",
    "            \"accuracy\": (true_classes == ensemble_test_predictions).mean(),\n",
    "            \"precision\": classification_report(\n",
    "                true_classes,\n",
    "                ensemble_test_predictions,\n",
    "                output_dict=True\n",
    "            )['macro avg']['precision'],\n",
    "            \"recall\": classification_report(\n",
    "                true_classes,\n",
    "                ensemble_test_predictions,\n",
    "                output_dict=True\n",
    "            )['macro avg']['recall'],\n",
    "            \"f1Score\": classification_report(\n",
    "                true_classes,\n",
    "                ensemble_test_predictions,\n",
    "                output_dict=True\n",
    "            )['macro avg']['f1-score']\n",
    "        },\n",
    "        \"confusionMatrix\": prepare_confusion_matrix_section(\n",
    "            true_classes,\n",
    "            ensemble_test_predictions\n",
    "        ),\n",
    "        \"results\": transform_results_ensemble(\n",
    "            true_classes,\n",
    "            ensemble_test_predictions,\n",
    "            ensemble_test_probabilities,\n",
    "            test_results[\"Observation Index\"]\n",
    "        ),\n",
    "        \"shapValues\": [],\n",
    "        \"paperUrl\": \"\"\n",
    "    }\n",
    "\n",
    "    dashboard_data = [gb_data, lgb_data, logistic_data, nn_data, ensemble_data, tabpfn_data]\n",
    "    with open(\"dashboard_data.json\", \"w\") as f:\n",
    "        json.dump(dashboard_data, f, indent=4)\n",
    "\n",
    "    print(\"Dashboard data is saved under dashboard_data.json\")\n"
   ],
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T20:07:43.697597Z",
     "start_time": "2025-01-16T20:07:43.636967Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Calling the function\n",
    "prepare_dashboard_data(\n",
    "    test_results=test_results,\n",
    "    feature_names=X.columns,\n",
    "    gb_shap_values=gb_shap_values_top,\n",
    "    lgb_shap_values=lgb_shap_values_top,\n",
    "    ensemble_test_predictions=ensemble_test_predictions,\n",
    "    ensemble_test_probabilities=ensemble_test_probabilities\n",
    ")"
   ],
   "id": "f6bac92d3a9d73c3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dashboard data is saved under dashboard_data.json\n"
     ]
    }
   ],
   "execution_count": 27
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
